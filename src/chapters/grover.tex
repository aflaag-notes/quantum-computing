\chapter{Grover's algorithm}

The algorithm that we are going to present in this chapter is, together with Shor's algorithm, one of the most famous quantum algorithms we currently know. In 1996, \textcite{grover} published a landmark paper called \curlyquotes{Quantum Mechanics Helps in Searching for a Needle in a Haystack}, which contains the algorithm that we will present in this chapter. In the same year, Grover famously noted:

\begin{quote}
It might be possible to combine the search scheme of this paper with \textcite{shor} and other quantum mechanical algorithms to design faster algorithms
\end{quote}

This statement led many to speculate that Grover may have drawn inspiration from Shor's algorithm, although the exact influence remains unclear. Nevertheless, we will see at the end of the chapter that it is indeed possible to apply the QPE technique to Grover's algorithm.

\section{The search problem}

The name of Grover's work already suggests the problem his work tried to solve: the search problem. The setting is the following: we are given an array of $N$ elements --- we can assume that $N$ is always a power of 2 for some $n$, i.e. $N= 2^n$ --- that contains $M$ \curlyquotes{solution} elements. However, we do not know their positions, and the problem asks to find the index of any solution element.

More formally, given a Boolean function $$\funcmap{f}{\{0, \ldots, N - 1\}}{\B}{x}{\soe{ll}{1 & A[x] \in S \\ 0 & \mbox{otherwise}}}$$ where $A$ is our array, and $S$ is the set of solution elements, the problem asks to return an $x$ such that $f(x) = 1$, i.e. such that $A[x]$ is a solution.

With a classical computation, it is easy to see that we need $O \rbk{\tfrac{N}{M}}$ accesses to $A$ to solve our problem, however we will see that the algorithm Grover developed is able to return a \curlyquotes{solution index} in $O\rbk{\sqrt{\tfrac{M}{N}}}$ with \tit{high probability}, i.e. Grover's algorithm provides a \tbf{quadratic} speedup compared to any classical algorithm --- however, it is probabilistic.

Before explaining the details of the algorithm, we need to define some new operators that will be used in Grover's algorithm, and introduce some general notation:

\begin{itemize}
    \item given an arbitrary qubit $\ket \psi$, we will write its superposition of states as follows $$\ket \psi = \sum_{b \in \B^n}{\alpha_b \ket b}$$ where $\sum_{b \in \B^n}{\abs{\alpha_b}} = 1$
    \item we define an operator $O_f$ (where $f$ is the indicator function of the array defined before) that computes as follows: $$\forall x \in \B^n \quad O_f \ket x := (-1)^{f(x)} \ket x$$
    \item given an arbitrary qubit $\ket \psi$, we define a new operator $W$ as follows: $$W := 2 \ket s \bra s - I$$ where $\ket s$ is the \tbf{uniform superposition}, a superposition of states in which each amplitude is equally likely $$\ket s = \dfrac{1}{\sqrt N} \sum_{x \in \B^n}{\ket x}$$
    \item finally, we will define an operator $G$ that will simply compose the last two operators we described $$G = W \cdot O_f$$
\end{itemize}

\begin{framedalgo}{Grover's algorithm}
    Given a Boolean function $f$ that describes the solution elements of an array having $M$ solution elements, and $n$ qubits, the algorithm returns a solution index with high probability TODO SPIEGA CHE VUOL DIRE. \\
    \hrule

    \quad
    \begin{algorithmic}[1]
        \Function{Grover}{$f$, $q_0$}
            \State $q_0 \gets H^{\otimes n}(q_0)$
            \For{$i \in \sbk{1, O\rbk{\sqrt{\tfrac{N}{M}}}}$} \Comment{where $N = 2^n$}
                \State $q_0 \gets G(q_0)$
            \EndFor
            \State \tbf{return} $\mbox{measure}(q_0)$
        \EndFunction
    \end{algorithmic}
\end{framedalgo}

First of all, we see that this algorithm takes only $n$ qubits as input, however the actual implementation of the algorithm is slightly different, as shown in the following quantum circuit below.

\begin{figure}[H]
	\[
		\Qcircuit @C=3em @R=3em {
                  & \lstick{q_0^{\otimes n}} & \gate{H^{\otimes n}} & \multigate{1}{G} &  \multigate{1}{\cdots} & \multigate{1}{G} & \meter & \cw \\ 
                  & \lstick{q_1} & \qw & \ghost{G} & \ghost{\cdots}  & \ghost{G} & \qw & \qw \\
		}
	\]
	\caption{The quantum circuit for Grover's algorithm.}
\end{figure}

Indeed, we can see that the real quantum circuit takes $n + 1$ inputs, and the additional register is called \curlyquotes{ancilla} becuase its only purpose is to actually implement the $O_f$ operator, which is designed exactly as if it was the $U_f$ black-box we discussed in previous algorithms. This can be done thanks to \cref{U lemma}, which guarantees that if we give $\ket -$ as the second input to $U_f$ we get $$O_f \ket x \otimes \ket -  = (-1)^{f(x)} \otimes \ket - $$ which implies that the ancilla register will sill be $\ket -$, therefore we can just ignore the second register completely throughout the whole computation, and we are sure that $O_f$ computes correctly.

Furthermore, before proceeding, let us prove that $G$ is actually a unitary operator, i.e. that this is a valid quantum computation.

\begin{framedprop}{}
    The $O_f$, $W$ and $G$ operators are unitary.
\end{framedprop}

\begin{proof}
    By \cref{unitary prod}, it suffices to prove that both $O_f$ and $W$ are unitary operators.

    \claim{
        The $O_f$ operator is unitary
    }{
        First, we notice that the $O_f$ operator computes as follows:
        \begin{equation}
            \begin{split}
                O_f & = O_f \cdot I \\ 
                    & = O_f \sum_{x} \ket x \bra x \\ 
                    & = \sum_{x} O_f \ket x \bra x \\ 
                    & = \sum_{x} (-1)^{f(x)} \ket x \bra x \\
            \end{split}
        \end{equation}
        This implies that $$O_f^\dag = \sum_x \rbk{(-1)^{f(x)} \ket x \bra x}^\dag = \sum_x (-1)^{f(x)} \ket x \bra x$$ so the operator is Hermitian. Hence, we have that
        \begin{equation*}
            \begin{split}
                O_f O_f & = \rbk{\sum_x (-1)^{f(x)} \ket x \bra x} \rbk{\sum_y (-1)^{f(y)} \ket y \bra y} \\ 
                        & = \sum_{x} \sum_y (-1)^{f(x) + f(y)} \ket x \braket{x|y} \bra y \\ 
                        & = \sum_x (-1)^{2f(x)} \ket x \bra x \\ 
                        & = \sum_x \ket x \bra x \\ 
                        & = I
            \end{split}
        \end{equation*}
        which suffices to prove the claim by Hermiticity.
    }

    Next, we show the $W$ operator.

    \claim{
        The $W$ operator is unitary.
    }{
        Again, since $$(\ket s \bra s)^\dag = \ket s \bra s$$ it immediately follows that $W$ is also Hermitian. Then, we see that
        \begin{equation*}
            \begin{split}
                WW & = (2 \ket s \bra s - I)(2 \ket s \bra s - I) \\ 
                   & = 4 \ket s \braket{s|s} \bra s - 2 \ket s \bra s - 2 \ket s \bra s - I^2 \\ 
                   & = 4 \ket s \bra s - 4 \ket s \bra s + I \\ 
                   & = I
            \end{split}
        \end{equation*}
        which proves that $W$ is unitary by Hermiticity.
    }

    The two claims above conclude the proof.
\end{proof}

Now that we know this operator is unitary, we can delve into the details of the algorithm. To see what happens at each iteration, we will describe the complete state of the system in a rather unusual way. Let $\ket a$ and $\ket b$ be the following superpositions: $$\ket a := \dfrac{1}{\sqrt{N - M}} \sum_{x \in \overline S}{\ket x} \quad \quad \ket b := \dfrac{1}{\sqrt M}\sum_{x \in S}{\ket x}$$ In other words, $\ket a$ is the uniform superposition of non-solution indices, and $\ket b$ is the superposition of solution ones.Moreover, it's easy to see that $\ket a$ and $\ket b$ are orthogonal, so they form an orthonormal basis for a 2D space.

Furthermore, because of how we defined $\ket a$ and $\ket b$, we can rewrite the uniform superposition $$\ket s = \dfrac{1}{\sqrt N} \sum_{x \in \B}{\ket x}$$ as shown below $$\ket s = \sqrt{\dfrac{N - M}{N}} \ket a + \sqrt{\dfrac{M}{N}} \ket b$$ This is quite interesting, because it means that we can describe $\ket s$ in terms of $\ket a$ and $\ket b$. Let us do exactly this, and plot the resulting graph on a 2D space that has $\ket a$ and $\ket b$ as axis.

\centeredsvg{0.5}{../assets/grover1}

We observe that:

\begin{itemize}
    \item both $\ket s$, $\ket a $ and $\ket b$ are normalized, and all the vectors that we are going to consider are quantum states, so they will be normalized too, therefore we can restrict our focus on a 2D circumference of radius 1 --- this plane is usually called \tbf{Grover plane}
    \item since we expect that $M \ll N$, we have that $\sqrt{\tfrac{M}{N}} \ll \sqrt{\tfrac{N - M}{N}}$, which basically means that the vector $\ket s$ is almost parallel to $\ket a$ (the bigger is the numer of solution indices, the bigger the angle between $\ket s$ and $\ket a$)
\end{itemize}

Consider any state $\ket \psi = \sum_{x \in \B^n}{\alpha_x \ket x}$; we observe that
\begin{equation*}
    \begin{split}
        O_f \ket \psi & = O_f \sum_{x \in \B^n}{\alpha_x \ket x}  \\ 
                      & = \sum_{x \in \B^n}{\alpha_x O_f \ket x} \\ 
                      & = \sum_{x \in \B^n}{\alpha_x (-1)^{f(x)} \ket x} 
    \end{split}
\end{equation*}
which basically means that each time we apply the $O_f$ operator we are flipping the sign of the amplitudes of the compoents of $\ket \psi$ that represent solution indices. In other words, the $O_f$ operator flips its input w.r.t. $\ket a$.

Moreover, we observe that any state $\ket \psi$ can be decomposed into $$\ket \psi = \alpha \ket s + \beta \ket{s_\bot}$$ where $\alpha \ket s$ is the projection of $\ket \psi$ along $\ket s$'s space --- thus $\alpha = \braket{s|\psi}$ --- and $\ket {s_\bot}$ is the projection of $\ket \psi$ along the space that is orthogonal to $\ket s$'s. Therefore, we have that
\begin{equation*}
    \begin{split}
        W \ket \psi & = W(\alpha \ket s + \beta \ket{s_\bot}) \\ 
                    & = 2 \ket s \bra s (\alpha \ket s + \beta \ket{s_\bot}) - (\alpha \ket s + \beta \ket{s_\bot}) \\ 
                    & = 2 \alpha \ket s \braket{s|s} + 2 \beta \ket s \braket{s|s_\bot} - (\alpha \ket s + \beta \ket{s_\bot}) \\ 
                    & = 2 \alpha \ket s \cdot 1 + 0 - (\alpha \ket s + \beta \ket{s_\bot}) \\ 
                    & = \alpha \ket s - \beta \ket{s_\bot}
    \end{split}
\end{equation*}
which means that what $W$ actually performs is leaving the component along $\ket s$'s space unchanged, and it flips the sign of the component of the orthogonal space. In other words, what $W$ computes is the reflection of $\ket \psi$ w.r.t. $\ket s$.

We can finally describe Grover's algorithm in detail. First, we see that
\begin{equation*}
    \begin{split}
        & q_0 \\ 
        = & \ket{0}^{\otimes n} \\ 
        \xrightarrow{H^{\otimes n}(q_0)} & \dfrac{1}{\sqrt{2^n}}\sum_{x \in \B^n}{(-1)^{0^n \cdot x} \ket x} \\ 
        = & \dfrac{1}{\sqrt N} \sum_{x \in \B^n}{\ket x} \\ 
        = & \ket s
    \end{split}
\end{equation*}

Indeed, the only purpose of the first Hadamard operator is to \curlyquotes{move} the initial state slightly away from $\ket a$ --- and also making each component initially equally likelly. Now, let's see what happens at each application of the $G = W \cdot O_f$ operator:

\begin{itemize}
    \item as previously shown, the operator $O_f$ reflects its input w.r.t. the $\ket a$ axis --- below we show what happens when we first apply $O_f \ q_0 = O_f \ket s$: \centeredsvg{0.5}{../assets/grover2}
    \item additionally, as previously explained the operator $W$ reflects its input w.r.t. the axis described by the $\ket s$ vector, thus when we compute $W \cdot O_f \ q_0$ we will end up with the following vector: \centeredsvg{0.5}{../assets/grover3}
    \item this suggests that each time we apply the operator $G$ we are making $q_0$ closer and closer to $\ket b$, as depicted below: \centeredsvg{0.5}{../assets/grover4}
\end{itemize}

This is the core idea of Grover's algorithm: if we rewrite $q_0$ in terms of $\ket a$ and $\ket b$ $$q_0 = \beta_0 \ket a + \beta_1 \ket b$$ we get that through Grover's algorithm we transformed $q_0$ such that it is now very close to $\ket b$, meaning that $\beta_0 \ll \beta_1$. This direcly implies that when we will measure $q_0$ at the end of Grover's procedure the likelihood that it will collapse into some $\ket x$ that is a component of $\ket b$ --- i.e. a solution index --- is very high. In other words, what happens with Grover's algorithm is that we gradually increase the amplitudes of the solution indices, in order to maximize the probability that our qubit will collapse in one them when it will be measured at the end of the procedure.

Now that we know how Grover's algorithm works, the only thing left to discuss is the $O \rbk{\sqrt{\tfrac{N}{M}}}$ factor. Why is it guaranteed that after this amount of applications of the $G$ operator we are done with the algorithm? Well, we actually have the opposite problem: in reality, we have to \tit{stop early enough}. Consider again how Grover's algorithm operates in the Grover plane; clearly, if we apply $G$ too many times, what happens is that $q_0$ will end up past $\ket b$ itself: \todo{drawing}

Let the angle between $\ket a$ and $\ket s$ be $\theta$; since $O_f$ flips $q_0$ w.r.t. $\ket a$, and $W$ flips $O_f \ q_0$ w.r.t. $\ket s$, $G$ will cumulatively rotate $q_0$ by $2 \theta$: \todo{drawing} More formally, we can actually show that the matrix $G$ is a rotation matrix of $2 \theta$.

\begin{framedprop}[label={grover op}]{}
    The Grover operator $G$ can be rewritten in the basis $\{\ket a, \ket b\}$ as follows: $$G = \rmat{\cos{2 \theta} & - \sin {2 \theta} \\ \sin{2 \theta} & \cos {2 \theta}}$$ where $\ket s = \cos \theta \ket a + \sin \theta \ket b$.
\end{framedprop}

\begin{proof}
    First, we need two small identities in trigonometry.

    \claim[Claim 1]{
        For any angle $\theta$ it holds that $2 \cos^2 \theta  - 1 = \cos 2 \theta$.
    }{
        Through algebraic manipulation we have that
        \begin{equation*}
            \begin{alignedat}{2}
                2 \cos^2 \theta - 1 & = 2 \cos^2 \theta - (\sin^2 \theta + \cos^2 \theta) & \\ 
                                    & = 2 \cos^2 \theta - \sin^2 \theta - \cos^2 \theta & \\ 
                                    & = \cos^2 \theta - \sin^2 \theta & \\ 
                                    & = \cos 2 \theta & (\mbox{double-angle formula}) \\ 
            \end{alignedat}
        \end{equation*}
    }

    \claim[Claim 2]{
        For any angle $\theta$ it holds that $1 - 2 \sin^2 \theta = \cos 2 \theta$.
    }{
        From the previous claim we have that $$1 - 2 \sin ^2 \theta = 1 - 2 (1 - \cos^2 \theta) = 1 - 2 + 2\cos^2 \theta = 1 + \cos 2 \theta - 1 = \cos 2 \theta$$
    }

    Let $\ket v$ be any vector described inside the $\{\ket a, \ket b\}$: $$\ket v = \alpha \ket a + \beta \ket b$$ We already saw how the $O_f$ operator flips its input w.r.t. the $\ket a$ axis, indeed its action on $\ket v$ would be the following: $$O_f \ket v = \alpha \ket a - \beta \ket b$$ Thus, starting from the definition of $G$, by the two previous claims we have that
    \begin{equation*}
        \begin{split}
            G \ket v & = W \cdot O_f  \ket v \\ 
                     & = W \cdot (\alpha \ket a - \beta \ket b) \\ 
                     & = (2 \ket s \bra s - I)(\alpha \ket a - \beta \ket b) \\ 
                     & = 2 \ket s \bra s (\alpha \ket a - \beta \ket b) - (\alpha \ket a - \beta \ket b) \\ 
                     & = 2 \ket s (\cos \theta \ket a + \sin \theta \ket b)^\dag (\alpha \ket a - \beta \ket b) - (\alpha \ket a - \beta \ket b) \\ 
                     & = 2 \ket s (\cos \theta \bra a + \sin \theta \bra b)(\alpha \ket a - \beta \ket b) - (\alpha \ket a - \beta \ket b) \\ 
                     & = 2 \ket s (\alpha \cos \theta - \beta \sin \theta) - (\alpha \ket a - \beta \ket b) \\ 
                     & = 2 (\cos \theta \ket a + \sin \theta \ket b) (\alpha \cos \theta - \beta \sin \theta) - (\alpha \ket a - \beta \ket b) \\ 
                     & = 2 \alpha \cos^2 \theta \ket a - 2 \beta \cos \theta \sin \theta \ket a + 2 \alpha \sin \theta \cos \theta \ket b - 2 \beta \sin^2 \theta \ket b - \alpha \ket a + \beta \ket b \\  
                     & = \sbk{\alpha \rbk{2 \cos^2 \theta - 1} - 2 \beta \cos \theta \sin \theta} \ket a + \sbk{\beta \rbk{1 - 2 \sin^2 \theta} + 2 \alpha \sin \theta \cos \theta} \ket b \\ 
                     & = (\alpha \cos 2 \theta - \beta \sin 2 \theta) \ket a + (\alpha \sin 2 \theta + \beta \cos 2 \theta) \ket b \\ 
                     & = \rmat{\alpha \cos 2 \theta - \beta \sin 2 \theta \\ \alpha \sin \theta + \beta \cos \theta} \\ 
                     & = \rmat{\cos 2 \theta & - \sin 2\theta \\ \sin \theta & \cos \theta} \rmat{\alpha \\ \beta} \\ 
                     & = \rmat{\cos 2 \theta & - \sin 2\theta \\ \sin \theta & \cos \theta} \ket v \\ 
        \end{split}
    \end{equation*}
    which concludes that $G$ must be the rotation matrix of $2 \theta$.
\end{proof}

Indeed, with each application of $G$ we are rotating $q_0$ by $2 \theta$, which means that at the $k$-th application it holds that $$G^k (H( q_0)) = \cos (2k + 1) \theta \ket a + \sin (2k + 1) \theta \ket b$$ for any $k \in \N$, where the additional 1 comes from the fact that $q_0 = \ket s$ through the Hadamard transformation at the start of the process. Thus, to evaluate the optimal number of iterations we need to find the optimal $k$, i.e. the one that maximizes the probability of measuring a solution index, which is equal to the squared amplitude of $\ket b$, namely $$\Pr[\mbox{measure}(q_0) = \ket b] = \sin^2 \sbk{(2k + 1) \theta}$$ Hence, we have that $\sin^2 \sbk{(2k + 1)\theta }= 1$ when $(2k +1) \theta = \tfrac{\pi}{2}$, and solving for $k$ we get that $$k = \dfrac{\pi}{4 \theta} - \dfrac{1}{2}$$ Lastly, since $\theta$ is the angle between $\ket a$ and $\ket s$, we can rewrite $\ket s$ as $$\ket s = \cos \theta \ket a + \sin \theta \ket b$$ which directly implies that $$\sin \theta = \sqrt {\dfrac{M}{N}} \iff \theta = \arcsin \sqrt{\dfrac{M}{N}}$$ and therefore
\begin{equation*}
    \begin{split}
        k & = \dfrac{\pi}{4 \theta} - \dfrac{1}{2}  \\
          & = \dfrac{\pi}{4 \arcsin \sqrt{\dfrac{M}{N}}} - \dfrac{1}{2}  \\  
                                               & \le  \dfrac{\pi}{4 \arcsin \sqrt{\dfrac{M}{N}}} \\ 
                                               & \le \dfrac{\pi}{4 \sqrt{\dfrac{M}{N}}} \\ 
                                               & = \dfrac{\pi}{4} \sqrt{\dfrac{N}{M}} \\ 
                                               & = O \rbk{\sqrt{\dfrac{N}{M}}}
   \end{split}
\end{equation*}
which finally explains the quadratic speedup of Grover's algorithm w.r.t. the classical version of the problem.

As a final note, we observe that Grover's algorithm assumes that $\theta \le \tfrac{\pi}{4}$, otherwise we overshoot $\ket b$ with a single iteration of the $G$ operator --- since $\theta > \tfrac{\pi}{4}$ would mean that $\ket s$ is placed on the \curlyquotes{upper half} of the Grover plane. However, to ensure this constraint on $\theta$ we only need that $M \le \tfrac{N}{2}$, i.e. at most half of the elements in our array are solution elements. Indeed, we see that $$M \le \dfrac{N}{2} \implies \sin \theta = \sqrt{\dfrac{M}{N}} \le \sqrt{\dfrac{1}{2}} \implies \theta \le \arcsin \sqrt{\dfrac{1}{2}} = \dfrac{\pi}{4}$$ What can we do if the number of solutions is more than half the size of the array? We simply invert the problem and find the non-solutions!

\subsection{Another perspective}

Interestingly enough, we can look at what happens to the amplitudes of $q_0$ from another perspective. Usually, the $W$ operator is called \tbf{diffusion operator}, and what it does is computing the so called \tit{inversion about the mean} of its input. Consider any state $\ket \psi = \sum_{x \in \B^n}{\alpha_x \ket x}$, and observe that
\begin{equation*}
    \begin{alignedat}{2}
        \braket{s|\psi} & = \rbk{\dfrac{1}{\sqrt N} \sum_{y \in \B^n}{\ket y}}^\dag \rbk{\sum_{x \in \B^n}{\alpha_x \ket x}} & \quad (\mbox{since $\bra s = \ket s ^\dag$})\\ 
                        & = \rbk{\dfrac{1}{\sqrt N} \sum_{y \in \B^n}{\bra y}} \rbk{\sum_{x \in \B^n}{\alpha_x \ket x}} & \\ 
                        & = \dfrac{1}{\sqrt N} \sum_{y \in \B^n}{\sum_{x \in \B^n}{\alpha_x \braket{y|x}}} & \\ 
                        & = \dfrac{1}{\sqrt N} \sum_{x \in \B^n}{\alpha_x} & \\ 
                        & = \sqrt N \overline{\alpha}_\psi
    \end{alignedat}
\end{equation*}
where $\overline{\alpha}_\psi$ is the average amplitude of $\ket \psi$. This implies that
\begin{equation*}
    \begin{alignedat}{2}
        W \ket \psi & = (2 \ket s \bra s - I) \ket \psi & \\ 
                    & = 2 \ket s \braket{s|\psi} - \sum_{x \in \B^n}{\alpha_x \ket x} & \\  
                    & = 2 \ket s \rbk{\sqrt N \overline{\alpha}_\psi} - \sum_{x \in \B^n}{\alpha_x \ket x} & \quad (\mbox{for the previous observation}) \\ 
                    & = 2 \rbk{\dfrac{1}{\sqrt N} \sum_{y \in \B^n}{\ket y}} \sqrt N \overline{\alpha}_\psi - \sum_{x \in \B^n}{\alpha_x \ket x} & \\
                    & = \sum_{x \in \B^n}{\rbk{2 \overline{\alpha}_\psi - \alpha_x} \ket x} & \\ 
    \end{alignedat}
\end{equation*}
In other words, when we apply $W$ to a superposition of states, what happens is that each amplitude of the basis states is trasformed as follows: $$\func{W}{\alpha_x}{2 \overline{\alpha}_\psi - \alpha_x}$$ To understand why this is important, let's look at what happens in Grover's algorithm after the first Hadamard application. TODO \todo{da finire}

\section{Fixed-Point Quantum Search}

As we saw in the previous sections, with Grover's algorithm we need to be careful on how many times we apply the $G$ operator, however we observe that the right value for $k$ --- i.e. the numeber of iterations --- strictly depends on both $N$ and $M$. Assuming that $N$, the length of the array, is known, it is not guaranteed that we know $M$ too (the number of solutions in the array). Therefore, if we want to apply Grover's algorithm we also need to be able to to a rough estimate on $M$, otherwise we might end up stopping either too early or too late. This makes Grover's algorithm fragile in some real-world scenarios.

Grover was actually aware of this problem, and in 2005 he designed a new algorithm which is able to solve this issue \cite{grover2}. The idea of the algorithm is to \tit{monotonically} increase the probability of finding a solution, without oscillating back down $\ket a$. This algorithm is called \tbf{Fixed-Point Quantum Search}, becuase the solutions actually become a \curlyquotes{stable fixed point} of the transformation --- i.e. once you reach a good solution, further iterations leave it basically unchanged. Indeed, with Grover's search each iteration is a constant-angle rotation, while in fixed-point search we will see that the phase angles in each rotation changes such that the rotation angle decreases over time.

First, we need to define two new operators: $$R_s := I - (1 - e^{i \theta}) \ket s \bra s \quad \quad R_t := I - (1 - e^{i \theta}) \ket t \bra t$$ where $\ket s$ is the starting state and $\ket t$ is the target state (in Grover's algorithm this was $\ket b$) --- this is the original notation that Grover used in his paper, and actually explains why we used $\ket s$ in the previous version of the algorithm, it's just the \curlyquotes{start}. These two operators are called \tbf{phase shift operators}, and it can be easily showed that they are both unitary operators --- we will omit the calculations here.

What are these two operators in the first place? When we presented the $W$ operator, we also noticed how it actually performes a reflection of its input w.r.t. the space of $\ket s$. Well, through a very similar argument it can be shown that $$W^\bot := I - 2 \ket s \bra s$$ performs a reflection of its input w.r.t. the space \tit{orthogonal} to $\ket s$ --- indeed, we end up with $$W^\bot \ket \psi = \beta \ket{s_\bot} - \alpha \ket s$$ If we now look at the $R_s$ operator, we can see that when we actually compute the reflection it performs we end up with $$R_s \ket \psi = \beta \ket{s_\bot} + e^{i \theta} \alpha \ket s$$ This suggests that what $R_s$ actually computes is a \curlyquotes{soft reflection} w.r.t. the space perpendicular to $\ket s$. Through an analogous argument, we can see that $$R_t \ket \psi= \beta \ket{s_\bot} + e^{i \theta} \alpha \ket t $$ meaning that $R_t$ computes a \curlyquotes{soft reflection} w.r.t. the space perpendicular to $\ket t$ --- however, we observe that the latter is literally the space of $\ket a$, indeed $O$ in Grover's algorithm could have been defined as $$O = I - 2 \ket b \bra b$$ and indeed it is sometimes defined such.

Now, we are going to define an addition operator called $U$ as such: let $U$ be any unitary operator such that, for some small $\varepsilon > 0$, it holds that $$\abs{\braket{t|Us}}^2 = 1 - \varepsilon$$ We observe that

\begin{itemize}
    \item by the laws of quantum mechanics we have that $$\Pr[\mbox{measure}\rbk{U \ket s} = \ket t] = \abs{\braket{t|Us}}^2$$ therefore we require $U$ to be an operator such that it \curlyquotes{drives $\ket s$ close to $\ket t$ with high probability} --- i.e. $1 - \varepsilon$
    \item we know that $U$ exists since it's just a rotation in a 2D space
    \item also, a geometric interpretation of the scalar product is that we are measuring the cosine of the angle between $\ket t$ and $\ket{Us}$, and we want this value to be very high (such that the angle woule be very small) --- we obeserve that we are in Hilbert spaces so the notion of \curlyquotes{angle} is not the same of the one we are used to with Euclidean spaces, but this is just to have an idea of what is happening with $U$
\end{itemize}

Moreover, define the following operator $$G := U R_s U^\dag R_t U$$ Let's see what happens when we evaluate $G \ket s$. By denoting $U_{ts} = \braket{t|Us}$, we can prove the following equality.

\begin{framedlem}{}
 It holds that $$G \ket s = U \ket s \sbk{e^{i\theta} + \abs{U_{ts}}^2  \rbk{e^{i \theta }-1}^2} + \ket t U_{ts}\rbk{e^{i \theta} - 1}$$
\end{framedlem}

\begin{proof}
    First, we prove the following equality.

    \claim{
        It holds that $\bra s U^\dag \ket t U_{ts} = \abs{U_{ts}}^2$.
    }{
        \begin{equation*}
            \begin{alignedat}{2}
                \bra s U^\dag \ket t U_{ts} & = \braket{s|U^\dag t} U_{ts} & \\ 
                         & = \braket{Us|t} U_{ts} & \quad (\mbox{by def. of adjoint}) \\ 
                         & = \overline{\braket{t|Us}} U_{ts} & \quad (\mbox{by prop. of scalar products}) \\ 
                         & = \overline{U_{ts}} U_{ts} & \\ 
                         & = \abs{U_{ts}}^2 & \quad (z \cdot \overline z = \abs{z}^2)\\ 
            \end{alignedat}
        \end{equation*}
    }

    Let $P_s = \ket s \bra s$ and $P_t = \ket t \bra t$; thus, we have that
    \begin{equation*}
        \begin{alignedat}{2}
            G & = U R_s U^\dag R_t U & \\ 
              & = U(I - (1 - e^{i\theta})P_s)U^\dag(I - (1 - e^{i\theta})P_t)U & \\ 
              & = (U - (1 - e^{i\theta})UP_s)U^\dag(U - (1 - e^{i\theta})UP_t) & \\ 
              & = (UU^\dag - (1 - e^{i\theta}) UP_sU^\dag)(U - (1 - e^{i\theta})UP_t) & \\ 
              & = (I - (1 - e^{i \theta})UP_sU^\dag)(U - (1 - e^{i\theta})UP_t) & \quad (UU^\dag = I) \\ 
              & = U - (1 - e^{i \theta}) P_tU - (1 - e^{i \theta})UP_s + (1 - e^{i\theta})^2 UP_s U^\dag P_t U & \\ 
        \end{alignedat}
    \end{equation*}
    Therefore, we find that $G \ket s$ can be evaluated as follows:
    \begin{equation*}
        \hspace{-1cm}
        \begin{alignedat}{2}
            G \ket s & = \rbk{U - (1 - e^{i \theta}) P_tU - (1 - e^{i \theta})UP_s + (1 - e^{i\theta}) UP_s U^\dag P_t U} \ket s & \\ 
                     & = U \ket s - (1 - e^{i \theta}) P_tU \ket s- (1 - e^{i \theta})UP_s \ket s + (1 - e^{i\theta})^2 UP_s U^\dag P_t U \ket s & \\  
                     & = U \ket s - (1 - e^{i \theta}) \ket t U_{ts} - (1 - e^{i\theta}) U \ket s + (1 - e^{i\theta})^2 U \ket s \bra s U^\dag \ket t U_{ts} & \\ 
                     & = U \ket s - (1 - e^{i \theta}) \ket t U_{ts} - (1 - e^{i\theta}) U \ket s + (1 - e^{i\theta})^2 U \ket s \abs{U_{ts}}^2 & \quad (\mbox{by the claim}) \\ 
                     & = \ldots & \quad (\mbox{algebraic manipulation}) \\ 
                     & = U \ket s \sbk{e^{i\theta} + \abs{U_{ts}}^2  \rbk{e^{i \theta }-1}^2} + \ket t U_{ts}\rbk{e^{i \theta} - 1} & \\ 
        \end{alignedat}
    \end{equation*}
\end{proof}

Most importantly, this equality can be used in the following proposition, which shows that we can actually find an angle $\theta$ for which the distance from $\ket t$ decreases significantly.

\begin{framedprop}{}
    There exists an angle $\theta$ such that $$\Pr[\mbox{measure}(G \ \ket s) = \ket t] = 1 - \varepsilon^3$$
\end{framedprop}

\begin{proof}
    Thanks to the previous lemma, we obtain that
    \begin{equation*}
        \hspace{-1cm}
        \begin{alignedat}{2}
            \Pr[\mbox{measure}\rbk{G \ket s} = \ket t] & = \abs{\braket{t|Gs}}^2 & \\ 
                                                       & = \abs{\bra t \sbk{U \ket s \rbk{e^{i\theta} + \abs{U_{ts}}^2  \rbk{e^{i \theta} - 1}^2} + \ket t U_{ts}\rbk{e^{i \theta} - 1}}}^2 & \\ 
                                                       & = \abs{\braket{t|Us} \rbk{e^{i\theta} + \abs{U_{ts}}^2  \rbk{e^{i \theta } - 1}^2 }+ \braket{t|t} U_{ts}\rbk{e^{i \theta} - 1}}^2 & \\ 
                                                       & = \abs{U_{ts} \rbk{e^{i\theta} + \abs{U_{ts}}^2  \rbk{e^{i \theta } - 1}^2 }+ U_{ts}\rbk{e^{i \theta} - 1}}^2 & \\ 
                                                       & = \abs{U_{ts} \sbk{2e^{i\theta} - 1 + \abs{U_{ts}}^2 \rbk{e^{i \theta} - 1}^2}}^2 & \\ 
                                                       & = \abs{U_{ts}}^2 \cdot \abs{2e^{i\theta} - 1 + \abs{U_{ts}}^2 \rbk{e^{i \theta} - 1}^2}^2 & \\ 
                                                       & = (1 - \varepsilon) \cdot \abs{2e^{i\theta} - 1 + (1 - \varepsilon)\rbk{e^{i \theta} - 1}^2}^2 & \quad \rbk{\abs{U_{ts}}^2 = 1 - \varepsilon}\\ 
        \end{alignedat}
    \end{equation*}

    Now, let's see what happens if we set $\theta = \tfrac{\pi}{3}$: we obtain that
    \begin{equation*}
        \begin{alignedat}{2}
            \Pr[\mbox{measure}\rbk{G \ket s} = \ket t] & = (1 - \varepsilon) \cdot \abs{2e^{i \tfrac{\pi}{3}} - 1 + (1 - \varepsilon)\rbk{e^{i\tfrac{\pi}{3}} - 1}^2}^2 & \\ 
                                                       & = (1 - \varepsilon) \cdot \abs{2e^{i \tfrac{\pi}{3}} - 1 - (1 - \varepsilon)e^{i \tfrac{\pi}{3}}}^2 & \quad \rbk{\rbk{e^{i\tfrac{\pi}{3}} - 1}^2 = - e^{i \tfrac{\pi}{3}}} & \\ 
                                                       & = (1 - \varepsilon) \cdot \abs{(1 + \varepsilon) e^{i \tfrac{\pi}{3}} - 1}^2 & \\ 
                                                       & = (1 - \varepsilon) \cdot \abs{(1 + \varepsilon)\rbk{\dfrac{1}{2} + i \dfrac{\sqrt 4}{2}} - 1}^2 & \quad (\mbox{by Euler's formula}) & \\ 
                                                       & = (1 - \varepsilon) \cdot \abs{\dfrac{1}{2} +i  \dfrac{\sqrt 3}{2} + \dfrac{\varepsilon}{2} + i \dfrac{\sqrt 3}{2} \varepsilon - 1}^2 & \\ 
                                                       & = (1 - \varepsilon) \cdot \abs{\dfrac{\varepsilon}{2} - \dfrac{1}{2} + i \dfrac{\sqrt 3}{2}(1 + \varepsilon)}^2 &  \\ 
                                                       & = (1 - \varepsilon) \cdot \sbk{\rbk{\dfrac{\varepsilon}{2} - \dfrac{1}{2}}^2 + \rbk{\dfrac{\sqrt 3}{2}(1 + \varepsilon)}^2 } & \quad \rbk{\abs{z}^2 = \Re^2(z) + \Im^2(z)} \\ 
                                                       & = (1 - \varepsilon) \cdot (1 + \varepsilon + \varepsilon^2)&  \\ 
                                                       & = 1 + \varepsilon + \varepsilon^2 - \varepsilon - \varepsilon^2 - \varepsilon^3 & \\ 
                                                       & = 1 - \varepsilon^3 & \\ 
        \end{alignedat}
    \end{equation*}

This shows that by applying $G$ the probability of measuring $\ket t$ has increased from $1 - \varepsilon$ to $1 - \varepsilon^3$.
\end{proof}

Indeed, it can be shown that by defining the following recursive sequence of operators $$\soe{ll}{U_0 := U & m = 0 \\ U_m = U_{m - 1} R_s U_{m-1}^\dag R_t U_{m - 1} & m \ge 1}$$ we get that $$\Pr[\mbox{measure}(U_m \ \ket s) = \ket t] = \abs{\braket{t|U_ms}}^2 = 1 - \varepsilon^{2{q_m} + 1}$$ where $q_m$ is the number of queries of $f(x)$.

TODO \todo{drawing of the vector that grows monotonically}

Unfortunately, there already exists a classical probabilistic algorithm that the failure probability drops as $\varepsilon^{q + 1}$  after $q$ queries to $f$. Thus, the quantum advantage with this method is lost.

So, what do we do now? In 2014 \textcite{yoder} proposed a fixed-point quantum serach algorithm that monotonically converges to the target state while still retaining the quadratic advantage of the original Grover's algorithm over classical algorithms. The algorithm involves phase-shift operators that are parametrized with angles different from $\theta = \tfrac{\pi}{3}$, and again involes building a sequence of operators using said phase shifts. However, the details of this result are way beyond the scope of our discussion, so we won't describe the details of their findings.

\section{Quantum counting}

At the end of the previous section we ended our discussion by noticing how the $G$ operator was actually performing a \tit{rotation} of an angle $2 \theta$, but there is still something left to be explained.

Let's try to understand how we can actually employ some of the ideas used in Shor's algorithm for Grover's quantum search. Recall that in \cref{grover op} we proved that the Grover operator can be rewritten as follows: $$G = \rmat{\cos{2 \theta} & - \sin {2 \theta} \\ \sin{2 \theta} & \cos {2 \theta}}$$ We observe the following property.

\begin{framedprop}{}
    The eigenvalues of $G$ are $e^{\pm 2 \theta i}$.
\end{framedprop}

\begin{proof}
    The characteristic polynomial of $G$ is given by
    \begin{equation*}
        \begin{split}
            p(\lambda) & = \det (G - \lambda I) \\ 
                       & = \det \rbk{\rmat{\cos{2 \theta} & - \sin {2 \theta} \\ \sin{2 \theta} & \cos {2 \theta}} - \rmat{\lambda & 0 \\ 0 & \lambda}} \\ 
                       & = \det \rmat{\cos 2 \theta - \lambda & - \sin 2 \theta \\ \sin 2 \theta & \cos 2 \theta - \lambda} \\ 
                       & = \lambda^2 - 2 \lambda \cos 2 \theta  + \cos^2 2 \theta + \sin ^2 2 \theta \\ 
                       & = \lambda^2 - 2 \lambda \cos 2 \theta  + 1 \\ 
        \end{split}
    \end{equation*}
    and solving for $p(\lambda) = 0$ yields the following two eigenvalues:
    \begin{equation*}
        \begin{split}
            \lambda & = \dfrac{2 \cos 2 \theta \pm \sqrt{4 \cos^2 2 \theta - 4}}{2} \\ 
                    & = \cos 2 \theta \pm \sqrt{\cos^2 2 \theta - 1} \\ 
                    & = \cos 2 \theta \pm \sqrt{- \sin^2 2 \theta} \\ 
                    & = \cos 2 \theta \pm i \sin 2 \theta \\ 
                    & = e^{\pm 2 \theta i } \\
        \end{split}
    \end{equation*}
\end{proof}

This means that we can use the QPE algorithm with the matrix $G$ in order to obtain an estimate of $\theta$! In particular, with this information we can actually solve two problems at once:

\begin{itemize}
    \item we can determine an estimate how many iterations of Grover's algorithm we need to run, since $$k = \dfrac{\pi}{4 \theta} - \dfrac{1}{2}$$
    \item we can also evaluate an estimate on $M$, the number of solutions, since $$\theta = \arcsin \sqrt{\dfrac{M}{N}} \iff M = N \sin^2 \theta$$
\end{itemize}

The problem of estimating $M$ is usually referred to as \tbf{quantum counting} in the literature, since the output is the number of solutions inside the given array.

TODO \todo{drawing}

It can be proven that the quantum counting circuit estimates $\pm 2\theta$ --- to be precise, it actually estimates $2 \theta$ or $2\pi - 2\theta$ since QPE does not return negative angles --- to a degree of accuracy up to a desired $2^-m$ with probability at least $1 - \varepsilon$. Moreover, it can be shown that $$\abs{\tilde M - M} < \rbk{2 \sqrt{MN} + \dfrac{N}{2^{m + 1}}} 2^{-m}$$ where $\tilde M$ is the estimated value of $M$ through the QPE algorithm. For instance, choosing $m = \ceil{\tfrac{n}{2}} + 1$ and $\varepsilon = \tfrac{1}{6}$, we get that $t = \ceil{\tfrac{n}{2}} + 3$ and an estimation of $M$ with an error of about $$\abs{\tilde M - M} < \sqrt{\dfrac{M}{2}} + \dfrac{1}{4} = O(\sqrt M)$$ with only $O(2^t) = O(\sqrt N)$ iterations of the Grover operator, i.e. array accesses. Classically, to know the value of $M$ with the same error in the estimate we would necessarily need $O(N)$ accesses.

Lastly, another problem that quantum counting solves is knowing if $M$ is 0 or not. Indeed, Grover's algorithm relies on the assumpion that $M \neq 0$, i.e. there are solution elements, otherwise the procedure actually does nothing.

\section{Exercises}

\begin{framedprob}{}
    Starting from the fact that $W$ is the operator that computes the inversion about the mean of the coefficients of its inputs, show that $$W = 2 \ket s \bra s - I$$ where $\ket s = \tfrac{1}{\sqrt N} \sum_x \ket x$.
\end{framedprob}

\solution{
Let $\ket \psi$ be a qubit, i.e. $$\ket \psi = \sum_x \alpha_x \ket x$$ for some coefficients $\alpha_x$, and assume that $W$ computes the inversion about the mean of each $\alpha_x$. This means that $$\func{W}{\alpha_x}{2 \overline{\alpha}_\psi - \alpha_x}$$ where $\overline{\alpha}_\psi$ is the average of the coefficients of $\ket \psi$. Therefore, we have that
    \begin{equation*}
        \begin{split}
        W \ket \psi &= \sum_x (2 \overline {\alpha}_\psi - \alpha_x) \ket x \\ 
                    &= 2 \overline{\alpha}_\psi \sum_x \ket x - \sum_x \alpha_x \ket x \\ 
                        &= 2 \rbk{\dfrac{1}{N} \sum_y \alpha_y} \rbk{\sum_x \ket x} - \ket \psi \\ 
                        & = 2 \rbk{\dfrac{1}{\sqrt N} \sum_y \alpha_y}\rbk{\dfrac{1}{\sqrt N} \sum_x \ket x} - \ket \psi \\ 
                        & = 2 \rbk{\dfrac{1}{\sqrt N} \sum_y \alpha_y} \ket s - \ket \psi \\  
                        & = 2 \rbk{\dfrac{1}{\sqrt N} \sum_z \bra z} \rbk{ \sum_w \alpha_w \ket w} \ket s - \ket \psi \\ 
                        & = 2 \braket{s|\psi} \ket s - \ket \psi \\  
                        & = (2 \ket s \bra s - I) \ket \psi
        \end{split}
    \end{equation*}
    which ultimately implies that $$W  = 2 \ket s \bra s - I$$
}

\begin{framedprob}{}
    Given an array of size $N = 4$ which contains only one \curlyquotes{solution} element at index 3, prove that Grover's algorithm finds the solution in only 1 application of the $G$ operator.
\end{framedprob}

\solution{
    First, we observe that since $N = 4$ we only need $n = 2$ qubits to perform Grover's algorithm, and in particular we also know that $$f(00) = f(01) = f(10) = 0 \quad \quad f(11) = 1$$ since the only solution element of the array is in the last position. Let's see what happens after running Grover's algorithm with only 1 application of the $G$ operator (we will omit the ancilla qubits in the calculations for brevity)
    \begin{equation*}
        \begin{split}
            & q_0 \\ 
            = & \ket{0}^{\otimes 2} \\ 
            \xrightarrow{H^{\otimes 2}(q_0)} & (H \otimes H)(\ket 0 \otimes \ket 0) \\ 
             = & H \ket 0 \otimes H \ket 0 \\ 
              = & \ket + \otimes \ket + \\ 
              = & \dfrac{1}{\sqrt 2}(\ket 0 + \ket 1) \otimes \dfrac{1}{\sqrt 2} (\ket 0 + \ket 1) \\ 
              = & \dfrac{1}{2} (\ket {00} + \ket{01} + \ket{10} + \ket{11}) \\ 
              \xrightarrow{O_f(q_0)} & \dfrac{1}{2} (O_f\ket {00} + O_f \ket{01} + O_f \ket{10} + O_f \ket{11}) \\ 
              = & \dfrac{1}{2} (\ket{00} + \ket{01} + \ket{10} - \ket{11}) \\ 
        \end{split}
    \end{equation*}
    The last part of the calculations involves applying the $W$ operator on the state, but since the calculations are quite dense we will see what happens with only one of the products and other ones can be proved to be analogous.
    
    \claim{
        $W \ket{00} = \tfrac{1}{2}(- \ket{00} + \ket{01} + \ket{10} + \ket{11})$
    }{
        Recall that $\ket s = \dfrac{1}{\sqrt N} \sum_{x \in \B^n} \ket x$ therefore, in our case, we have that $$\ket s = \dfrac{1}{2}(\ket{00} +\ket{01} + \ket{10} + \ket{11})$$ and $\bra s$ is defined accordingly. Hence, we have that
        \begin{equation*}
            \begin{split}
                & W \ket{00} \\ 
                = & (2 \ket s \bra s - I) \ket{00} \\ 
                = & 2 \ket s \braket{s|00} - \ket{00} \\ 
                = & 2 \ket s \dfrac{1}{2}(\bra{00} + \bra{01} + \bra{10} + \bra{11}) \ket{00} - \ket {00} \\ 
                = & \ket s (\braket{0|0} \braket{00} + \braket{0|0} \braket{1|0} + \braket{1|0} \braket{00} + \braket{1|0} \braket{1|0}) - \ket{00} \\ 
                = & \ket s \abs{\braket{0|0}}^2 - \ket{00}  \\
                = & \ket s  - \ket{00} \\ 
                = & \dfrac{1}{2} (\ket{00} + \ket{01} + \ket{10} + \ket{11}) - \ket{00} \\ 
                = & \dfrac{1}{2} (- \ket{00} + \ket{01} +\ket{10} + \ket{11})
            \end{split}
        \end{equation*}
    }

    Then, to conclude the calculations we see that 
    \begin{equation*}
        \begin{split}
            & \dfrac{1}{2} (\ket{00} + \ket{01} + \ket{10} - \ket{11}) \\ 
            \xrightarrow{W(q_0)} & \dfrac{1}{2}( W\ket{00} + W\ket{01} + W \ket{10} - W \ket{11}) \\ 
            = & \dfrac{1}{2} \sbk{\dfrac{1}{2}(-\ket {00} + \ket{01} + \ket{10} + \ket{11}) + \dfrac{1}{2} (\ket{00} - \ket{01} + \ket{10} + \ket{11}) + W \ket {10} - W \ket{11}} \\
            = & \dfrac{1}{2} \sbk{(\ket{10} + \ket{11}) + \dfrac{1}{2}(\ket {00} + \ket{01} - \ket{10} + \ket{11}) - \dfrac{1}{2}(\ket{00} + \ket{01} +\ket{10} -\ket{11})} \\
            = & \dfrac{1}{2} \sbk{(\ket{10} + \ket{11})  + (- \ket{10} + \ket{11})} \\
            = & \dfrac{1}{2} \cdot 2\cdot \ket{11} \\
            = & \ket {11}
        \end{split}
    \end{equation*}
    This proves that when measuring the final state, the qubit will collapse to the state $\ket{11}$ with probability 1, meaning that the solution is in fact at index 3.

    Although this solution is perfectly valid, it does not really capture the idea of Grover's algorithm, so we want to briefly present a more \curlyquotes{intuitive} solution of this exercise. Since $\ket a$ is the superposition of all the non-solution indices, and $\ket b$ is the superposition of the solution indices, we have that $$\ket a = \dfrac{1}{\sqrt 3}(\ket{00} + \ket{01} + \ket{10}) \quad \quad \ket b = \ket{11}$$ and indeed $$\ket s = \dfrac{\sqrt 3}{2} \ket a + \dfrac{1}{2} \ket b$$ which means that $$\cos \theta = \dfrac{\sqrt 3}{2} \quad \quad \sin \theta = \dfrac{1}{2}$$ and the only possible angle satisfying these requirements is $\theta = \tfrac{\pi}{6}$. Indeed, since we know that the $G$ operator performs a rotation of its input by an angle of $2 \theta$, Grover's algorithm on this array would act as follows:

    \begin{itemize}
        \item first, apply $H^{\otimes 2}$ to $\ket 0 ^{\otimes 2}$, thus placing the input to $\ket s$; this means that the angle between the input and $\ket a$ is now $\tfrac{\pi}{6}$
        \item then apply $G$, i.e. rotate the input by $$2 \theta = 2 \cdot \dfrac{\pi}{6} = \dfrac{\pi}{3}$$ which means that the angle between the input and $\ket s$ will now become $$\dfrac{\pi}{6} + \dfrac{\pi}{3} = \dfrac{\pi}{2}$$
    \end{itemize}

    In other words, the input now lies precisely over $\ket b$, meaning that Grover's algorithm correctly found the solution element with 1 application of the $G$ operator.
}
