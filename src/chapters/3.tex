\chapter{Quantum algorithms}

Now that we presented all the mathematical tools we need to perform quantum computations, we are ready to explore some of the most famous and most important quantum algorithm that have been developed in recent years. But before introducing any algorithm, let's discuss \tit{why} we are interested in quantum computing at all.

Consider some computable function $f(x)$ and some classical algorithm that is able to compute it for any valid input $x$. If we have an input $x_1$, and we want to compute its output we need to run the algorithm in order to compute $f(x_1)$. Analogously, if we have another input $x_2$ we need to run the algorithm again in order to compute $f(x_2)$. In general, if we want to know the outputs $f(x_1), \ldots, f(x_N)$ of $N$ inputs $x_1, \ldots, x_N$, with classical computing we must the algorithm $N$ distinct times, because there is no way to compute more than one output at a time. With quantum computations, however, we will see that not ony this is possible, but we can actually compute \tit{all} the possible outputs related to all the possible inputs \tbf{simultaneously}. To the best of our knowledge, through the laws of quantum mechanics we \tit{really} can compute all the possible outpus $f(x)$ for each $x \in \B^n$ --- for a fixed input length $n$.

How does this work in practice? Recall that we \tit{seemingly arbitrarily} decided to denote the vectors of the canonical basis with binary strings, such as $\ket{000}$, $\ket{001}$, $\ket{010}$ and so on (we chose $n = 3$ for this example). This is no coincidence: since this vectors form a basis, we can write any quantum state $\ket \psi$ as a linear combination of them, and by doing so we obtain something like $$\ket \psi = \sum_{x \in \B^3}{\alpha_x \ket x}$$ Say that we want to compute $f(x)$ for all $x \in \B^3$. Assuming that the function $f(x)$ can be defined with a unitary operator $U$ (which means that $U$ is 1 single quantum gate), by linearity of quantum operators when we apply $U$ to $\ket \psi$ we get that $$U \ket \psi = \sum_{x \in \B^3}{\alpha_x U \ket x}$$ Notice what happened here! By only applying $U$ to $\ket \psi$ we actually applied $U$ on \tit{all the vectors of the basis} \tbf{simultaneously}. This explains the choice of the labeling of the vectors of the canonical basis.

So, what's the catch? Well, we also recall that superpositions of states must be measured at some point, and this is the problem: when we measure the result of $U \ket \psi$ we will inevitably only get 1 single value, namely $U \ket x$ for some $x \in \B^n$, with probability $\abs{\alpha_x}^2$. This makes finding \tit{useful} quantum algorithm extremely difficult, because even if we are performing all the computations at once we can only see the result of 1 possible output, at random. Hence, not only quantum algorithms are hard to discover because of this inherent limitation of quantum mechanics, but they must also be more efficient than any classical alternative we currently know, otherwise there is really no point in using this very complicated computing framework (both in terms of hardware and software). The algorithms that we will see in this chapter --- and also in the next one --- are some of the most important quantum procedures that we know, and sparked a lot of interest in this area of research in recent years.

But before starting our discussion about quantum algorithm, we need to present another peculiarity of quantum computation. When we introduced quantum operators we underlined the fact that each quantum gate has to be a \tit{unitary} operator, and we now have the mathematical foundation to know that if a matrix is unitary, it is clearly also invertible --- indeed, its adjoint is its inverse. This directly implies a very important property of quantum computation: except for the measurement operation, every quantum computation operation is \tbf{reversible}.

Truth be precise, classical computation \tit{admits} reversible computation. In fact, we observe that we do have examples of reversible classical computaion that does not lose efficiency --- in terms of time. In 1963 \textcite{lecerf} proposed a reversible Turing machine, and in 1973 a landmark result by \textcite{bennett} proved that any standard Turing machine can be actually simulated by a reversible one --- his construction involves augmenting the Turing machine with an auxiliary \tit{history tape}, which can potentially lead to a large space overhead.

Consider the following bitwise operator $T$, that given three bits it works as follows: $$\funcmap{T}{\{0, 1\}^3}{\{0, 1\}^3}{(a, b, c)}{(a, b, c \oplus (a \land b))}$$ The corresponding classical gate of this bitwise operator is called \tbf{Toffoli gate}, and it has very special characteristics. First, observe how it computes: while $a$ and $b$ remains unchanged, $c$ is basically flipped if and only if both $a$ and $b$ are set to 1. In other words, both $a$ and $b$ act as \curlyquotes{control bits}over the \curlyquotes{target bit} $c$, indeed the Toffoli gate is sometimes called \tbf{Controlled Controlled NOT (CCNOT)}. As we did for the CNOT, this operator is clearly invertible since we are passing the bits $a$ and $b$ to the output too --- also, the truth table of the Toffoli gate easily shows that it is indeed invertible. Moreover, we observe that by associativity of the XOR it holds that $$(c \oplus (a \land b)) \oplus (a \land b) = c \oplus (a \land b) \oplus (a \land b) = c$$ which directly implies that $$T^2 = I \implies T = T^{-1}$$ However, above all the most important property of the Toffoli gate is that it is \tbf{universal}. In fact, even though gates like NAND and NOR are universal too, they are \tit{irreversible}. This implies that with the $T$ operator we can build any reversible Boolean function, and since any ordinary Boolean function can be embedded into a reversible one --- by adding extra bits to make it invertible --- any classical computation can be simulated using Toffoli gates only.

\section{Deutsch's algorithm}

Even though quantum computation provides reversibility \curlyquotes{for free}, we saw that classical computation can still achieve invertibility of computation. However, the next characteristic that we are going to describe has no classical analogue.

First, let's start with a problem seemingly unrelated to our discussion. Given a Boolean function $\func{f}{\{0, 1\}^n}{\{0, 1\}}$, we would like to embed $f$ inside a quantum computation. However, when $n \ge 3$ we are guaranteed that $f(x)$ is not reversible --- it cannot be injective. This is a problem, since in quantum coputing all gates must be reversible --- given that quantum evolution is unitary. Thus, how do we turn $f$ into a reversible computation?

We define a map $U_f$ defined as follows: $$\funcmap{U_f}{\{0, 1\}^{n + 1}}{\{0, 1\}^{n + 1}}{(x, y)}{(x, y \oplus f(x))}$$ First, we observe that $$(y \oplus f(x)) \oplus f(x) = y \oplus (f(x) \oplus f(x)) = y$$ which trivially proves that $U_f$ is reversible. Moreover, we can actually prove that when applied to qubits the corresponding quantum operator $$\funcmap{U_f}{\mathcal H}{\mathcal H}{\ket x \ket y}{\ket x \ket{y \oplus f(x)}}$$ is indeed unitary --- we observe that we are omitting the tensor product symbol in the function definition, as usual in the literature.

\begin{framedprop}{}
    Given a Boolean function $\func{f}{\{0, 1\}^n}{\{0, 1\}}$, the operator $U_f$ is unitary.
\end{framedprop}

\begin{proof}
    TODO \todo{TODO}
\end{proof}

This proves that the construction of $U_f$ is precisely the gate that allows us to embed $f$ into any quantum computation. Moreover, we observe that

\begin{itemize}
    \item $\ket y = \ket 0 \implies U_f \ket x \ket 0 = \ket x \ket{f(x)}$
    \item $\ket y = \ket 1 \implies U_f \ket x \ket 1 = \ket x \ket{\lnot f(x)}$
\end{itemize}

However, until now we only considered already collapsed qubits, but what if we consider a quantum input that is in a superposition? For instance, let $$\ket x = \dfrac{1}{\sqrt 2}(\ket 0 + \ket 1)$$ and assume that $\ket y = \ket 0$ for simplicity; this implies that
\begin{equation*}
    \begin{alignedat}{2}
        U_f \ket x \ket y & = U_f \dfrac{1}{\sqrt 2}(\ket 0 + \ket 1) \otimes \ket 0 & \\ 
                          & = U_f \dfrac{1}{\sqrt 2}(\ket{00} + \ket{10}) & \\ 
                          & = \dfrac{1}{\sqrt 2}(U_f \ket {00} + U_f \ket {10}) & \quad (\mbox{by linearity of $U_f$}) \\ 
                          & = \dfrac{1}{\sqrt 2}(\ket 0 \ket{f(0)} + \ket{1} \ket{f(1)}) & \\ 
    \end{alignedat}
\end{equation*}
But notice what just happened: both $f(0)$ and $f(1)$ have been computed \tbf{simultaneously}, in one gate application. This has no classical equivalent, we would have to evaluate $f(0)$ and $f(1)$ \tit{separately}. This phenomenon is called \tbf{quantum parallelism}, and it can be achieved only because:

\begin{itemize}
    \item qubits are in superpositions
    \item quantum gates are linear
\end{itemize}

However, we observe that the result of our calculations is \tit{still a superposition}. In fact, if we measure the output of $U_f \ket x \ket y$ we would still get either $\ket 0 \ket{f(0)}$ or $\ket 1 \ket {f(1)}$, both with 50\% probability. This is a problem: the fact that we can compute $f(0)$ and $f(1)$ at the same time seems promising, but can we retrieve their actual values?

Unfortunately, this is not possible. Indeed, quantum parallelism cannot help us with \tit{local} properties --- i.e. when we need all individual outputs --- it can only help when we need \tbf{global} properties. This limit derives from the fact that measurements prevent \curlyquotes{seeing} both outcomes, in fact if we were able to compute $f(0)$ and $f(1)$ simultaneously from this superposition we would be violating the laws of quantum mechanics themselves.

Then, how do we extract useful \tit{global} information from the superposition output? In 1985 \textcite{deutsch} defined a quantum algorithm which is able to compute $f(0) \oplus f(1)$, which clearly tells us if $f(0)$ equals $f(1)$ or not.

\begin{framedalgo}{Deutsch algorithm}
    Given a Boolean function $f$ and 2 qubits, the algorithm returns $\ket 0$ if $f$ if $f(0) = f(1)$, $\ket 1$ otherwise. \\
    \hrule

    \quad
    \begin{algorithmic}[1]
        \Function{Deutsch}{$f$, $q_0$, $q_1$}
            \State $q_1 \gets X(q_1)$
            \State $q_0, q_1 \gets (H \otimes H)(q_0, q_1)$
            \State $q_0, q_1 \gets U_f(q_0, q_1)$
            \State $q_0 \gets H(q_0)$
            \State \tbf{return} $\mbox{measure}(q_0)$
        \EndFunction
    \end{algorithmic}
\end{framedalgo}

\begin{figure}[H]
	\[
		\Qcircuit @C=3em @R=3em {
                  & \lstick{q_0} & \gate H & \qw & \multigate{1}{U_f} & \gate{H} & \meter & \cw \\ 
                  & \lstick{q_1} & \gate{X} & \gate H & \ghost{U_f} & \qw & \qw & \qw \\
		}
	\]
	\caption{The quantum circuit for Deutsch's algorithm. The box labeled with $U_f$ represents a \curlyquotes{black-box} for whatever computation $U_f$ represents (which directly depends on the chioce of $f$).}
\end{figure}

Proving that this quantum circuit is correct, however, will be a little more involved than what we did for the quantum teleportation. First, we need a lemma that will simplify our calculations.

\begin{framedlem}[label={U lemma}]{}
    For any Boolean function $f$ defined on $n$ bits, and $a \in \{0, 1\}^n$, it holds that $$U_f \ket a \otimes \dfrac{1}{\sqrt 2}(\ket 0 - \ket 1) = (-1)^{f(a)} \ket a \otimes \dfrac{1}{\sqrt 2}(\ket 0 - \ket 1)$$
\end{framedlem}

\begin{proof}
    First, by algebraic manipulation we see that
    \begin{equation*}
        \begin{split}
            U_f \ket a \otimes \dfrac{1}{\sqrt 2}(\ket 0 - \ket 1) & = U_f \dfrac{1}{\sqrt 2}(\ket{a0} - \ket{a1}) \\ 
                                                                   & = \dfrac{1}{\sqrt 2}(U_f \ket{a0} - \ket{a1}) \\ 
                                                                   & = \dfrac{1}{\sqrt 2}(\ket{a \ f(a)} - \ket{a \ \lnot f(a)}) \\ 
        \end{split}
    \end{equation*}
    and now, we observe that

    \begin{itemize}
        \item if $f(a) = 0$, then $$\dfrac{1}{\sqrt 2}(\ket{a0} - \ket{a1}) = (-1)^0 \ket{a} \otimes \dfrac{1}{\sqrt 2}(\ket 0 - \ket 1)$$
        \item if $f(a) = 1$, then $$\dfrac{1}{\sqrt 2}(\ket{a1} - \ket{a0}) = (-1)^1 \ket{a} \otimes \dfrac{1}{\sqrt 2}(\ket 0 - \ket 1)$$
    \end{itemize}
\end{proof}

We are now ready to prove the correctness of Deutsch's algorithm. To make things less cluttered, we will use the following standard notation: $$\ket + := \dfrac{1}{\sqrt 2}(\ket 0 + \ket 1) \quad \quad \ket - := \dfrac{1}{\sqrt 2}(\ket 0 - \ket 1)$$ In particular, we observe that $$H \ket 0 = \ket + \quad \quad H \ket 1 = \ket - $$ Moreover, we will omit the subscript of the corresponding qubit when the context is clear enough
\begin{equation*}
    \hspace{-0.7cm}
    \begin{alignedat}{2}
      & q_0 \otimes q_1 &  \\ 
        = & \ket 0 \otimes \ket 0 & \\ 
        \xrightarrow{X(q_1)} & \ket 0 \otimes \ket 1 & \\ 
        \xrightarrow{(H \otimes H)(q_0, q_1)} & \ket + \otimes \ket - &  \\ 
        = & \dfrac{1}{\sqrt 2}(\ket{00} - \ket{01} + \ket{10} - \ket{11}) & \\ 
        = & \dfrac{1}{\sqrt 2} \ket 0_0 \ket -_1  + \dfrac{1}{\sqrt 2} \ket 1_0 \ket -_1 & \\ 
        \xrightarrow{U_f(q_0, q_1)} & \dfrac{1}{\sqrt 2} (-1)^{f(0)} \ket 0_0 \ket -_1 + \dfrac{1}{\sqrt 2} (-1)^{f(1)} \ket 1_0 \ket -_1 & \quad (\mbox{by the lemma}) \\ 
        = & \dfrac{1}{\sqrt 2}\rbk{(-1)^{f(0)} \ket 0_0 + (-1)^{f(1)} \ket 1_0} \otimes \ket -_1 & \\ 
        \xrightarrow{H(q_0)} & \dfrac{1}{\sqrt 2}\rbk{(-1)^{f(0)} \ket +_0 + (-1)^{f(1)} \ket -_0} \otimes \sqrt 2 \ket -_1 & \\ 
        = & \dfrac{1}{2} \rbk{(-1)^{f(0)}(\ket 0 + \ket 1) + (-1)^{f(1)}(\ket 0 - \ket 1)} \otimes \sqrt 2 \ket -_1 & \\ 
        = & \dfrac{1}{2}\rbk{\rbk{(-1)^{f(0)} + (-1)^{f(1)}} \ket 0 + \rbk{(-1)^{f(0)} - (-1)^{f(1)}} \ket 1} \otimes \sqrt 2 \ket -_1 & \\ 
    \end{alignedat}
\end{equation*}
Now, since the final operation of the circuit involves measuring $q_0 = \alpha \ket 0 + \beta \ket 1$, the only two things that we care about are its probability amplitudes, namely $$\alpha = \dfrac{1}{2}\rbk{(-1)^{f(0)} + (-1)^{f(1)}}$$ $$\beta = \dfrac{1}{2}\rbk{(-1)^{f(0)} + (-1)^{f(1)}}$$ and we see that

\begin{itemize}
    \item if $f(0) = f(1)$, then $$(-1)^{f(0)} = (-1)^{f(1)} \implies \soe{l}{\alpha = \tfrac{1}{2}\rbk{2(-1)^{f(0)}} = (-1)^{f(0)} \\ \beta = 0}$$ which implies that $$q_0 = (-1)^{f(0)} \ket 0 + 0 \cdot \ket 1 = (-1)^{f(0)} \ket 0$$ and we can ignore the $(-1)^{f(0)}$ factor since its a global phase
    \item if $f(0) \neq f(1)$, then $$(-1)^{f(0)} = - (-1)^{f(1)} \implies \soe{l}{\alpha = 0 \\ \beta = \tfrac{1}{2}\rbk{2(-1)^{f(0)}} = (-1)^{f(0)}}$$ which implies that $$q_0 = 0 \cdot \ket 0 + (-1)^{f(0)} \ket 1 = (-1)^{f(0)} \ket 1$$ by the same reasoning as the other case
\end{itemize}

In the end, this proves that if $f(0) = f(1)$, $q_0$ will collapse to $\ket 0$, while if $f(0) \neq f(1)$ $q_1$ will colapse to $\ket 1$, proving that Deutsch's algorithm works correctly.

\section{Deutsch-Josza algorithm}

Even though Deutsch's algorithm is quite interesting and offers advantages that classical computation cannot achieve, still it seems like it wouldn't be very usefult in practice. In fact, usually we are interested in the \tit{values} of $f(0)$ and $f(1)$, and as we already mentioned quantum mechanics will not allow us to compute both the values at the same time --- meaning that even if we use Deutsch's algorithm to now whether $f(0)$ is equal to $f(1)$ or not, we would still need to compute at least one between $f(0)$ and $f(1)$ in order to know both values.

This is because, in reality, the algorithm that we are using is only solving a particular case of a more complex problem. In fact, a couple of years later \textcite{dj} realized that if we use $q_1 = \ket 1$ and $q_0 = \ket{0}^{\otimes n}$ (i.e. we use $n$ qubits set to $\ket 0$) this algorithm is actually able to tell \tbf{constant} and \tbf{balanced} functions apart.

\begin{frameddefn}{Constant function}
    A Boolean function $\func{f}{\{0, 1\}^n}{\{0,1\}}$ is said to be \tbf{constant} if $$\exists b \in \{0, 1\} \quad \forall x \in \{0, 1\}^n \quad f(x) = b$$
\end{frameddefn}

The definition of constant Boolean function has nothing special, and balanced functions are exactly what the name suggests, i.e. half of the inputs output 0 and the other half output 1, which can be succintly expressed as follows.

\begin{frameddefn}{Balanced function}
    A Boolean function $\func{f}{\{0, 1\}^n}{\{0,1\}}$ is said to be \tbf{balanced} if it holds that $$\sum_{x \in \{0, 1\}^n}{f(x)} = 2^{n - 1}$$
\end{frameddefn}

We observe that a Boolean function can be neither constant nor balanced, so this decision problem is actually a \tbf{promise problem}: given a Boolean function $f$ that is either constant or balanced --- note that it cannot be both --- decide if the function is constant or balanced. Indeed, we see that Deutsch's algorithm solved the same exact problem for $n = 2$: in fact, if $f(0) = f(1)$ it means that $f$ is constant, otherwise the latter is balanced.

Moreover, this problem actually shows the power of quantum parallelism more evidently: with a classical computation, to solve this decision problem we would need at most $$2^{n - 1} + 1 = O(2^n)$$ queries to $f$, instead our quantum computation still only requires \und{one} evaluation of $f$ to solve the problem.

\begin{framedalgo}{Deutsch-Josza algorithm}
    Given a Boolean function $f$ and $n + 1$ qubits, the algorithm returns $\ket{0}^{\otimes n}$ if $f$ is constant, $\ket 1$ otherwise. \\
    \hrule

    \quad
    \begin{algorithmic}[1]
        \Function{DeutschJosza}{$f$, $q_0$, $q_1$}
            \State $q_1 \gets X(q_1)$
            \State $q_0, q_1 \gets (H^{\otimes n} \otimes H)(q_0, q_1)$
            \State $q_0, q_1 \gets U_f(q_0, q_1)$
            \State $q_0 \gets H^{\otimes n}(q_0)$
            \State \tbf{return} $\mbox{measure}(q_0)$
        \EndFunction
    \end{algorithmic}
\end{framedalgo}

Note that in this algorithm $q_0$ are actually $n$ qubits, thus $q_0$ is initially set to $\ket{0}^{\otimes n}$. Before proving the correctness of this general version of the algorithm, let us first take a look at the quantum circuit that defines it.

\begin{figure}[H]
	\[
		\Qcircuit @C=3em @R=3em {
                  & \lstick{q_0^{\otimes n}} & \gate{H^{\otimes n}} & \qw & \multigate{1}{U_f} & \gate{H^{\otimes n}} & \meter & \cw \\ 
                  & \lstick{q_1} & \gate{X} & \gate H & \ghost{U_f} & \qw & \qw & \qw \\
		}
	\]
	\caption{The quantum circuit for the Deutsch-Josza algorithm.}
\end{figure}

\begin{framedprop}[label={H prop}]{}
    For any $x \in \B^n$ it holds that $$H^{\otimes n}\ket x = \dfrac{1}{\sqrt 2 ^n} \sum_{a \in \B^n}{(-1)^{x \cdot a} \ket a}$$
\end{framedprop}

\begin{proof}
    First, consider the following claim.

    \claim{
        $\forall a \in \B \quad H \ket a = \tfrac{1}{\sqrt 2} \sum_{b \in \B}{(-1)^{a \cdot b} \ket b}$.
    }{
        We observe that $$H \ket 0 = \ket + = \dfrac{1}{\sqrt 2}(\ket 0 + \ket 1) = \dfrac{1}{\sqrt 2} \sum_{b \in \B}{(-1)^{0 \cdot b} \ket b}$$ and analogously $$H \ket 1 = \ket - = \dfrac{1}{\sqrt 2}(\ket 0 - \ket 1) = \dfrac{1}{\sqrt 2} \sum_{b \in \B}{(-1)^{1 \cdot b} \ket b}$$
    }

    In the rest of the proof we will denote with the $\cdot$ symbol the \curlyquotes{canonical} scalar product, i.e. $$\forall x, y \in \B^n \quad x \cdot y := \sum_{i = 1}^n{x_i y_i}$$ Fix $x \in \B^n$; by the previous claim, we have that
    \begin{equation*}
        \begin{alignedat}{2}
            H^{\otimes n} \ket x & = \bigotimes_{i = 1}^n{H \ket{x_i}} & \\ 
                                 & = \bigotimes_{i = 1}^n{\rbk{\dfrac{1}{\sqrt 2} \sum_{b \in \B}{(-1)^{x_ib} \ket b}} \ket{x_i} }  & \quad (\mbox{by the claim}) & \\ 
                                 & = \dfrac{1}{\sqrt 2^n} \bigotimes_{i = 1}^n{\rbk{\ket 0 + (-1)^{x_i} \ket 1}} & \\ 
                                 & = \dfrac{1}{\sqrt 2^n} \sum_{a \in \B^n}{(-1)^{x \cdot a} \ket a } & \\ 
        \end{alignedat}
    \end{equation*}
\end{proof}
    
Finally, we are ready to prove the correctness of the algorithm.

\begin{equation*}
    \begin{alignedat}{2}
        & q_0 \otimes q_1 & \\ 
        = & \ket{0}^{\otimes n} \otimes \ket 0 & \\ 
        \xrightarrow{X(q_1)} & \ket{0}^{\otimes n} \otimes \ket 1 & \\ 
        \xrightarrow{H^{\otimes n}(q_0, q_1)} & H^{\otimes n} \ket{0}^{\otimes n} \otimes H \ket 1 & \\ 
        = & \dfrac{1}{\sqrt 2^n} \sum_{a \in \B^n}{(-1)^{0^n \cdot a} \ket a} \otimes \ket - & \quad (\mbox{by \cref{H prop}}) \\ 
        = & \dfrac{1}{\sqrt 2^n} \sum_{a \in \B^n}{\ket a} \otimes \ket -  & \\ 
        = & \dfrac{1}{\sqrt 2^n } \sum_{a \in \B^n}{(\ket a \otimes \ket - )} & \\ 
        \xrightarrow{U_f(q_0, q_1)} & \dfrac{1}{\sqrt 2^n} \sum_{a \in \B^n}{\rbk{(-1)^{f(a)} \ket a \otimes \ket - }} & \quad (\mbox{by \cref{U lemma}}) \\ 
        = & \dfrac{1}{\sqrt 2 ^n} \sum_{a \in \B^n}{(-1)^{f(a)} \ket a} \otimes \ket - & \\ 
        \xrightarrow{H^{\otimes n}(q_0)} & H^{\otimes n} \rbk{\dfrac{1}{\sqrt 2 ^n} \sum_{a \in \B^n}{(-1)^{f(a)} \ket a }} \otimes \ket - & \\ 
        = & \dfrac{1}{\sqrt 2 ^n} \sum_{a \in \B^n}{\rbk{(-1)^{f(a)} H^{\otimes n} \ket a }} \otimes \ket - & \\ 
        = & \dfrac{1}{\sqrt 2 ^n} \sum_{a \in \B^n}{(-1)^{f(a)} \rbk{\dfrac{1}{\sqrt 2^n} \sum_{b \in \B^n} {(-1)^{a \cdot b} \ket b}}} \otimes \ket - & \quad (\mbox{by \cref{H prop}}) \\ 
        = & \dfrac{1}{2^n} \sum_{a \in \B^n}{\sum_{b \in \B^n}{(-1)^{f(a) + a \cdot b}} \ket b } \otimes \ket - & \\ 
        = & \dfrac{1}{2^n} \sum_{a \in \B^n}{\sum_{b \in \B^n}{(-1)^{f(a) + a \cdot b}} \ket b } \otimes \ket - & \\ 
        = & \sum_{b \in \B^n}{\rbk{\dfrac{1}{2^n}\sum_{a \in \B^n}{(-1)^{f(a) + a \cdot b}}} \ket b_0} \otimes \ket -_1 & \\
    \end{alignedat}
\end{equation*}

Now note that this state describes the superposition of the system, but the next step of the algorithm will only measure $q_0$, therefore we can ignore $\ket -_1$ and just focus on the amplitudes of $q_0$. Then, by calling $$\forall b \in \B^n \quad \alpha_b := \dfrac{1}{2^n} \sum_{a \in \B^n}{(-1)^{f(a) + a \cdot b}}$$ we can rewrite $q_0$ as follows $$q_0 = \sum_{b \in \B^n}{\alpha_b \ket b}$$ Finally, since we want to determine the probability that $q_0$ collapses into the state $\ket{0}^{\otimes n}$ specifically, we can easily evaluate the associated amplitude of the latter, i.e. $$\alpha_{0^n} = \dfrac{1}{2^n}\sum_{a \in \B^n}{(-1)^{f(a)+ a \cdot 0^n}} = \dfrac{1}{2^n} \sum_{a \in \B^n}{(-1)^{f(a)}}$$ From this, we can easily conclude that:

\begin{itemize}
    \item if $f$ is constant, then $$\alpha_{0^n} = \dfrac{1}{2^n} \sum_{a \in \B^n}{(-1)^{f(a)}} = \dfrac{1}{2^n} \cdot 2^n \cdot (-1)^{b} = (-1)^b$$ where $b \in \B$, meaning that it is guaranteed that $q_0$ will collapse to $0^n$
    \item if $f$ is balanced, then $$\alpha_{0^n} = \dfrac{1}{2^n} \sum_{a \in \B}{(-1)^{f(a)}} = \dfrac{1}{2^n} \cdot 0 = 0$$ meaning that it is guaranteed that $q_0$ will \tit{not} collapse to $0^n$
\end{itemize}

\section{Grover's algorithm}

Even though Deutsch-Josza algorithm is able to solve the decision problem we described through \tit{one} single evaluation of $f$, it is fairly apparent that the problem their algorithm solves is quite artificial. However the next algorithm that we are going to discuss solves a problem that is definitely more useful.

In 1997, \textcite{grover} published a landmark paper called \curlyquotes{Quantum Mechanics Helps in Searching for a Needle in a Haystack}, and the name already suggests the problem his work tried to solve: the search problem. The setting is the following: we are given an array of $N$ elements --- we can assume that $N$ is always a power of 2 for some $n$, i.e. $N= 2^n$ --- that contains $M$ \curlyquotes{solution} elements. However, we do not know their positions, and the problem asks to find the index of any solution element.

More formally, given a Boolean function $$\funcmap{f}{\{0, \ldots, N - 1\}}{\B}{x}{\soe{ll}{1 & A[x] \in S \\ 0 & \mbox{otherwise}}}$$ where $A$ is our array, and $S$ is the set of solution elements, the problem asks to return an $x$ such that $f(x) = 1$, i.e. such that $A[x]$ is a solution.

With a classical computation, it is easy to see that we need $O \rbk{\tfrac{N}{M}}$ accesses to $A$ to solve our problem, however we will see that the algorithm Grover developed is able to return a \curlyquotes{solution index} in $O\rbk{\sqrt{\tfrac{M}{N}}}$ with \tit{high probability}, i.e. Grover's algorithm provides a \tbf{quadratic} speedup compared to any classical algorithm --- however, it is probabilistic.

Before explaining the details of the algorithm, we need to define some new operators that will be used in Grover's algorithm, and introduce some general notation:

\begin{itemize}
    \item given an arbitrary qubit $\ket \psi$, we will write its superposition of states as follows $$\ket \psi = \sum_{b \in \B^n}{\alpha_b \ket b}$$ where $\sum_{b \in \B^n}{\abs{\alpha_b}} = 1$
    \item we define an operator $O_f$ (where $f$ is the indicator function of the array defined before) that computes as follows: $$\forall x \in \B^n \quad O_f \ket x := (-1)^{f(x)} \ket x$$
    \item given an arbitrary qubit $\ket \psi$, we define a new operator $W$ as follows: $$W := 2 \ket s \bra s - I$$ where $\ket s$ is the \tbf{uniform superposition}, a superposition of states in which each amplitude is equally likely $$\ket s = \dfrac{1}{\sqrt N} \sum_{x \in \B^n}{\ket x}$$
    \item finally, we will define an operator $G$ that will simply compose the last two operators we described $$G = W \cdot O_f$$
\end{itemize}

\begin{framedalgo}{Grover's algorithm}
    Given a Boolean function $f$ that describes the solution elements of an array having $M$ solution elements, and $n$ qubits, the algorithm returns a solution index with high probability TODO SPIEGA CHE VUOL DIRE. \\
    \hrule

    \quad
    \begin{algorithmic}[1]
        \Function{Grover}{$f$, $q_0$}
            \State $q_0 \gets H^{\otimes n}(q_0)$
            \For{$i \in \sbk{1, O\rbk{\sqrt{\tfrac{N}{M}}}}$} \Comment{where $N = 2^n$}
                \State $q_0 \gets G(q_0)$
            \EndFor
            \State \tbf{return} $\mbox{measure}(q_0)$
        \EndFunction
    \end{algorithmic}
\end{framedalgo}

First of all, we see that this algorithm takes only $n$ qubits as input, however the actual implementation of the algorithm is slightly different, as shown in the following quantum circuit below.

\begin{figure}[H]
	\[
		\Qcircuit @C=3em @R=3em {
                  & \lstick{q_0^{\otimes n}} & \gate{H^{\otimes n}} & \multigate{1}{G} &  \multigate{1}{\cdots} & \multigate{1}{G} & \meter & \cw \\ 
                  & \lstick{q_1} & \qw & \ghost{G} & \ghost{\cdots}  & \ghost{G} & \qw & \qw \\
		}
	\]
	\caption{The quantum circuit for Grover's algorithm.}
\end{figure}

Indeed, we can see that the real quantum circuit takes $n + 1$ inputs, and the additional register is called \curlyquotes{ancilla} becuase its only purpose is to actually implement the $O_f$ operator, which is designed exactly as if it was the $U_f$ black-box we discussed in previous algorithms. This can be done thanks to \cref{U lemma}, which guarantees that if we give $\ket -$ as the second input to $U_f$ we get $$O_f \ket x \otimes \ket -  = (-1)^{f(x)} \otimes \ket - $$ which implies that the ancilla register will sill be $\ket -$, therefore we can just ignore the second register completely throughout the whole computation, and we are sure that $O_f$ computes correctly.

Furthermore, before proceeding, let us prove that $G$ is actually a unitary operator, i.e. that this is a valid quantum computation.

\claim[Claim]{
    The operator $G$ is unitary.
}{
    By \cref{unitary prod} we know that proving that $W$ and $O_f$ are unitary is sufficient to prove the claim. \todo{todo da finire}
}

Now that we know this operator is unitary, we can delve into the details of the algorithm. To see what happens at each iteration, we will describe the complete state of the system in a rather unusual way. Let $\ket a$ and $\ket b$ be the following superpositions: $$\ket a := \dfrac{1}{\sqrt{N - M}} \sum_{x \in \overline S}{\ket x} \quad \quad \ket b := \dfrac{1}{\sqrt M}\sum_{x \in S}{\ket x}$$ In other words, $\ket a$ is the uniform superposition of non-solution indices, and $\ket b$ is the superposition of solution ones. To explain the $\tfrac{1}{\sqrt M}$ factor in front of $\ket b$, we recall that in quantum mechanics all state vectors must be normalized, and since the squared norm of $\ket {\tilde b}$ (which will denote $\ket b$ without the multiplicative factor in front) is equal to
\begin{equation*}
    \begin{alignedat}{2}
        \braket{\tilde b| \tilde b} & = \rbk{\sum_{x \in S}{\ket x}}^\dag \rbk{\sum_{x \in S}{\ket x}} & \\ 
                                    & = \rbk{\sum_{x \in S}{\bra x}} \rbk{\sum_{y \in S}{\ket y}} & \\ 
                                    & = \sum_{x \in S}{\sum_{y \in S}{\braket{x|y}}} & \\ 
                                    & = \sum_{x \in S}{\sum_{y \in S}{\delta_{xy}}} & \quad (\mbox{basis states are orthonormal}) \\ 
                                    & = \sum_{x \in S}{1} & \\ 
                                    & = \abs{S} \\ 
                                    & = M
    \end{alignedat}
\end{equation*}
the norm of $\ket{\tilde b}$ is $\sqrt M$, hence to normalize it it suffices to add $\tfrac{1}{\sqrt M}$ in front of $\ket{\tilde b}$--- the reasoning for $\ket a$ is analogous. Moreover, it's easy to see that $\ket a$ and $\ket b$ are orthogonal, so they form an orthonormal basis for a 2D space.

Now, because of how we defined $\ket a$ and $\ket b$, we can rewrite the uniform superposition $$\ket s = \dfrac{1}{\sqrt N} \sum_{x \in \B}{\ket x}$$ as shown below $$\ket s = \sqrt{\dfrac{N - M}{N}} \ket a + \sqrt{\dfrac{M}{N}} \ket b$$ This is quite interesting, because it means that we can describe $\ket s$ in terms of $\ket a$ and $\ket b$. Let us do exactly this, and plot the resulting graph on a 2D space that has $\ket a$ and $\ket b$ as axis.

\centeredsvg{0.5}{../assets/grover1}

We observe that:

\begin{itemize}
    \item both $\ket s$, $\ket a $ and $\ket b$ are normalized, and all the vectors that we are going to consider are quantum states, so they will be normalized too, therefore we can restrict our focus on a 2D circumference of radius 1 --- this plane is usually called \tbf{Grover plane}
    \item since we expect that $M \ll N$, we have that $\sqrt{\tfrac{M}{N}} \ll \sqrt{\tfrac{N - M}{N}}$, which basically means that the vector $\ket s$ is almost parallel to $\ket a$ (the bigger is the numer of solution indices, the bigger the angle between $\ket s$ and $\ket a$)
\end{itemize}

Consider any state $\ket \psi = \sum_{x \in \B^n}{\alpha_x \ket x}$; we observe that
\begin{equation*}
    \begin{split}
        O_f \ket \psi & = O_f \sum_{x \in \B^n}{\alpha_x \ket x}  \\ 
                      & = \sum_{x \in \B^n}{\alpha_x O_f \ket x} \\ 
                      & = \sum_{x \in \B^n}{\alpha_x (-1)^{f(x)} \ket x} 
    \end{split}
\end{equation*}
which basically means that each time we apply the $O_f$ operator we are flipping the sign of the amplitudes of the compoents of $\ket \psi$ that represent solution indices. In other words, the $O_f$ operator flips its input w.r.t. $\ket a$.

Moreover, we observe that any state $\ket \psi$ can be decomposed into $$\ket \psi = \alpha \ket s + \beta \ket{\psi_\bot}$$ where $\alpha \ket s$ is the projection of $\ket \psi$ along $\ket s$'s space --- thus $\alpha = \braket{s|\psi}$ --- and $\ket {\psi_\bot}$ is the projection of $\ket \psi$ along the space that is orthogonal to $\ket s$'s. Therefore, we have that
\begin{equation*}
    \begin{split}
        W \ket \psi & = W(\alpha \ket s + \beta \ket{\psi_\bot}) \\ 
                    & = 2 \ket s \bra s (\alpha \ket s + \beta \ket{\psi_\bot}) - (\alpha \ket s + \beta \ket{\psi_\bot}) \\ 
                    & = 2 \alpha \ket s \braket{s|s} + 2 \beta \ket s \braket{s|\psi_\bot} - (\alpha \ket s + \beta \ket{\psi_\bot}) \\ 
                    & = 2 \alpha \ket s \cdot 1 + 0 - (\alpha \ket s + \beta \ket{\psi_\bot}) \\ 
                    & = \alpha \ket s - \beta \ket{\psi_\bot}
    \end{split}
\end{equation*}
which means that what $W$ actually performs is leaving the component along $\ket s$'s space unchanged, and it flips the sign of the component of the orthogonal space. In other words, what $W$ computes is the reflection of $\ket \psi$ w.r.t. $\ket s$.

We can finally describe Grover's algorithm in detail. First, we see that
\begin{equation*}
    \begin{split}
        & q_0 \\ 
        = & \ket{0}^{\otimes n} \\ 
        \xrightarrow{H^{\otimes n}(q_0)} & \dfrac{1}{\sqrt{2^n}}\sum_{x \in \B^n}{(-1)^{0^n \cdot x} \ket x} \\ 
        = & \dfrac{1}{\sqrt N} \sum_{x \in \B^n}{\ket x} \\ 
        = & \ket s
    \end{split}
\end{equation*}

Indeed, the only purpose of the first Hadamard operator is to \curlyquotes{move} the initial state slightly away from $\ket a$ --- and also making each component initially equally likelly. Now, let's see what happens at each application of the $G = W \cdot O_f$ operator:

\begin{itemize}
    \item as previously shown, the operator $O_f$ reflects its input w.r.t. the $\ket a$ axis --- below we show what happens when we first apply $O_f \ q_0 = O_f \ket s$: \centeredsvg{0.5}{../assets/grover2}
    \item additionally, as previously explained the operator $W$ reflects its input w.r.t. the axis described by the $\ket s$ vector, thus when we compute $W \cdot O_f \ q_0$ we will end up with the following vector: \centeredsvg{0.5}{../assets/grover3}
    \item this suggests that each time we apply the operator $G$ we are making $q_0$ closer and closer to $\ket b$, as depicted below: \centeredsvg{0.5}{../assets/grover4}
\end{itemize}

This is the core idea of Grover's algorithm: if we rewrite $q_0$ in terms of $\ket a$ and $\ket b$ $$q_0 = \beta_0 \ket a + \beta_1 \ket b$$ we get that through Grover's algorithm we transformed $q_0$ such that it is now very close to $\ket b$, meaning that $\beta_0 \ll \beta_1$. This direcly implies that when we will measure $q_0$ at the end of Grover's procedure the likelihood that it will collapse into some $\ket x$ that is a component of $\ket b$ --- i.e. a solution index --- is very high. In other words, what happens with Grover's algorithm is that we gradually increase the amplitudes of the solution indices, in order to maximize the probability that our qubit will collapse in one them when it will be measured at the end of the procedure.

Now that we know how Grover's algorithm works, the only thing left to discuss is the $O \rbk{\sqrt{\tfrac{N}{M}}}$ factor. Why is it guaranteed that after this amount of applications of the $G$ operator we are done with the algorithm? Well, we actually have the opposite problem: in reality, we have to \tit{stop early enough}. Consider again how Grover's algorithm operates in the Grover plane; clearly, if we apply $G$ too many times, what happens is that $q_0$ will end up past $\ket b$ itself: \todo{drawing}

Let the angle between $\ket a$ and $\ket s$ be $\theta$; since $O_f$ flips $q_0$ w.r.t. $\ket a$, and $W$ flips $O_f \ q_0$ w.r.t. $\ket s$, $G$ will cumulatively rotate $q_0$ by $2 \theta$: \todo{drawing} Indeed, with each application of $G$ we are rotating $q_0$ by $2 \theta$, which means that at the $k$-th application it holds that $$G^k \ q_0 = \cos (2k + 1) \theta \ket a + \sin (2k + 1) \theta \ket b$$ for any $k \in \N$, where the additional 1 comes from the fact that $q_0 = \ket s$ through the Hadamard transformation at the start of the process. Thus, to evaluate the optimal number of iterations we need to find the optimal $k$, i.e. the one that maximizes the probability of measuring a solution index, which is equal to the squared amplitude of $\ket b$, namely $$\Pr[\mbox{measure}(q_0) = \ket b] = \sin^2(2k + 1) \theta$$ Hence, we have that $\sin^2(2k + 1)\theta = 1$ when $(2k +1) \theta = \tfrac{\pi}{2}$, and solving for $k$ we get that $$k = \dfrac{\pi}{4 \theta} - \dfrac{1}{2}$$ Lastly, since $\theta$ is the angle between $\ket a$ and $\ket s$, we can rewrite $\ket s$ as $$\ket s = \cos \theta \ket a + \sin \theta \ket b$$ which directly implies that $$\sin \theta = \sqrt {\dfrac{M}{N}} \iff \theta = \arcsin \sqrt{\dfrac{M}{N}}$$ and therefore
\begin{equation*}
    \begin{split}
        k & = \dfrac{\pi}{4 \theta} - \dfrac{1}{2}  \\
          & = \dfrac{\pi}{4 \arcsin \sqrt{\dfrac{M}{N}}} - \dfrac{1}{2}  \\  
                                               & \le  \dfrac{\pi}{4 \arcsin \sqrt{\dfrac{M}{N}}} \\ 
                                               & \le \dfrac{\pi}{4 \sqrt{\dfrac{M}{N}}} \\ 
                                               & = \dfrac{\pi}{4} \sqrt{\dfrac{N}{M}} \\ 
                                               & = O \rbk{\sqrt{\dfrac{N}{M}}}
   \end{split}
\end{equation*}
which finally explains the quadratic speedup of Grover's algorithm w.r.t. the classical version of the problem.

As a final note, we observe that Grover's algorithm assumes that $\theta \le \tfrac{\pi}{4}$, otherwise we overshoot $\ket b$ with a single iteration of the $G$ operator --- since $\theta > \tfrac{\pi}{4}$ would mean that $\ket s$ is placed on the \curlyquotes{upper half} of the Grover plane. However, to ensure this constraint on $\theta$ we only need that $M \le \tfrac{N}{2}$, i.e. at most half of the elements in our array are solution elements. Indeed, we see that $$M \le \dfrac{N}{2} \implies \sin \theta = \sqrt{\dfrac{M}{N}} \le \sqrt{\dfrac{1}{2}} \implies \theta \le \arcsin \sqrt{\dfrac{1}{2}} = \dfrac{\pi}{4}$$ What can we do if the number of solutions is more than half the size of the array? We simply invert the problem and find the non-solutions!

\subsection{Another perspective}

Interestingly enough, we can look at what happens to the amplitudes of $q_0$ from another perspective. Usually, the $W$ operator is called \tbf{diffusion operator}, and what it does is computing the so called \tit{inversion about the mean} of its input. Consider any state $\ket \psi = \sum_{x \in \B^n}{\alpha_x \ket x}$, and observe that
\begin{equation*}
    \begin{alignedat}{2}
        \braket{s|\psi} & = \rbk{\dfrac{1}{\sqrt N} \sum_{y \in \B^n}{\ket y}}^\dag \rbk{\sum_{x \in \B^n}{\alpha_x \ket x}} & \quad (\mbox{since $\bra s = \ket s ^\dag$})\\ 
                        & = \rbk{\dfrac{1}{\sqrt N} \sum_{y \in \B^n}{\bra y}} \rbk{\sum_{x \in \B^n}{\alpha_x \ket x}} & \\ 
                        & = \dfrac{1}{\sqrt N} \sum_{y \in \B^n}{\sum_{x \in \B^n}{\alpha_x \braket{y|x}}} & \\ 
                        & = \dfrac{1}{\sqrt N} \sum_{y \in \B^n}{\sum_{x \in \B^n}{\alpha_x \delta_{xy}}} & \quad (\mbox{basis states are orthonormal}) \\ 
                        & = \dfrac{1}{\sqrt N} \sum_{x \in \B^n}{\alpha_x \sum_{y \in \B^n}{\delta_{xy}}} & \\ 
                        & = \dfrac{1}{\sqrt N} \sum_{x \in \B^n}{\alpha_x} & \\ 
                        & = \sqrt N \overline{\alpha}_\psi
    \end{alignedat}
\end{equation*}
where $\overline{\alpha}_\psi$ is the average amplitude of $\ket \psi$. This implies that
\begin{equation*}
    \begin{alignedat}{2}
        W \ket \psi & = (2 \ket s \bra s - I) \ket \psi & \\ 
                    & = 2 \ket s \braket{s|\psi} - \sum_{x \in \B^n}{\ket x} & \\  
                    & = 2 \ket s \rbk{\sqrt N \overline{\alpha}_\psi} - \sum_{x \in \alpha_x \ket x} & \quad (\mbox{for the previous observation}) \\ 
                    & = 2 \rbk{\dfrac{1}{\sqrt N} \sum_{y \in \B^n}{\ket y}} \sqrt N \overline{\alpha}_\psi - \sum_{x \in \B^n}{\alpha_x \ket x} & \\
                    & = \sum_{x \in \B^n}{\rbk{2 \overline{\alpha}_\psi - \alpha_x} \ket x} & \\ 
    \end{alignedat}
\end{equation*}
In other words, when we apply $W$ to a superposition of states, what happens is that each amplitude of the basis states is trasformed as follows: $$\func{W}{\alpha_x}{2 \overline{\alpha}_\psi - \alpha_x}$$ To understand why this is important, let's look at what happens in Grover's algorithm after the first Hadamard application. TODO \todo{da finire}

\subsection{Fixed-Point Quantum Search}

As we saw in the previous sections, with Grover's algorithm we need to be careful on how many times we apply the $G$ operator, however we observe that the right value for $k$ --- i.e. the numeber of iterations --- strictly depends on both $N$ and $M$. Assuming that $N$, the length of the array, is known, it is not guaranteed that we know $M$ too (the number of solutions in the array). Therefore, if we want to apply Grover's algorithm we also need to be able to to a rough estimate on $M$, otherwise we might end up stopping either too early or too late. This makes Grover's algorithm fragile in some real-world scenarios.

Grover was actually aware of this problem, and in 2005 he designed a new algorithm which is able to solve this issue \cite{grover2}. The idea of the algorithm is to \tit{monotonically} increase the probability of finding a solution, without oscillating back down $\ket a$. This algorithm is called \tbf{Fixed-Point Quantum Search}, becuase the solutions actually become a \curlyquotes{stable fixed point} of the transformation --- i.e. once you reach a good solution, further iterations leave it basically unchanged. Indeed, with Grover's search each iteration is a constant-angle rotation, while in fixed-point search we will see that the phase angles in each rotation changes such that the rotation angle decreases over time.

First, we need to define two new operators: $$R_s := I - (1 - e^{i \theta}) \ket s \bra s \quad \quad R_t := I - (1 - e^{i \theta}) \ket t \bra t$$ where $\ket s$ is the starting state and $\ket t$ is the target state (in Grover's algorithm this was $\ket b$) --- this is the original notation that Grover used in his paper, and actually explains why we used $\ket s$ in the previous version of the algorithm, it's just the \curlyquotes{start}. These two operators are called \tbf{phase shift operators}; as usual, before proceeding we need to show that both operators are actually unitary.

\claim{
    $R_t$ and $R_s$ are unitary operators.
}{
    TODO \todo{todo}
}

What are these two operators in the first place? When we presented the $W$ operator, we also noticed how it actually performes a reflection of its input w.r.t. the space of $\ket s$. Well, through a very similar argument it can be shown that $$W^\bot := I - 2 \ket s \bra s$$ performs a reflection of its input w.r.t. the space \tit{orthogonal} to $\ket s$ --- indeed, we end up with $$W^\bot \ket \psi = \beta \ket{\psi_\bot} - \alpha \ket s$$ If we now look at the $R_s$ operator, we can see that when we actually compute the reflection it performs we end up with $$R_s \ket \psi = \beta \ket{\psi_\bot} + e^{i \theta} \alpha \ket s$$ This suggests that what $R_s$ actually computes is a \curlyquotes{soft reflection} w.r.t. the space perpendicular to $\ket s$. Through an analogous argument, we can see that $$R_t \ket \psi= \beta \ket{\psi_\bot} + e^{i \theta} \alpha \ket t $$ meaning that $R_t$ computes a \curlyquotes{soft reflection} w.r.t. the space perpendicular to $\ket t$ --- however, we observe that the latter is literally the space of $\ket a$, indeed $O$ in Grover's algorithm could have been defined as $$O = I - 2 \ket b \bra b$$ and indeed it is sometimes defined such.

Now, we are going to define an addition operator called $U$ as such: let $U$ be any unitary operator such that, for some small $\varepsilon > 0$, it holds that $$\abs{\braket{t|Us}}^2 = 1 - \varepsilon$$ We observe that

\begin{itemize}
    \item by the laws of quantum mechanics we have that $$\Pr[\mbox{measure}\rbk{U \ket s} = \ket t] = \abs{\braket{t|Us}}^2$$ therefore we require $U$ to be an operator such that it \curlyquotes{drives $\ket s$ close to $\ket t$ with high probability} --- i.e. $1 - \varepsilon$
    \item we know that $U$ exists since it's just a rotation in a 2D space
    \item also, a geometric interpretation of the scalar product is that we are measuring the cosine of the angle between $\ket t$ and $\ket{Us}$, and we want this value to be very high (such that the angle woule be very small) --- we obeserve that we are in Hilbert spaces so the notion of \curlyquotes{angle} is not the same of the one we are used to with Euclidean spaces, but this is just to have an idea of what is happening with $U$
\end{itemize}

Moreover, define the following operator $$G := U R_s U^\dag R_t U$$ Let's see what happens when we evaluate $G \ket s$. By denoting $U_{ts} = \braket{t|Us}$, we can prove the following equality.

\begin{framedlem}{}
 It holds that $$G \ket s = U \ket s \sbk{e^{i\theta} + \abs{U_{ts}}^2  \rbk{e^{i \theta }-1}^2} + \ket t U_{ts}\rbk{e^{i \theta} - 1}$$
\end{framedlem}

\begin{proof}
    First, we prove the following equality.

    \claim{
        It holds that $\bra s U^\dag \ket t U_{ts} = \abs{U_{ts}}^2$.
    }{
        \begin{equation*}
            \begin{alignedat}{2}
                \bra s U^\dag \ket t U_{ts} & = \braket{s|U^\dag t} U_{ts} & \\ 
                         & = \braket{Us|t} U_{ts} & \quad (\mbox{by def. of adjoint}) \\ 
                         & = \overline{\braket{t|Us}} U_{ts} & \quad (\mbox{by prop. of scalar products}) \\ 
                         & = \overline{U_{ts}} U_{ts} & \\ 
                         & = \abs{U_{ts}}^2 & \quad (z \cdot \overline z = \abs{z}^2)\\ 
            \end{alignedat}
        \end{equation*}
    }

    Let $P_s = \ket s \bra s$ and $P_t = \ket t \bra t$; thus, we have that
    \begin{equation*}
        \begin{alignedat}{2}
            G & = U R_s U^\dag R_t U & \\ 
              & = U(I - (1 - e^{i\theta})P_s)U^\dag(I - (1 - e^{i\theta})P_t)U & \\ 
              & = (U - (1 - e^{i\theta})UP_s)U^\dag(U - (1 - e^{i\theta})UP_t) & \\ 
              & = (UU^\dag - (1 - e^{i\theta}) UP_sU^\dag)(U - (1 - e^{i\theta})UP_t) & \\ 
              & = (I - (1 - e^{i \theta})UP_sU^\dag)(U - (1 - e^{i\theta})UP_t) & \quad (UU^\dag = I) \\ 
              & = U - (1 - e^{i \theta}) P_tU - (1 - e^{i \theta})UP_s + (1 - e^{i\theta})^2 UP_s U^\dag P_t U & \\ 
        \end{alignedat}
    \end{equation*}
    Therefore, we find that $G \ket s$ can be evaluated as follows:
    \begin{equation*}
        \hspace{-1cm}
        \begin{alignedat}{2}
            G \ket s & = \rbk{U - (1 - e^{i \theta}) P_tU - (1 - e^{i \theta})UP_s + (1 - e^{i\theta}) UP_s U^\dag P_t U} \ket s & \\ 
                     & = U \ket s - (1 - e^{i \theta}) P_tU \ket s- (1 - e^{i \theta})UP_s \ket s + (1 - e^{i\theta})^2 UP_s U^\dag P_t U \ket s & \\  
                     & = U \ket s - (1 - e^{i \theta}) \ket t U_{ts} - (1 - e^{i\theta}) U \ket s + (1 - e^{i\theta})^2 U \ket s \bra s U^\dag \ket t U_{ts} & \\ 
                     & = U \ket s - (1 - e^{i \theta}) \ket t U_{ts} - (1 - e^{i\theta}) U \ket s + (1 - e^{i\theta})^2 U \ket s \abs{U_{ts}}^2 & \quad (\mbox{by the claim}) \\ 
                     & = \ldots & \quad (\mbox{algebraic manipulation}) \\ 
                     & = U \ket s \sbk{e^{i\theta} + \abs{U_{ts}}^2  \rbk{e^{i \theta }-1}^2} + \ket t U_{ts}\rbk{e^{i \theta} - 1} & \\ 
        \end{alignedat}
    \end{equation*}
\end{proof}

Most importantly, this equality can be used in the following proposition, which shows that we can actually find an angle $\theta$ for which the distance from $\ket t$ decreases significantly.

\begin{framedprop}{}
    There exists an angle $\theta$ such that $$\Pr[\mbox{measure}(G \ \ket s) = \ket t] = 1 - \varepsilon^3$$
\end{framedprop}

\begin{proof}
    Thanks to the previous lemma, we obtain that
    \begin{equation*}
        \hspace{-1cm}
        \begin{alignedat}{2}
            \Pr[\mbox{measure}\rbk{G \ket s} = \ket t] & = \abs{\braket{t|Gs}}^2 & \\ 
                                                       & = \abs{\bra t \sbk{U \ket s \rbk{e^{i\theta} + \abs{U_{ts}}^2  \rbk{e^{i \theta} - 1}^2} + \ket t U_{ts}\rbk{e^{i \theta} - 1}}}^2 & \\ 
                                                       & = \abs{\braket{t|Us} \rbk{e^{i\theta} + \abs{U_{ts}}^2  \rbk{e^{i \theta } - 1}^2 }+ \braket{t|t} U_{ts}\rbk{e^{i \theta} - 1}}^2 & \\ 
                                                       & = \abs{U_{ts} \rbk{e^{i\theta} + \abs{U_{ts}}^2  \rbk{e^{i \theta } - 1}^2 }+ U_{ts}\rbk{e^{i \theta} - 1}}^2 & \\ 
                                                       & = \abs{U_{ts} \sbk{2e^{i\theta} - 1 + \abs{U_{ts}}^2 \rbk{e^{i \theta} - 1}^2}}^2 & \\ 
                                                       & = \abs{U_{ts}}^2 \cdot \abs{2e^{i\theta} - 1 + \abs{U_{ts}}^2 \rbk{e^{i \theta} - 1}^2}^2 & \\ 
                                                       & = (1 - \varepsilon) \cdot \abs{2e^{i\theta} - 1 + (1 - \varepsilon)\rbk{e^{i \theta} - 1}^2}^2 & \quad \rbk{\abs{U_{ts}}^2 = 1 - \varepsilon}\\ 
        \end{alignedat}
    \end{equation*}

    Now, let's see what happens if we set $\theta = \tfrac{\pi}{3}$: we obtain that
    \begin{equation*}
        \begin{alignedat}{2}
            \Pr[\mbox{measure}\rbk{G \ket s} = \ket t] & = (1 - \varepsilon) \cdot \abs{2e^{i \tfrac{\pi}{3}} - 1 + (1 - \varepsilon)\rbk{e^{i\tfrac{\pi}{3}} - 1}^2}^2 & \\ 
                                                       & = (1 - \varepsilon) \cdot \abs{2e^{i \tfrac{\pi}{3}} - 1 - (1 - \varepsilon){i \tfrac{\pi}{3}}}^2 & \quad \rbk{\rbk{e^{i\tfrac{\pi}{3}} - 1}^2 = - e^{i \tfrac{\pi}{3}}} & \\ 
                                                       & = (1 - \varepsilon) \cdot \abs{(1 + \varepsilon) e^{i \tfrac{\pi}{3}} - 1}^2 & \\ 
                                                       & = (1 - \varepsilon) \cdot \abs{(1 + \varepsilon)\rbk{\dfrac{1}{2} + i \dfrac{\sqrt 4}{2}} - 1}^2 & \quad (\mbox{by Euler's formula}) & \\ 
                                                       & = (1 - \varepsilon) \cdot \abs{\dfrac{1}{2} +i  \dfrac{\sqrt 3}{2} + \dfrac{\varepsilon}{2} + i \dfrac{\sqrt 3}{2} \varepsilon - 1}^2 & \\ 
                                                       & = (1 - \varepsilon) \cdot \abs{\dfrac{\varepsilon}{2} - \dfrac{1}{2} + i \dfrac{\sqrt 3}{2}(1 + \varepsilon)}^2 &  \\ 
                                                       & = (1 - \varepsilon) \cdot \sbk{\rbk{\dfrac{\varepsilon}{2} - \dfrac{1}{2}}^2 + \rbk{\dfrac{\sqrt 3}{2}(1 + \varepsilon)}^2 } & \quad \rbk{\abs{z}^2 = \Re^2(z) + \Im^2(z)} \\ 
                                                       & = (1 - \varepsilon) \cdot (1 + \varepsilon + \varepsilon^2)&  \\ 
                                                       & = 1 + \varepsilon + \varepsilon^2 - \varepsilon - \varepsilon^2 - \varepsilon^3 & \\ 
                                                       & = 1 - \varepsilon^3 & \\ 
        \end{alignedat}
    \end{equation*}

This shows that by applying $G$ the probability of measuring $\ket t$ has increased from $1 - \varepsilon$ to $1 - \varepsilon^3$.
\end{proof}

Indeed, it can be shown that by defining the following recursive sequence of operators $$\soe{ll}{U_0 := U & m = 0 \\ U_m = U_{m - 1} R_s U_{m-1}^\dag R_t U_{m - 1} & m \ge 1}$$ we get that $$\Pr[\mbox{measure}(\ket{U_m} \ket s) = \ket t] = \abk{\braket{t|U_ms}}^2 = 1 - \varepsilon^{2{q_m} + 1}$$ where $q_m$ is the number of queries of $f(x)$.

TODO \todo{drawing of the vector that grows monotonically}

Unfortunately, there already exists a classical probabilistic algorithm that the failure probability drops as $\varepsilon^{q + 1}$  after $q$ queries to $f$. Thus, the quantum advantage with this method is lost.

So, what do we do now? In 2014 \textcite{yoder} proposed a fixed-point quantum serach algorithm that monotonically converges to the target state while still retaining the quadratic advantage of the original Grover's algorithm over classical algorithms. The algorithm involves phase-shift operators that are parametrized with angles different from $\theta = \tfrac{\pi}{3}$, and again involes building a sequence of operators using said phase shifts. However, the details of this result are way beyond the scope of our discussion, so we won't describe the details of their findings.
