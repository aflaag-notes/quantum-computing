\chapter{Quantum Error Correction}

TODO \todo{introduction}

\section{Repetition codes}

\subsection{Three bits bit flip code}

Before discussing error-correction in quantum contexts, let's present what how the classical world handles noise in communications. Suppose we have two parties, Alice and Bob, and say that Alice wants to send Bob some bit $b \in \B$. However, the channel they has at their disposal is \tbf{noisy}, meaning that with some probability $p$ the bit she sends will be flipped and Bob will receive the wrong information --- we will assume that each flip is independent of each other. How can Alice raise the odds of Bob getting the bit she originally sent? What she needs is some type of \tit{redundancy}, and the most straightforward and way to achieve it is clearly \tbf{repetition}.

Therefore, suppose Alice now sends 2 bits to Bob instead of one, such that both bits are equal to the original $b$ she intended to sent $$b = b_0 = b_1$$ Her idea is to make Bob able to recover the original $b$ by looking at both $b_0$ and $b_1$. However, there is an issue with this idea: since both $b_0$ and $b_1$ could flip during the transmission, its easy to see that if $b_0 \neq b_1$ Bob has no way to determine what $b$ was. This suggests that what Alice needs to provide is \tit{more} information!

Suppose that Alice now sends $b_0$, $b_1$ and $b_2$ such that $$b = b_0 = b_1 = b_2$$ and for now, let's assume that $b_0$ flipped during the transmission. Then, Bob will see that $$b_0 \neq b_1 = b_2$$ which ultimately helps him determine that the value Alice wanted to send is contained in the bits $b_1$ and $b_2$, and $b_0$ flipped because of the noise. For a more practical example, if Alice wants to send 0 to Bob she transmits 000 through the noisy medium, but whenever Bob receive 100 he will infer that what Alice actually sent was 000, therefore the bit he was meant to receive was 0.

This idea is called \tit{majority voting}, as Bob decides how to recover the bit by looking at the value of the bits that appears more often, however its easy too see a very big flaw of this approach: with 2 or more bit flips the majority voting fails! If Bob receives 110, by majority he will infer that we was meant to receive 1 from Alice, however he cannot be sure that what really happened is that both $b_0$ and $b_1$ flipped through the transmission and what Alice originally sent was 0. In general, we have that
\begin{equation*}
	\begin{split}
		\Pr[\mbox{recovering the wrong bit}] & = \Pr[\mbox{at least 2 two bits flipped}]                 \\
		                                     & = \Pr[\mbox{2 bits flipped}] + \Pr[\mbox{3 bits flipped}] \\
		                                     & = 3p^2(1 - p) + p^3                                       \\
		                                     & = 3p^2 - 3p^3 + p^3                                       \\
		                                     & = 3p^2 - 2p^3
	\end{split}
\end{equation*}
This means that as long as $$3p^2 - 2p^3 < p \iff p < \dfrac{1}{2}$$ this error-correction code improves reliability. The codes of these type are called \tbf{repetition codes}, as they rely on repeating the same information multiple times for increase the probability of transmitting the correct information.

\subsection{Three qubits bit flip code}

Can we replicate the same idea in a quantum setting? More specifically, suppose that Alice wants to send a qubit $$\ket \psi = \alpha \ket 0 + \beta \ket 1$$ to Bob through a quantum channel, however said channel is noisy and might introduce errors with probability $p$ at each transmission they perform. Because of the peculiarities of the quantum worlds, we know there are some important differences between classical and quantum information that require new ideas in order to apply the same redundancy technique presented:

\begin{itemize}
	\item first and foremost, the \nameref{nct} forbids the cloning of a quantum state multiple times, so it is not as straightforward as in the classical context to produce the needed redundancy
	\item in the quantum context values are \tbf{continuous}, and errors are continuous as well, meaning that determining which error occurred would appear to require infinite precision --- more details about this will be discussed later
	\item in classical error-correction Bob can just check the values of the received bits and that's it, however upon receiving qubits Bob cannot simply \tbf{measure} them to \curlyquotes{look at their value}, otherwise he would destroy all the information making the recovery impossible
\end{itemize}

Fortunately, we can circumvent all of these problems with some clever ideas. First, consider the following quantum circuit:

\begin{figure}[H]
	\[
		\Qcircuit @C=2.5em @R=2.5em {
		& \lstick{\ket \psi}    & \ctrl{1}  & \ctrl{2} & \qw \\
		& \lstick{\ket 0}       & \targ  & \qw & \qw      \\
		& \lstick{\ket 0}       & \qw    & \targ   & \qw \\
		}
	\]
	% \caption{The circuit.}
\end{figure}

Let's analyze what this circuit does:

\begin{equation*}
	\begin{split}
		                                    & \ket \psi \otimes \ket 0 \otimes \ket 0                                   \\
		=                                   & (\alpha \ket 0 + \beta \ket 1) \otimes \ket 0 \otimes \ket 0              \\
		=                                   & (\alpha \ket {00} + \beta \ket {10}) \otimes \ket 0                       \\
		\xrightarrow{\mbox{CNOT}(q_0, q_1)} & \mbox{CNOT}(\alpha \ket {00} + \beta \ket {10}) \otimes \ket 0            \\
		=                                   & (\alpha \mbox{CNOT} \ket{00} + \beta \mbox{CNOT}\ket{10} ) \otimes \ket 0 \\
		=                                   & (\alpha \ket {00} + \beta \ket{11}) \otimes \ket 0                        \\
		\xrightarrow{\mbox{CNOT}(q_0, q_2)} & \alpha \ket{000} + \beta \ket{111}                                        \\
	\end{split}
\end{equation*}

This is exactly what we needed: we started with a state $\ket \psi = \alpha \ket 0 + \beta \ket 1$ and we ended up with $$\ket{\hat \psi} = \alpha \ket{000} + \beta \ket{111}$$ which yields the redundancy we need in order to utilize the repetition strategy.

Now, suppose that Alice sends each of the three qubits through the noisy quantum channel, and each qubit will experience the effect of the noise, independently, with probability $p$. But what is this effect in practice? So far, we did not mention the word \tit{flip}, because we actually need to define what a \tit{flip} even is in this context. Well, if a \curlyquotes{classical bit flip} is an application of a NOT operator on some bit $b_i$, it makes sense to define a \curlyquotes{quantum bit flip} as the application of the $X$ operator of some $q_i$ sent, analogously.

However, this immediately shows that there is a \tbf{continuous spectrum} of possible errors that can occur to the qubits --- namely every possible linear transformation --- and apart from the $X$ gate itself none of them have a classical analogue! For now, let's just focus on the $X$ operator, and suppose that \tit{at most one} bit flip occurred on the qubits Alice sent to Bob. How can Bob recover the original message? Consider the following table:

\begin{center}
	\begin{tabular}{l|l}
		\hline
		Projector                                         & Error occured      \\
		\hline\hline
		$P_0 = \ket{000} \bra{000} + \ket{111} \bra{111}$ & No error occurred  \\
		\hline
		$P_1 = \ket{100} \bra{000} + \ket{011} \bra{011}$ & First bit flipped  \\
		\hline
		$P_2 = \ket{010} \bra{010} + \ket{101} \bra{101}$ & Second bit flipped \\
		\hline
		$P_3 = \ket{001} \bra{001} + \ket{110} \bra{110}$ & Third bit flipped  \\
		\hline
	\end{tabular}
\end{center}

This table contains 4 projectors that Bob can apply to discover which qubit, if any, flipped during the transmission. We underline that Bob \tit{must} use these projectors in order to understand which bit flip happend, because he cannot measure what he received. For instance, say that the first bit flipped during the transmission, i.e. Bob receives $$\ket{\hat \psi} = \alpha \ket{100} + \beta \ket{011}$$ Then, when he applies $P_1$ to $\ket{\hat \psi}$ he discovers that
\begin{equation*}
	\begin{split}
		\braket{\hat \psi|P_1 \hat \psi} & = \braket{\hat \psi|(\ket{100} \bra{000} + \ket{011} \bra{011})(\alpha \ket{000} + \beta \ket{111})} \\
		                                 & = \braket{ \hat \psi| \alpha \ket{100} + \beta \ket{011}}                                            \\
		                                 & = \braket{\hat \psi | \hat \psi}                                                                     \\
		                                 & = 1                                                                                                  \\
	\end{split}
\end{equation*}
This means that Bob is sure that the second bit flipped, hence the original message can be recovered flawlessly by flipping the second qubit received. Again, this error-correction procedure works perfectly, provided that bit flips occur on at most one qubit per message, so reliability still requires that $p < 1/2$.

TODO \todo{add slide 14 and 15}

\subsection{Three qubits phase flip code}

As previously mentioned, the case of the $X$ gate is \curlyquotes{easy} to solve, in the sense that it requires no significant innovation w.r.t. any classical context. However, what if instead of performing an application of the $X$ operator with probability $p$, our noisy channel applies the $Z$ transformation instead? When Alice sends her qubit $\ket \psi = \alpha \ket 0 + \beta \ket 1$ it is transformed with probability $p$ into $$Z \ket \psi = \alpha \ket 0 - \beta \ket 1$$ As previously mentioned, this scenario has no classical analogue, but it is still easy to handle. In fact, we already know a key fact: the $Z$ operator acts like a standard bit flip in the $Z$ basis, i.e. $$Z \ket +  = \ket - \quad \quad Z \ket - = \ket +$$ This suggests that we can still employ the same redundancy strategy of the previous section, provided that we perform a change of basis. Luckily, we already know a matrix that performs the change from the X to the Z bases, as discussed in \cref{measurements}, namely the Hadamard operator! Therefore, all Alice has to do is send the following:

\begin{figure}[H]
	\[
		\Qcircuit @C=2.5em @R=2.5em {
		& \lstick{\ket \psi}    & \ctrl{1}  & \ctrl{2} & \gate{H} & \qw \\
		& \lstick{\ket 0}       & \targ  & \qw & \gate{H} & \qw      \\
		& \lstick{\ket 0}       & \qw    & \targ   & \gate{H} & \qw \\
		}
	\]
	% \caption{The circuit.}
\end{figure}

meaning that at the end of the circuit we will get exactly:
\begin{equation*}
	\begin{split}
		                                           & \ket \psi \otimes \ket 0 \otimes \ket 0                        \\
		\xrightarrow{\ldots}                       & \ldots                                                         \\
		=                                          & \alpha \ket{000} + \beta \ket{111}                             \\
		\xrightarrow{H^{\otimes 3}(q_0, q_1, q_2)} & H^{\otimes 3}(\alpha \ket{000} + \beta \ket{111})              \\
		=                                          & \alpha H^{\otimes 3} \ket{000} + \beta H^{\otimes 3} \ket{111} \\
		=                                          & \alpha \ket{+++} + \beta \ket{---}                             \\
	\end{split}
\end{equation*}

Then, Bob can discover if a qubit phase flip occured through the following table:
\begin{center}
	\begin{tabular}{l|l}
		\hline
		Projector                                         & Error occured        \\
		\hline\hline
		$P_0 = \ket{+++} \bra{+++} + \ket{---} \bra{---}$ & No error occurred    \\
		\hline
		$P_1 = \ket{-++} \bra{-++} + \ket{+--} \bra{+--}$ & First phase flipped  \\
		\hline
		$P_2 = \ket{+-+} \bra{+-+} + \ket{-+-} \bra{-+-}$ & Second phase flipped \\
		\hline
		$P_3 = \ket{++-} \bra{++-} + \ket{--+} \bra{--+}$ & Third phase flipped  \\
		\hline
	\end{tabular}
\end{center}

Once Bob learns which qubit phase flipped (if any) he can apply a $Z$ operator to the corresponding qubit. We say that the channel that applies the $X$ matrix and the one that applies the $Z$ martix are \tbf{unitarily equivalent}, since there is a unitary operator --- the Hadamard gate, in this case --- such that the action of one channel is the same as the other, provided that the first channel is preceded by $U$ and followed by $U^\dag$ in every operation performed --- we will leave the proof of this fact as an exercise.

TODO \todo{i have no idea what slide 19 is saying}

\subsection{The Shor code}

The last quantum repetition code we will present is called \tbf{Shor code}, after its inventor, and it is a simple code that protecs againts the effects of at most one \tit{arbitrary Pauli error} on any qubit. Not surprisingly, the code is a combination of the three qubit bit and phase flip codes we saw earlier. In particular, we observe that in the bit flip code the quantum circuit acted on $\ket 0$ and $\ket 1$ as follows: $$\ket 0 \xrightarrow{\mbox{bit flip code}} \ket{000}$$ $$\ket 1 \xrightarrow{\mbox{bit flip code}} \ket{111}$$ Similarily, the circuit for the phase flip acted as follows: $$\ket 0 \xrightarrow{\mbox{phase flip code}} \ket{+++}$$ $$\ket 1 \xrightarrow{\mbox{phase flip code}} \ket{---}$$ Then, the Shor code applies the following transformation: $$\ket 0 \xrightarrow{\mbox{phase flip code}} \ket{+++} \xrightarrow{\mbox{bit flip code}} \dfrac{1}{\sqrt 8}(\ket{000} + \ket{111})^{\otimes 3}$$ $$\ket 1 \xrightarrow{\mbox{phase flip code}} \ket{---} \xrightarrow{\mbox{bit flip code}} \dfrac{1}{\sqrt 8}(\ket{000} - \ket{111})^{\otimes 3}$$ Indeed, this code is able to detect any Pauli error on any qubit, meaning that it can detect if either

\begin{itemize}
	\item a bit flip
	\item a phase flip
	\item both a bit and a phase flip
\end{itemize}

occurred on at most one transmitted qubit --- and it also works by switching the order of the concatenated codes. This is because

\begin{itemize}
	\item bit flips correspond to $\sigma_x$
	\item phase flip correspond to $\sigma_z$
	\item $\sigma_y = i \sigma_x \sigma_z$, so we can correct $Y$ transformations as well
\end{itemize}

Therefore, we just need to apply the procedures for detecting bit and phase flips one after the other in order to retrieve the original message.

We note that the fact that the Shor code actually enables the correction of combined bit and phase flip errors on a single qubit suggests that we could use the Shor code for more complicated type of errors. However, we might expect some additional work to be done in order to be protected against \tbf{arbitrary errors}, however quite suprisingly we can actually prove that this code detects any possible error with \tit{no additional work required}! At the beginning of our discussion we said that there is a \tit{continuum} of errors that may occur on a single qubit, which may seem to imply that we need much more sophisticated quantum error-correction procedures, however such continuum can actually be handled by correcting only a \tit{discrete} subset of it. All the other possible errors are corrected automatically. The discretization of the errors is central to why quantum error-correction works, and should be regarded in contrast to classical error-correction for analog systems, where no such discretization of errors is possible.

\begin{framedthm}{}
	Any quantum error-correcting code that corrects at most $k$ Pauli errors on at most $k$ qubits will also correct an arbitrary quantum operation on thos qubits.
\end{framedthm}

\begin{proof}[Idea of the proof.]
	TODO \todo{pag 434 of the book}
\end{proof}

\section{Linear codes}

Let's go back to the classical scenario for a moment. So far we only discussed codes that involve repetition of the same information multiple times in order to recover the original message. However, we saw how the repetition codes we presented fail whenever the number of error is two or more. In particular, this occurs because the majority rule cannot be applied reliably anymore. Nevertheless, one might suggest that by simply sending \tit{even more} repetition of the same information we could increase the chances of recovering the original message, which is indeed true. However, a \curlyquotes{good} error-correcting code should also be able to provide a good balance between the transmitted bits and the redundancy bits.

Let's give some terminology: each message sent through an error-correcting code is called \tbf{codeword}, and they are assumed to have always the same length --- for instance, in the repetition code we analyzed previously the length of each codeword was 3. Then, we have the following definition.

\begin{frameddefn}{Code rate}
	Given an error-correcting code that has codewords of length $n$, such that each codeword contains $k$ \tit{information symbols} and $n - k$ \tit{reduncancy symbols}, we define the \tbf{code rate} of the code as $$R = \dfrac{k}{n}$$
\end{frameddefn}

It's easy to see that a \curlyquotes{good} code is required to have $R$ as close to 1 as possible, in order to reduce the number of \tit{redundancy symbols}. However, any repetition-based error-correcting code has a rate significantly smaller than 1, in fact as the number of repetitions required to correct more errors increases, the rate approaches 0! In this section we present a more rubust type of error-correcting codes, called \tbf{linear codes}.

\begin{frameddefn}{Linear code}
	A \tbf{linear code} encoding $k$ bits of information into an $n$ bit code space --- written as $[n, k]$ --- is an error-correcting code specified by a matrix $G \in \Z_2^{n \times k}$ called \tbf{generator matrix}, such that each $k$ bit message $x$ is encoded as follows $$\funcmap{\mbox{Enc}}{\Z_2^{k \times 1}}{\Z_2^{n \times 1}}{x}{Gx}$$ where the columns of $G$ are linearly independent.
\end{frameddefn}

For instance, consider the following generator matrix: $$G = \smat{1 \\ 1 \\ 1}$$ This matrix has dimensions $3 \times 1$, so it expects inputs of size $1 \times 1$ and outputs codewords of size $3 \times 1$. In fact, a keen eye might have noticed that this matrix performs the three repetition code we saw earlier: $$\smat{1 \\ 1 \\ 1} \smat{b} = \smat{1 \cdot b \\ 1 \cdot b \\ 1 \cdot b} = \smat{b \\ b \\ b}$$ As said earlier, this is an example of a $[3, 1]$ code.

We observe that in the definition we used $\Z_2$ instead of $\B$ in order to highlight the fact that all the operations are performed modulo 2. Moreover, the set of possible codewords for a code corresponds to the vector space spanned by the columns of its generator matrix $G$, so for all the messages to be uniquely encoded we require that the columns of $G$ are linearly independent. Lastly, we will identify a linear code with the codewords that it can generate, so a code $C$ of type $[n, k]$ can be written as $$C := \{Gx \mid x \in \Z_2^k\}$$ where $G$ is the generator matrix of $C$.

A great advantage of linear codes over more general error-correcting codes is their compact specification: a general code encoding $k$ information bits into $n$ bits requires $2^k$ codewords, each of length $n$, for a total of $n 2^k$ bits needed to provide a description of the entire code. With a linear code instead, we only need to specify the $nk$ bits of the generator matrix, and that's it. This is an exponential saving in the amount of memory required!

Now that we know what linear codes are, we can proceed to describe how to perform error-correction with them. In fact, in the repetition codes we just had to use a majority rule and that was enough to retrieve the origianl message, however in this case the error-correction procedure is not as intuitive. But first, we need to provide a different definition of linear codes.

\begin{framedthm}{Linear code (alt. def.)}
	Given an $[n, k]$ generator code $C$ it holds that $$C = \ker H$$ where $H \in \Z_2^{(n - k) \times n}$ is the \tit{parity check matrix} of $C$.
\end{framedthm}

\begin{proof}
	Let $G$ be the generator matrix of $C$; pick $n - k$ linearly independent vectors $y_1, \ldots, y_{n - k}$ that are orthogonal to the columns of $G$ --- where the scalar product must be equal to 0 modulo 2. Then, construct the parity check matrix $H$ of $C$ as follows: $$H := \smat{y_1^T \\ \vdots \\ y_{n - k}^T}$$

	\claim[Claim 1]{
		Given that $C = \{Gx \mid x \in \Z_2^k\}$, it holds that $C = \ker H$.
	}{
		First, we show that $C \subseteq \ker H$. Fix any $c \in C$, and consider the input $x \in \Z_2^k$ such that $c = Gx$. We observe that $$Hc = H(Gx) = (HG)x$$ and by construction each row $y_i^T$ is orthogonal to all columns of $G$, which implies that $$\forall i, j \quad y_i^Tg_j = 0 \implies Hc = (HG)x = 0 \cdot x = 0 \iff c \in \ker H$$

		Now note that $H$ has $n - k$ linearly independent rows by construction, which implies that $\rk(H) = n - k$. Therefore, by the rank-nullity theorem $$\dim(\ker H) = n - \rk(H) = n - (n - k) = k$$ Then, since $C \subseteq \ker H$, and $C$ has dimension $k$, this must imply that $C = \ker H$.
	}

	Now we need to do the opposite in order to prove that the two definitions are actually equivalent. Let $H$ be the parity check matrix of $H$; pick $k$ linearly independent vectors $y_1, \ldots, y_k$ spanning the kernel of $H$. Then, construct the generator matrix $G$ of $C$ as follows: $$G := \smat{y_1 & \ldots & y_k}$$

	\claim[Claim 2]{
		Given that $C = \ker H$, it holds that $C = \{Gx \mid x \in \Z_2^k\}$.
	}{
		Fix any $c \in \{Gx \mid x \in \Z_2^k\}$ generated by some $x \in \Z_2^k$. Then, it holds that $c = Gx$ meaning that $$c = x_1 y_1 + \ldots + x_k y_k$$ Now, we compute $Hc$ by linearity, obtaining $$Hc = H(x_1 y_1 + \ldots + x_k y_k) = H(x_1 y_1) + \ldots + H(x_k y_k)$$ Observe that $x_1, \ldots, x_k$ are scalars, and each $y_i \in \ker H \implies Hy_i = 0$, therefore $$Hc = x_1 H y_1 + \ldots x_k H y_k = 0$$ This proves that $Hc \in \ker H = C$, therefore $\{Gx \mid x \in \Z_2^k\} \subseteq G$.

		For the second inclusion, observe that $$\mbox{span}\{y_1, \ldots, y_k\} = \ker H = C$$ by construction of $G$. Therefore, for any fixed $c \in C = \ker H$ it must hold that $c$ can be written as a linear combination of $y_1, \ldots, y_k$, i.e. $$\exists x_1, \ldots, x_k \in \Z_2 \quad c = x_1 y_1 + \ldots + x_k y_k$$ which concludes that $c = Gx$ where $x$ is the vector composed of the coefficients of the linear combination.
	}

	This shows that the two definitions are equivalent.
\end{proof}

As an example of parity check matrix, consider the $[3,1]$ repetition code described by the generator matrix we saw earlier $$G = \smat{1 \\ 1 \\ 1}$$ To construct $H$, we pick $3 - 1 = 2$ linearly independent vectors orthogonal to the columns of $G$, say $$y_1 = \smat{1 \\ 1 \\ 0} \quad \quad y_2 = \smat{0 \\ 1 \\ 1}$$ and define the parity check matrix as $$H := \smat{1 & 1 & 0 \\ 0 & 1 & 1}$$

Ok, how do we use this matrix now? Suppose that we have a linear code $C$ generated by a matrix $G$, and we encode a message $x$ as $y = Gx$, but an error $e$ due to noise corrupts $y$ giving the corrupted codeword $$y' = y + e$$ Because the parity check matrix $H$ is such that $C = \ker H$, it holds that $$Hy = 0 \implies Hy' = H(y + e) = Hy + He = He$$ We call $Hy' = He$ the \tbf{error syndrome} of $y'$, and it is the key component that enables error-correction in linear codes.

To see how to perform error-correction, suppose that at most 1 error occurred. Then, the error syndrome $Hy'$ is equal to 0 in the no error case, and it is equal to $H e_j$ when an error occurs on the $j$-th bit (where $e_j$ is the $j$-th vector of the canonical basis). Then, under the assumption that errors occur on at most one bit, it is possible to perform error-correction by computing the error syndrome $Hy'$ and comparing it to the different possible values of $He_j$, to dermine which (if any) bit need to be corrected. This looks like an interesting strategy, but we would like to be even more general.

\begin{frameddefn}{Hamming distance}
	Given two words $x$ and $y$ each of $n$ bits, we define the \tbf{Hamming distance} of $x$ and $y$ as follows: $$d(x, y) := \abs{\{i \mid x_i \neq y_i\}}$$
\end{frameddefn}

In simpler terms, the Hamming distance between two bit strings is the number of places at which they differ, for instance $$d \rbk{\smat{1 & 1 & 0 & 0}, \smat{0 & 1 & 0 & 1}} = 2$$

\begin{frameddefn}{Hamming weight}
	Given a word $x$ of $n$ bits, we define the \tbf{Hamming weight} of $x$ as follows: $$w(x) := d(x, 0)$$
\end{frameddefn}

The weight of a word is the number of places at which $x$ is non-zero. It is easy to show that the following proposition holds.

\begin{framedprop}{}
	Given two words $x$ and $y$ each of $n$ bits, it holds that $$d(x, y) = w(x + y)$$ where \curlyquotes{$+$} is the addition modulo 2.
\end{framedprop}

These two definitions are crucial because global properties of a linear code can be understood using the Hamming distance.

\begin{frameddefn}{Distance of a code}
	The \tbf{distance} of a linear code $C$ of type $[n, k]$ is defined as the minimum distance between its codewords $$d(C) := \min_{\substack{x, y \in C \\ x \neq y}}{d(x, y)}$$ We usually refer to $C$ as a linear code of type $[n, k, d]$, where $d = d(C)$.
\end{frameddefn}

Note that for any codewords $x, y \in C$ it holds that $x + y \in C$ by linearity of the code $C$, therefore by the previous proposition we obtain that $$d(C) = \min_{\substack{x \in C \\ x \neq 0}}{w(x)}$$ which means that the minimum distance is actually also equal to the \tbf{minimum weight}.

The importance of the distance of a code is highlighted in the following result.

\begin{framedthm}[label={code thm}]{}
	A linear code $C$ of type $[n, k, d]$ can correct $t = \floor{\tfrac{d - 1}{2}}$ errors, and can detect $d - 1$ errors.
\end{framedthm}

\begin{proof}
	TODO \todo{da scrivere slide 6}
\end{proof}

TODO \todo{how does ts work}

\subsection{The Hamming code}

Let $r \ge 2$ be an integer, and consider the matrix $H$ whose columns are all the $2^r - 1$ bit strings of length $r$ different from the 0 bit string. This is a parity check matrix of dimensions $(2^r - 1) \times (2^r - r - 1)$ and it defines a linear code known as the \tbf{Hamming code}, named after Turing Prize winner \href{https://it.wikipedia.org/wiki/Richard_Hamming}{Richard Hamming}. A very famous special example of the Hamming code is obtained with $r = 3$, which defines the following parity check matrix $$H = \smat{0 & 0 & 0 & 1 & 1 & 1 & 1 \\ 0 & 1 & 1 & 0 & 0 & 1 & 1 \\ 1 & 0 & 1 & 0 & 1 & 0 & 1}$$ which describes a linear code of type $[7, 4, 3]$. By \cref{code thm}, we know that this code is able to correct $$t = \floor{\dfrac{3 - 1}{2}} = 1$$ Let's give an example of error-correction through the $r = 3$ case of the Hamming code in action: suppose that the received codeword is $$y' = \smat{1 \\ 0 \\ 1 \\ 1 \\ 1 \\ 1 \\ 0 }$$ Now, we have to compute the syndrome of $y'$, namely $$Hy' = \smat{0 & 0 & 0 & 1 & 1 & 1 & 1 \\ 0 & 1 & 1 & 0 & 0 & 1 & 1 \\ 1 & 0 & 1 & 0 & 1 & 0 & 1} \smat{1 \\ 0 \\ 1 \\ 1 \\ 1 \\ 1 \\ 0 } = \smat{1 \\ 0 \\ 1}$$ Then, since this is not the 0 vector some error must have occurred, and because we know that $Hy' = He_j$ where $e_j$ has 1 only in the $j$-th position we have that $$He_j = \smat{1 \\ 0 \\ 1} \implies e_j = \smat{0 \\ 0 \\ 0 \\ 0 \\ 1 \\ 0 \\ 0} \implies j = 5$$ which means that the error occured in the 5-th position of $y'$. This means that the original $y$ was $$y = \smat{1 \\ 0 \\ 1 \\ 1 \\ 0 \\ 1 \\ 0 }$$ Now, to retrieve the original message $x$ we need to construct a matrix that we did not discuss yet. First, we need the generator matrix $G$, and we can costruct it by following the construction in \cref{code thm}. For this example we will employ the most common generator matrix usually defined for the Hamming code $[7, 4, 3]$, which is the following $$G := \smat{0 & 0 & 1 & 1 \\ 0 & 1 & 0 & 1 \\ 0 & 1 & 1 & 1 \\ 1 & 0 & 0 & 1 \\ 1 & 0 & 1 & 1 \\ 1 & 1 & 0 & 1 \\ 1 & 1 & 1 & 1}$$ This matrix is not invertible because it is not a square matrix, however we can simply choose $k$ linearly indepentend rows from $G$ and construct the following matrix $$\hat G := \smat{0 & 0 & 1 & 1 \\ 0 & 1 & 0 & 1 \\ 0 & 1 & 1 & 1 \\ 1 & 0 & 0 & 1}$$ which is now guaranteed to be invertible. Most importantly, we observe that we chose the 4 rows of the matrix $G$, and we will use this information later. Now, it can be proven that the inverse (modulo 2) of $\hat G$ is the following matrix: $$\hat G^{-1} = \smat{1 & 1 & 1 & 1 \\  1 & 0 & 1 & 0 \\ 0 & 1 & 1 & 0 \\ 1 & 1 & 1 & 0}$$ Proving that $\hat G ^{-1} \hat G = I  \bmod 2 $ is left as exercise. Finally, to obtain the message $x$ that generated $y$ we must first truncate $y$ to the first 4 bits --- because of how we constructed $\hat G$ --- thus obtaining $$\hat y = \smat{1 \\ 0 \\ 1 \\ 1}$$ which ultimately allows us to retrieve the original message $$\hat G^{-1} \hat y = \smat{1 & 1 & 1 & 1 \\  1 & 0 & 1 & 0 \\ 0 & 1 & 1 & 0 \\ 1 & 1 & 1 & 0} \smat{1 \\ 0 \\ 1 \\ 1} = \smat{1 \\ 0 \\ 1 \\ 0} = x$$

\subsection{The Quantum Hamming code}

TODO \todo{snapshot}
