\documentclass[a4paper, 12pt]{report}

\usepackage[dvipsnames]{xcolor}

%%%%%%%%%%%%%%%%%
% Set Variables %
%%%%%%%%%%%%%%%%%

\def\useItalian{0}  % 1 = Italian, 0 = English

\def\courseName{Quantum Computing}

\def\coursePrerequisites{TODO}

% \def\book{"My book",\\Author 1, ...}

% \def\authorName{Simone Bianco}
% \def\email{bianco.simone@outlook.it}
% \def\github{https://github.com/Exyss/university-notes}
% \def\linkedin{https://www.linkedin.com/in/simone-bianco}

\def\authorName{Alessio Bandiera}
\def\email{alessio.bandiera02@gmail.com}
\def\github{https://github.com/aflaag-notes}
\def\linkedin{https://www.linkedin.com/in/alessio-bandiera-a53767223}

% Do not change

%%%%%%%%%%%%
% Packages %
%%%%%%%%%%%%

\usepackage{../../packages/Nyx/nyx-packages}
\usepackage{../../packages/Nyx/nyx-styles}
\usepackage{../../packages/Nyx/nyx-frames}
\usepackage{../../packages/Nyx/nyx-macros}
\usepackage{../../packages/Nyx/nyx-title}
\usepackage{../../packages/Nyx/nyx-intro}

%%%%%%%%%%%%%%
% Title-page %
%%%%%%%%%%%%%%

\logo{../../packages/Nyx/logo.png}

\if\useItalian1
    \institute{\curlyquotes{\hspace{0.25mm}Sapienza} UniversitÃ  di Roma}
    \faculty{Ingegneria dell'Informazione,\\Informatica e Statistica}
    \department{Dipartimento di Informatica}
    \ifdefined\book
        \subtitle{Appunti integrati con il libro \book}
    \fi
    \author{\textit{Autore}\\\authorName}
\else
    \institute{\curlyquotes{\hspace{0.25mm}Sapienza} University of Rome}
    \faculty{Faculty of Information Engineering,\\Informatics and Statistics}
    \department{Department of Computer Science}
    \ifdefined\book
        \subtitle{Lecture notes integrated with the book \book}
    \fi
    \author{\textit{Author}\\\authorName}
\fi

\title{\courseName}
\date{\today}

% \supervisor{Linus \textsc{Torvalds}}
% \context{Well, I was bored\ldots}

\addbibresource{./references.bib}

%%%%%%%%%%%%
% Document %
%%%%%%%%%%%%

\begin{document}
\maketitle

% The following style changes are valid only inside this scope 
{
	\hypersetup{allcolors=black}
	\fancypagestyle{plain}{%
		\fancyhead{}        % clear all header fields
		\fancyfoot{}        % clear all header fields
		\fancyfoot[C]{\thepage}
		\renewcommand{\headrulewidth}{0pt}
		\renewcommand{\footrulewidth}{0pt}}

	\romantableofcontents
}

\introduction

%%%%%%%%%%%%%%%%%%%%%

\chapter{Introduction on Quantum Computation}

\section{The Qubit}

\href{https://en.wikipedia.org/wiki/Quantum_computing}{Quantum computing} is a rapidly developing discipline that explores how the laws of quantum mechanics can be used to \tit{process information}. While classical computation is based on \tit{bits} that take values of either 0 or 1, quantum computation relies on quantum bits, or \tbf{qubits}. A qubit can exist in a \curlyquotes{superposition} of classical states, allowing it to encode richer information than a single bit. Furthermore, qubits can exhibit particular properties that enable forms of information processing with no classical counterpart. Such properties provide the foundation for algorithms that promise to solve certain problems more efficiently than their classical analogues.

The design of quantum algorithms requires a different perspective from that of classical computation. In classical computer science, the majority of widely studied algorithms are \tit{deterministic}, meaning that for a given input they will always produce the \tit{same output}. Some algorithms are \tit{randomized}, making use of probability to achieve efficiency or simplicity, yet even in those cases the computation itself is ultimately classical in nature. In fact, to achieve such \tit{randomness} classical algorithms employ \tbf{pseudo-random number generation}, which must ultimately produce \underline{finite} sequences.

Quantum computation, by contrast, \tit{incorporates probability} at its core. The act of measuring a quantum system does not reveal a single, predetermined result, but rather yields one outcome from a distribution of possible outcomes, with probabilities governed by the system's quantum state. This fundamental probabilistic characteristic distinguishes quantum algorithms from their classical counterparts.

In fact, in the context of quantum computing we are often interested in \tbf{probabilistic algorithms}: for such algorithms, a given input $i$ can lead to a finite set of possible outputs $o_1, \ldots, o_N$, each occurring with an associated probability $p_1, \ldots, p_N$ --- where $\sum_{i = 1}^n{p_i} = 1$.

As previously mentioned, the quantum equivalent of the classical bits are the \tbf{qubit}, but define the qubits we first need to define some preliminary concepts. The following vectors are called \tbf{basis states} $$\ket{0} := \rmat{1 \\ 0} \quad \quad \ket 1 := \rmat{0 \\ 1}$$ and they represent the classical bits 0 and 1 respectively --- the notation above is called \curlyquotes{braket} notation and it will be explored in greater detail in later sections.

So what is a qubit? A qubit is the basic unit of information in quantum computing, which represents a \tbf{superposition} of states simultaneously --- note that we will refer to qubits and their states interchangeably, since the only thing that we care about a qubit is its own state

In practice, the state of a qubit is a vector $$\ket \psi = \alpha \ket 0 + \beta \ket 1 = \alpha \rmat{1 \\ 0 } + \beta \rmat{0 \\ 1} = \rmat{\alpha \\ \beta}$$ where $\alpha, \beta \in \C$ such that $\abs \alpha ^2 + \abs \beta ^2 = 1$ are called \tbf{probability amplitudes}. But why are we talking about probabilities in the first place? The \curlyquotes{true} state of a qubit \tbf{cannot be observed}, and we say that the qubit is in a \tit{superpotion} of $\ket 0$ and $\ket 1$ in the sense that $\alpha$ and $\beta$ describe the probabilities of getting either states once the qubit is measured. This is because to know the value of a qubit we have to \tit{measure it}, and the measurement operation itself will make the qubit \tit{collapse} into either $\ket 0$ or $\ket 1$ with probabilities $\abs \alpha ^2$ and $\abs \beta ^2$ respectively, i.e. $$\Pr[\mbox{measured qubit is $\ket 0$}] = \abs \alpha ^2 \quad \quad \Pr[\mbox{measured qubit is $\ket 1$}] = \abs \beta ^2$$ To use a more compact notation, we will denote this property as follows: $$\alpha \ket 0 + \beta \ket 1 \soe{ll}{\ket 0 & @ \  \abs{\alpha}^2 \\ \ket 1 & @ \ \abs{\beta}^2}$$ where the $@$ notation (read as \curlyquotes{at}) denotes the probabilty of the corresponding outcome. Note that if we measure a collapsed qubit we will keep observing the same state indefinitely.

In reality, to be precise qubits actually collapse into any multiple $z \ket 0$ or $z\ket 1$, where $z \in \C$ is a complex number such that $\abs z = 1$, but this is not relevant from a physical point of view. In fact, for any $\theta$ physicists treat $\ket \psi = \ket 0$ and $\ket {\psi'} = e^{i \theta} \ket 0 $ as the \tit{same physical state}, because probabilities depend on squared magnitudes and thus $$\abs{e^{i \theta} \alpha}^2 = \abs \alpha^2$$ (and the same applies for $\beta$ too) even though $\ket \psi$ and $\ket {\psi'}$ are different vectors mathematically. Therefore, in general we can actually drop the \tbf{global phases} from the qubits entirely.

\section{Qubit operations}

What can we do with qubits other then \tit{measure them}? The operations that can be applied on qubits are restricted to \tbf{unitary transformations}, which are linear maps that preserve the norms --- we will discuss the precise definition in the next chapter. For instance, the identity matrix $I$ is an example of trivial unitary transformation, but also the NOT matrix, which is the following $$\mbox{NOT} := \rmat{0 & 1 \\ 1 & 0}$$ which has the effect of \tit{swapping} the input basis state $$\mbox{NOT} \ket 0 = \ket 1 \quad \quad \mbox{NOT} \ket 1 = \ket 0$$ This matrix behaves as the classical NOT gate with the usual bits in classical computing, in fact will refer to \tit{transformations} and \tit{gates} interchangeably.

More in general, the NOT operation belongs to a family of operation represented by the so called \tbf{Pauli matrices}.

\begin{frameddefn}{Pauli matrices}
	The \tbf{Pauli matrices} are the following four $2 \times 2$ matrices: $$I := \rmat{1 & 0 \\ 0 & 1} \quad \sigma_x := \rmat{0 & 1 \\ 1 & 0} \quad \sigma_y := \rmat{0 & -i \\ i & 0} \quad \sigma_z := \rmat{1 & 0 \\ 0 & -1}$$
\end{frameddefn}

In particular, we observe that the second matrix $\sigma_x$ is exactly the matrix of the NOT operator. We will see the Z and Y operators --- representing the other two matrices, respectively --- as well in later sections.

Another very important transformation is represented by the \tbf{Hadamard gate}, which is the following matrix $$H := \dfrac{1}{\sqrt 2}\rmat{1 & 1 \\ 1 & -1}$$ This matrix has the effect of \curlyquotes{mapping} classical states into superpositions: $$H \ket 0 = \dfrac{1}{\sqrt 2}\rmat{1 & 1 \\ 1 & -1} \rmat{1 \\ 0} = \dfrac{1}{\sqrt 2} \rmat{1 \\ 0} = \dfrac{1}{\sqrt 2} (\ket 0 + \ket 1) \soe{ll}{\ket 0 & @ \  \frac{1}{2} \\ \ket 1 & @ \  \frac{1}{2}}$$ For instance, in this example given $\ket 0$ which represents the classical bit 0, we get a qubit as output of the linear transformation. In general, the operation performed by the Hadamard gate can be represented as follows: $$\forall a \in \{0, 1\} \quad \dfrac{1}{\sqrt 2} \rbk{\ket 0 + (-1)^a \ket 1}$$

As a side note, as we mentioned at the beginning of the chapter quantum mechanics has randomness intrinsically, and since the operation $H \ket 0$ returns a qubit that has 50\% of probability of being either $\ket 0$ or $\ket 1$ once measured, this operation provides a \underline{true} random number generator.

Lastly, can we \tit{represent} qubits graphically? Well, we may be tempted to anwer negatively to this question, since a qubit is described by two complex numbers $\alpha, \beta \in \C$, which implies that we actually need 4 dimensions to correctly represent our vector. However, through polar coordinates we can actually define a graphical representation which allows us to \curlyquotes{picture} qubits, through the so called \tbf{Bloch sphere}. First, consider a qubit $$\ket \psi = \alpha \ket 0 + \beta \ket 1$$ for some $\alpha, \beta \in \C$ such that $\abs{\alpha}^2 + \abs{\beta}^2 = 1$, as usual. Now, recalling that any complex number $z \in \C$ can be actually written as follows $$z = \abs z e^{i \theta}$$ for some angle $\theta$, we can actually rewrite our qubit as follows:
\begin{equation*}
	\begin{alignedat}{2}
		\ket \psi & = \abs \alpha e^{i \theta_\alpha} \ket 0 + \abs \beta e^{i \theta_\beta} \ket 1                             &                                                                            \\
		          & = e^{i \theta_\alpha} \rbk{\abs \alpha \ket 0 + \abs \beta e^{i \rbk{\theta_\beta - \theta_\alpha}} \ket 1} &                                                                            \\
		          & = \abs \alpha \ket 0 + \abs \beta e^{i \rbk{\theta_\beta - \theta_\alpha}} \ket 1                           & \quad \rbk{\mbox{$e^{i \theta_\alpha}$ is a global phase}}                 \\
		          & = \abs \alpha \ket 0 + e^{i \varphi} \ket 1                                                                 & \quad (\mbox{let $\varphi: = \theta_\beta - \theta_\alpha \in [0, 2\pi)$}) \\
	\end{alignedat}
\end{equation*}
and finally, since $\abs{\alpha}^2 + \abs{\beta}^2 = 1$, is precisely the equation of the circumference of radius 1, we usually rewrite the last equation as follows: $$\ket \psi = \cos{\rbk{\dfrac{\theta}{2}}} \ket 0+ e^{i \varphi} \sin{\rbk{\dfrac{\theta}{2}}} \ket 1$$ where $\theta \in [0, \pi], \varphi \in [0, 2\pi)$. This formulation of the qubit $\ket \psi$ allows us to represent it inside the Bloch sphere: in fact, in this formulation the qubit is normalized, which implies that it will lie on a 3 dimensional unit sphere, and it is described by the two phases $\theta$ and $\varphi$ --- in 2D polar coordinates there is only 1 angle, as in 3D polar coordinate there are two angles.

\centeredimage[The Bloch sphere representing some qubit.]{0.2}{../assets/bloch.png}

\subsection{The tensor product}

So far we have dealt with only one qubit at a time, but what if we have two qubits? First, let's look at the classical counterpart. If we take two bits $a, b \in \{0, 1\}$, we can represent 4 possible binary numbers, namely 00, 01, 10 and 11, which we can algebraically obtain by computing the usual cartesian product $$\{0, 1\}^2 = \{0, 1\} \times \{0, 1\} = \{(0, 0), (0, 1), (1, 0), (1, 1)\}$$ Note that in the cartesian products it holds that:

\begin{itemize}
	\item the length of the tuples of the product is linear w.r.t. the number of factors of the cartesian products --- in this case, 2
	\item each element of a tuple is \tit{independent} from the other elements of the tuple
\end{itemize}

How can we evaluate all the possible states that two qubits can represent, instead? To answer this question, we need to introduce a new operator, which is called \tbf{tensor product}. Given two vectors $\rmat{a \\ b}$ and $\rmat{c \\ d}$, their tensor product is defined as follows $$\rmat{a \\ b} \otimes \rmat{c \\ d} := \rmat{ac \\ ad \\ bc \\ bd}$$ Hence, consider two qubits $$\ket \psi = \alpha_0 \ket 0 + \alpha_1 \ket 1 = \rmat{\alpha_0 \\ \alpha_1} \quad \quad \ket \phi = \beta \ket 0 + \beta_1 \ket 1 = \rmat{\beta_0 \\ \beta_1}$$ To obtain all the possible states of $\ket \psi$ and $\ket \phi$ we just have to compute the tensor product between them, which is
\begin{equation*}
	\begin{split}
		\ket \psi \otimes \ket \phi & = \rmat{\alpha_0           \\ \alpha_1} \otimes \rmat{\beta_0 \\ \beta_1} \\
		                            & = \alpha_0 \beta_0 \rmat{1 \\ 0 \\ 0 \\ 0 } + \alpha_0 \beta_1 \rmat{0 \\ 1 \\ 0 \\ 0} + \alpha_1 \beta_0 \rmat{0 \\ 0 \\ 1 \\ 0} + \alpha_1 \beta_1 \rmat{0 \\ 0 \\ 0 \\ 1}
	\end{split}
\end{equation*}
At the beginning of the chapter we defined $\ket 0$ and $\ket 1$ to be $\rmat{0 \\ 1}$ and $\rmat{1 \\ 0}$ without providing an explaination; now that we are dealing with more than 2 dimensions we can show why such names are used. In fact, we will use the following naming convention $$\ket{00} := \rmat{1 \\ 0 \\ 0 \\ 0 } \quad \ket {01} := \rmat{0 \\ 1 \\ 0 \\ 0} \quad \ket{10} := \rmat{0 \\ 0 \\ 1 \\ 0} \quad \ket{11} := \rmat{0 \\ 0 \\ 0 \\ 1}$$ and in general it holds that $$\ket{\mbox{bin}(i)} = e_i$$ where $\mbox{bin}(i)$ represents for the binary representation of $i$, and $e_i$ is the $i$-th vector of the canonical basis. This implies that we can rewrite the previous tensor product as follows: $$\ket \psi \otimes \ket \phi = \alpha_0 \beta_0 \ket{00} + \alpha_0 \beta_1 \ket{01} + \alpha_1 \beta_0 \ket{10} + \alpha_1 \beta_1 \ket {11} = \sum_{i, j \in \{0, 1\}}{\alpha_i \beta_j \ket{ij}}$$ As a final note, it can be easily proven that $$\forall i, j \in \{0, 1\} \quad \ket i \otimes \ket j = \ket{ij}$$

For example, given two qubits $$\ket \phi = \dfrac{1}{\sqrt 2} (\ket 0 + \ket 1) \quad \ket \psi = \dfrac{1}{\sqrt 2} (\ket 0 + \ket 1)$$ we get that
\begin{equation*}
	\begin{split}
		\ket \psi \otimes \ket \phi & = \rmat{\tfrac{1}{\sqrt 2}                                                   \\ \tfrac{1}{\sqrt 2}} \otimes \rmat{\tfrac{1}{\sqrt 2} \\ \tfrac{1}{\sqrt 2}} \\
		                            & = \rmat{\tfrac{1}{2}                                                         \\ \tfrac{1}{2} \\ \tfrac{1}{2} \\ \tfrac{1}{2}} \\
		                            & = \dfrac{1}{2}(\ket{00} + \ket{01} + \ket{10} + \ket{11})                    \\
		                            & \soe{ll}{\ket 0 \mbox{and} \ket 0                         & @ \ \tfrac{1}{4} \\ \ket 0 \mbox{and} \ket 1 & @ \ \tfrac{1}{4} \\ \ket 1 \mbox{and} \ket 0 & @ \ \tfrac{1}{4} \\ \ket 1 \mbox{and} \ket 1 & @ \ \tfrac{1}{4}}
	\end{split}
\end{equation*}
where the probabilities at the end refer to the two individual qubits. To recap, in general the tensor product $\ket \psi \otimes \ket \phi$ of two qubits encodes the superposition of 4 basis states, namely $\ket {00}$, $\ket{01}$, $\ket{10}$ and $\ket {11}$.

Moreover, the following property can be proved easily.

\begin{framedprop}{Distributive property of $\otimes$}
	Given three qubits $\ket \psi, \ket \phi$ and $\ket \chi$, it holds that $$(\ket \psi + \ket \phi) \otimes \ket \chi = \ket \psi \otimes \ket \chi + \ket \phi \otimes \ket \chi$$
\end{framedprop}

\subsection{Controlled operations}

Another familyh of very important gates in quantum computing is the \tit{controlled operations}. The first controlled operation that we are going to discuss is the so called \tbf{Controlled NOT (CNOT)} gate, which is defined as follows:

\begin{center}
	\begin{tabular}{cc|c}
		\hline
		$a$ & $b$ & $\mbox{CNOT}(a, b)$ \\
		\hline\hline
		0   & 0   & 0                   \\
		\hline
		0   & 1   & 1                   \\
		\hline
		1   & 0   & 1                   \\
		\hline
		1   & 1   & 0                   \\
		\hline
	\end{tabular}
\end{center}

In fact, the names comes from the fact that the first input $a$ is called \tit{control bit}, which if set to 1 will flip the \tit{target bit} $b$ --- in fact, in its implementation what actually happens is that $b$'s wire itself is flipped. Therefore, in general we will write that $$\mbox{CNOT}(a, b) = (a, a \oplus b)$$

First, we observe that this function is clearly not invertible, since for instance if we know that the output is 0 we still need the input $a$ to evaluate if $b$ was 0 or 1. Hence, to solve this issue we usually pair the output of CNOT with $a$ itself, so that we can actually invert the computation.

Moreover, so far we only dealt with transformation that only expected one qubit argument as input, but the CNOT gate would certainly need 2 inputs to perform any computation, so how do we provide two inputs to it? As we showed before, we konw that $$\forall i, j \in \{0, 1\} \quad \ket i \otimes \ket j = \ket{i j}$$ which directly implies that the vector $\ket{ij}$ encapsulated two qubits at once without ambiguity. Hence, we can actually leverage the tensor product to provide the input to the CNOT matrix, such that the quantum CNOT will behave as follows $$\mbox{CNOT}(\ket a \otimes \ket b) = \ket a \otimes \ket{a \oplus b}$$ Hence, the matrix that behaves as such is the following $$\mbox{CNOT} := \rmat{1 & 0 & 0 & 0 \\ 0 & 1 & 0 & 0 \\ 0 & 0 & 0 & 1 \\ 0 & 0 & 1 & 0}$$ which expects a $4 \times 1$ input vector, and outputs a $4 \times 1$ output vector as well.

Laslty, as for the CNOT operator, we can actually define controlled operators for both Y and Z, which are respectively called CY and CZ operators.

\subsection{Quantum circuits}

Now that we introduced a couple of quantum gates, we can show how computation is actually represented in quantum computing. For instance, consider the following picture:

\centeredimage[The NOT gate.]{0.8}{../assets/not.png}

In this example, we have 1 single input qubit, namely $q$, and the box labeled with an $X$ represents the NOT gate. We observe that, by convetion, all qubits in quantum circuits are assumed to be set to $\ket 0$.

In the following example, instead, it is represented how the Hadamard gate looks like in quantum circuits.

\centeredimage[The Hadamard gate.]{0.8}{../assets/h.png}

Moreover, if we consider two qubits as inputs $q_0$ and $q_1$, we can represent the CNOT operator as follows:

\centeredimage[The CNOT gate.]{0.8}{../assets/cnot.png}

We observe that $q_1$ then becomes the output of the CNOT operation, and $q_0$ remains unchanged. Lastly, the measurement operation is represented with the following picure:

\centeredimage[The measure operation.]{0.8}{../assets/meas.png}

In particular, in this cirtuit we see that:

\begin{itemize}
	\item the vertical \curlyquotes{double line} reprents \tit{classical bits}
	\item the number 1 next to the label \curlyquotes{meas} indicates the number of qubits that have been measured
	\item the number 0 is the index of the measured qubit
\end{itemize}

\centeredimage[An exmaple of measurement of 2 qubits.]{0.8}{../assets/meas2.png}

Lastly, another very important circuit is the following, which produces the so called \tbf{Greenberger-Horne-Zeilinger (GHZ)} state

\centeredimage[The GHZ quantum circuit.]{0.8}{../assets/ghz.png}

which is represented as follows $$\ket{\mbox{GHZ}} := \dfrac{1}{\sqrt 2}\rbk{\ket{000} + \ket{111}}$$

\section{Peculiarities of quantum mechanics}

\subsection{Quantum entanglement}

Consider the following quantum state $$\ket \psi = \dfrac{1}{\sqrt 2}\rbk{\ket {01} + \ket {10}}$$ Can this state be rewritten as the tensor product of two distinct quantum states? We observe that for this to be possible we would require some complex values $\alpha_0, \alpha_1, \beta_0, \beta_1$ such that $$\soe{l}{\alpha_0 \beta_0 = \alpha_1 \beta_1 = 0 \\ \alpha_0 \beta_1 = \alpha_1 \beta_0 = \tfrac{1}{\sqrt 2}}$$ but $\alpha_0 \beta_0 = 0$ implies that at least one between $\alpha_0$ and $\beta_0$ has to be 0, meaning that at least one between $\alpha_0\beta_1$ and $\alpha_1\beta_0$ has to be 0 as well. This proves that there is no such pair of quantum states which can describe $\ket \psi$ through the tensor product operation. In fact, we see that $$\ket \psi = \dfrac{1}{\sqrt 2}\rbk{\ket{01} + \ket{10}}  \soe{ll}{\ket{01} & @ \ \tfrac{1}{2} \\ \ket{10} & @ \ \tfrac{1}{2}}$$ Indeed, this particular state we chose is one of the so called \tbf{Bell states}.

\begin{frameddefn}{Bell states}
	The following are the four \tbf{Bell states}: $$\ket{\Phi^+} := \dfrac{1}{\sqrt 2}\rbk{\ket{00} + \ket{11}}$$ $$\ket{\Phi^-} := \dfrac{1}{\sqrt 2}\rbk{\ket{00} - \ket{11}}$$ $$\ket{\Psi^+} := \dfrac{1}{\sqrt 2}\rbk{\ket{01} + \ket{10}}$$ $$\ket{\Psi^-} := \dfrac{1}{\sqrt 2}\rbk{\ket{01} - \ket{10}}$$
\end{frameddefn}

Whenever we have a state $\ket \psi$ that cannot be represented as the tensor product of two simpler quantum states, we say that the state is \tbf{entangled} --- or that its possible outcomes are entangled. In particular, entangled states describe a very weird phenomenon first proposed as a thought experiment in a groundbreaking paper by \tbf{Einstein, Podolsky and Rosen (EPR)} \cite{epr}, the so called \tbf{EPR paradox}.

The thought experiment involves a pair of particles prepared in such \tit{entangled state}. Einstein, Podolsky, and Rosen pointed out that, in this state, if the position of the first particle were measured, the result of measuring the position of the second particle \tit{could be predicted}. If instead the momentum of the first particle were measured, then the result of measuring the momentum of the second particle could be predicted. They argued that no action taken on the first particle could instantaneously affect the other, since this would involve information being transmitted faster than light, which is impossible according to the theory of relativity. Einstein famously called this phenomenon \curlyquotes{spooky action at a distance}, and to the best of our knowledge the theory of quantum mechanics says that if we have two engangled states, and measure one of them --- for instance, say that it collapses to $\ket 0$ --- the other state will \tbf{instantaneously} collapse to $\ket 1$ (and viceversa). They are \tit{perfectly anti-correlated}, even if the two states are phisically light-years away from each other.

To be precise, entanglement is \tit{not} a way to tranfer information --- collapsing happens instantaneously, which would violate the fact that nothing can travel faster than light, not even information. Instead, it is a way to share correlations nonlocally. In fact, it is a phenomenon that regards the \tit{whole quantum system} considered: for instance, given three qubits $q_0, q_1, q_2$, such that $q_1$ and $q_2$ are entangled, we might want to only measure $q_0 \otimes q_1$, which in turn will make $q_2$ collapse into some quantum state that has to be mathematically computed in order to be predicted --- this will be more clear when we will describe \tbf{quantum teleportation} in \cref{quantum teleportation}.

To finish off this section, we can actually generate entangled states, or \tbf{EPR pairs} for short, through quantum gates as such:

\centeredimage[The quantum circuit for $\ket{\Phi^+}$.]{0.8}{../assets/epr.png}

In particular, we observe that the first Hadamard gate will transform $\ket 0$ to $\dfrac{1}{\sqrt 2}\rbk{\ket 0 + \ket 1}$, and through the CNOT operation we obtain $$\ket{\Phi^+} := \dfrac{1}{\sqrt 2}\rbk{\ket{00} + \ket{11}}$$

\subsection{No-cloning theorem}

An operation that we take for granted in classical computation is the possibility to \tit{copy} the value of a bit: if Alice has two bits $x, y \in \{0, 1\}$, and she wants to copy the value of $x$ into $y$, she can do it without any issues. However, in quantum mechanics this is \tit{not} possible, because it would quite literally violate the laws of physics --- as far as we understand it.

In 1982 \textcite{nocloning} proved the so called \tbf{no-cloning theorem}, which states that it is impossible to create an independent and identical copy of an arbitrary \tit{unknown} quantum state.

\begin{framedthm}{No-cloning theorem}
	There is no quantum transformation that copies an unknown quantum state.
\end{framedthm}

\begin{proof}
	by way of contradiction, suppose that there exists such a transformation CP that is able to copy an unknown quantum state --- and in particular, we observe that such transformation would have to be linear. But clearly, in order to have a copy we need to actually \tit{store} it somewhere, so we can assume that CP has to take two inputs, one being the state that we want to copy and the other one being the state that we want to replace with the copy of the first one. In other words, we are assuming that $$\exists y \forall x \quad \mbox{CP}(x \otimes y) = x \otimes x$$ Now, through some algebraic manipulation we get that
	\begin{equation*}
		\begin{alignedat}{2}
			       & \exists y \forall x \quad \mbox{CP}(x \otimes y) = x \times x                                                  &                                         \\
			\equiv & \exists y \forall x, a \quad \mbox{CP}((x + a) \otimes y) = (x + a) \otimes (x + a)                            &                                         \\
			\equiv & \exists y \forall x, a \quad \mbox{CP}(x \otimes y + a \otimes y) = (x + a) \otimes (x + a)                    & (\mbox{by distributivity of $\otimes$}) \\
			\equiv & \exists y \forall x, a \quad \mbox{CP}(x \otimes y) + \mbox{CP}(a \otimes y) = (x + a) \otimes (x + a)         & (\mbox{by linearity of CP})             \\
			\equiv & \exists y \forall x, a \quad x \otimes x + a \otimes a = x \otimes x + x \otimes a + a \otimes x + a \otimes a & \quad (\mbox{by definition of CP})      \\
			\equiv & \exists y \forall x, a \quad \mathbf 0 = x \otimes a + a \times x
		\end{alignedat}
	\end{equation*}
	which should be true for every $x$ and every $a$, however it does not hold for $x = \ket 0$ and $a = \ket 1$, thus raising a contradiction $\lightning$.
\end{proof}

The no-cloning theorem represents an inherent limitation of quantum computation, and has direct impacts on \tbf{quantum cryptography} and \tbf{quantum error correction}, but must importantly it directly impacts a phenomenon called \tbf{quantum teleportation}

\subsection{Quantum teleportation} \label{quantum teleportation}

So far we saw that quantum states cannot be cloned, but can we at least \tit{send} them? Suppose that Alice wants to send Bob $\ket \psi$, described by some $\alpha$ and $\beta$. Clearly, the only thing that Bob has to receive are indeed the probability amplitudes of $\ket \psi$, so even if Alice cannot clone her quantum state, nothing prevents her to build a quantum circuit which \tit{destroys} her $\ket \psi$ but does allow Bob to receive $\alpha$ and $\beta$. This process is called \tbf{quantum teleportation}, and can be achieved through the following algorithm.

\begin{framedalgo}{Quantum teleportation algorithm}
    Given three qubits $q_0$, $q_1$ and $q_2$, the algorithm moves the state of $q_0$ into $q_2$ \\
    \hrule

    \quad
    \begin{algorithmic}[1]
        \Function{QuantumTeleportation}{$q_0$, $q_1$, $q_2$}
            \State $q_1 = H(q_1)$
            \State $q_2 = CX(q_1, q_2)$ \Comment{entangle $q_1$ and $q_2$}
            \State $q_1 = CX(q_0, q_1)$
            \State $q_0, q_1 = \mbox{measure}(q_0, q_1)$
            \If{$q_1 == \ket 1$}
                \State $q_2 = X(q_2)$
            \EndIf
            \If{$q_0 == \ket 1$}
                \State $q_2 = Z(q_2)$
            \EndIf
            \State \tbf{return} $q_2$
        \EndFunction
    \end{algorithmic}
\end{framedalgo}

\centeredimage[The Quantum Teleportation circuit.]{0.8}{../assets/telep.png}

There is quite a lot to unpack in this diagram. First, the quantum state that we want to teleport is $q_0$ in this diagram, and it will be teleported in $q_2$ at the end of the quantum computation.

In the first part of the circuit, we see that $q_1$ and $q_2$ are entangled (in an initial stage of the process, not performed by Alice nor Bob) in the Bell state $\ket{\Phi^+}$ thanks to the Hadamard and the CNOT gates --- as we described in previous sections. In a real-world scenario, we will assume that $q_1$ and $q_2$ are given to Alice and Bob respectively (through some \tbf{quantum channel} such as optical fibers or free-space links in order to avoid \tit{decoherence}), and quantum mechanics will guarantee that the teleportation will work even our two protagonists are thousands of kilometers away from each other.

After creating and entangling $q_1$ and $q_2$ (say for instance in a lab as preparation), we have the part of circuit that concerns Alice: in fact, she must apply a CNOT to her entangled qubit $q_1$, controlled by $q_0$, and then apply a Hadamard transformation to $q_0$. At this point, the circuit must apply a measurement to both $q_0$ and $q_1$ --- and in particular, this operation will \tit{destroy} the original state as previously anticipated.

Finally, it's Bob's turn: to obtain the original quantum state of $q_0$, the only thing he needs to do is first apply a CNOT to his entangled qubit $q_2$, controlled by $q_1$'s outcome, followed by an application of a CZ, controlled by $q_0$'s outcome instead --- we observe that this part is indicated in the diagram through the \texttt{0x2} and \texttt{0x1} labels respectively. In fact, in the label \texttt{0xX} the number \texttt X represents the hexadecimal representation of the binary number obtained by joining the classical bits all together --- for instance, in this circuit we have that 2 represents 10, meaning that only $q_1$ will be checked in the condition, and 1 represents 01, which means that only $q_0$ will be the control bit.

To show why the circuit actually works, we first need to discuss how computations with qubits and quantum gates is performed. In particular, we do not consider qubits \tit{individually}, but instead we consider the whole \tbf{system} of qubits, i.e. $q_0 \otimes q_1 \otimes q_2$ simultaneously, and thus we will perform calculations as such. In fact, even if the drawing represents Alice's measurements of $q_0$ and $q_1$ independently, what happens in reality is that Alice is going to measure $q_0 \otimes q_1$ such that $q_3$ will collapse into its opposite.

Finally, we are ready to prove the correctness of the quantum teleportation circuit. First, we need to represent the initial quantum state, namely
\begin{equation*}
	\begin{split}
		  & q_0 \otimes q_1 \otimes q_2                                                                                                                 \\
		= & \ket \psi \otimes \ket{\Phi^+}                                                                                                              \\
		= & (\alpha \ket{0}_{0} + \beta \ket{1}_{0}) \otimes \dfrac{1}{\sqrt 2}(\ket{00}_{12} + \ket{11}_{12})                                          \\
		= & \dfrac{1}{\sqrt 2} [\alpha \ket{0}_{0} \otimes (\ket{00}_{12} + \ket{11}_{12}) + \beta \ket{1}_{0} \otimes (\ket{00}_{12} + \ket{11}_{12})]
	\end{split}
\end{equation*}
where the notation $\ket{00}_{12}$ represents for example that we are considering $q_1$ and $q_2$'s parts of states, respectively. From now on, we will omit the $\otimes$ symbol --- as for the \curlyquotes{normal} product. The next step is to apply the CNOT on $q_0$ and $q_1$, therefore the quantum state of the system becomes the following: $$\dfrac{1}{\sqrt 2}\sbk{\alpha \ket{0}_{0} (\ket{00}_{12} + \ket{11}_{12}) + \beta \ket{1}_{0} (\ket{10}_{12} + \ket{01}_{12})}$$ Next, we have to apply the Hadamard gate on $q_0$, which turns the quantum state into the following
\begin{equation*}
	\begin{split}
		  & \dfrac{1}{\sqrt 2} \sbk{
			\alpha \dfrac{1}{\sqrt 2} ( \ket{0}_{0} + \ket{1}_{0} ) ( \ket{00}_{12} + \ket{11}_{12} )
			+ \beta \dfrac{1}{\sqrt 2} ( \ket{0}_{0} - \ket{1}_{0} ) ( \ket{10}_{12} + \ket{01}_{12} )
		}                            \\
		= & \dfrac{1}{2} \sbk{
			\alpha ( \ket{000}_{012} + \ket{011}_{012} + \ket{100}_{012} + \ket{111}_{012} )
			+ \beta ( \ket{010}_{012} + \ket{001}_{012} - \ket{110}_{012} - \ket{101}_{012} )
		}                            \\
		= & \dfrac{1}{2} \sbk{
			\ket{00}_{01} ( \alpha \ket{0}_{2} + \beta \ket{1}_{2} )
			+ \ket{01}_{01} ( \beta \ket{0}_{2} + \alpha \ket{1}_{2} )  + \ket{10}_{01} ( \alpha \ket{0}_{2} - \beta \ket{1}_{2} )
			+ \ket{11}_{01} ( \alpha \ket{1}_{2} - \beta \ket{0}_{2} )
		}                            \\
		= & \dfrac{1}{2} \sbk{
			\ket{00}_{01} \ket{\psi}_{2}
			+ \ket{01}_{01} X \ket{\psi}_{2}
			+ \ket{10}_{01} Z \ket{\psi}_{2}
			+ \ket{11}_{01} XZ \ket{\psi}_{2}
		}
	\end{split}
\end{equation*}
Finally, Alice will perform the measurement on $q_0$ and $q_1$, and what will happen is that the \tit{whole} quantum state of the quantum circuit will collapse as follows: $$\soe{ll}{\ket{00}_{01} \otimes \ket \psi_2 & @ \ \tfrac{1}{4} \\ \ket{01}_{01} \otimes X \ket \psi_2 & @ \ \tfrac{1}{4} \\ \ket{10}_{01} \otimes Z \ket \psi_2 & @ \ \tfrac{1}{4} \\ \ket{11}_{01} \otimes XZ \ket \psi_2 & @ \ \tfrac{1}{4}}$$ Note that to perform the measurement operation on just $q_0$ and $q_1$ we would need some mathematical tools that we will encounter in the next chapters, therefore we will only show the probabilities of the outcomes as presented above. In fact, fom this table we can easily explain the last part of the quantum teleportation circuit, i.e. Bob's part, as shown below.

\begin{center}
	\begin{tabular}{c|c|c}
		\hline
		Alice's outcome & Bob's part & Bob's result                 \\
		\hline\hline
		0 and 0         & $I$        & $\ket \psi$                  \\
		\hline
		0 and 1         & $X$        & $XX \ket \psi = \ket \psi$   \\
		\hline
		1 and 0         & $Z$        & $ZZ \ket \psi = \ket \psi$   \\
		\hline
		1 and 1         & $XZ$       & $XZXZ \ket \psi = \ket \psi$ \\
		\hline
	\end{tabular}
\end{center}

Lastly, note that even if the mathematical calculations don't highlight the fact that $q_0$ and $q_1$ are measured, these two bits are effectively \tit{destroyed} as we already described. In fact, $q_2$ will be the only usable qubit after the whole process --- for instance, nothing prevents us from applying more transformations on the state $\ket{00}_{01} \otimes \ket \psi_2$ \tit{mathematically}, but in reality $q_0$ and $q_1$ are not usable anymore.

\chapter{Mathematical foundations}

Now that we laid down some preliminary concepts regarding quantum machanics and quantum computaions, we need to discuss some \tbf{mathematical foundations} in order to progress and achieve a deeper meaning of the tools that we are going to use. In fact, at the end of this chapter we will state precisely the postulates of quantum mechanics, but in order to understand them we need to introduce some crucial definitions.

We will start our mathematical discussion with the definition of \tbf{scalar product} --- we will assume the definitions of vector space, basis and linear independence are already known by the reader.

\begin{frameddefn}{Scalar product}
	Given a scalar product vector space $V$, a \tbf{scalar product} $\funcmap{\braket{\cdot , \cdot }}{V \times V}{\C}{(v, w)}{\braket{v,w}}$ is a function that satisfies the following properties:

	\begin{itemize}
		\item $\forall u, v, w \in V, \alpha, \beta \in \C \quad \braket{u,\alpha v + \beta w} = \alpha \braket{u|v} + \beta \braket{u| w}$
		\item $\forall u, v \in V \quad \overline{\braket{u,v}} = \braket{v, u}$ --- where $\overline z$ is the conjugate of $z \in \C$
		\item $\forall u \in U \quad \braket{u, u} \ge 0$ and $\braket{u, u} = 0$ if and only if $u = 0$
	\end{itemize}
\end{frameddefn}

Scalar products are also called \tit{inner product}, and are used to define many other tools on top of the vector space considered.

\begin{framedprop}{}
	For any scalar product vector space $V$, any scalar product satisfies the following property: $$\forall u, v, w \in V, \alpha, \beta \in \C \quad \braket{\alpha u + \beta v, w} = \overline \alpha \braket{u, w} + \overline \beta \braket{v, w}$$
\end{framedprop}

\begin{proof}
	TODO \todo{TODO}
\end{proof}

In particular, the scalar product that we are going to use for our purposes is defined as follows: $$\forall u, v \in \C^n \quad \braket{u, v} = \sum_{i = 1}^n{\overline{u_i} v_i}$$ In fact, we can prove that this is indeed a scalar product as follows:

\begin{itemize}
	\item TODO \todo{TODO}
	\item TODO \todo{TODO}
	\item TODO \todo{TODO}
\end{itemize}

From now on, when we refer to a \curlyquotes{scalar product} we will refer to this particular definition.

We are finally ready to explain the \curlyquotes{braket} that we used from the beginning of the previous chapter. This notation was invented by the Nobel Prize in Physics \href{https://it.wikipedia.org/wiki/Paul_Dirac}{Paul Dirac}, and it works as follows: first, observe that our scalar product can be rewritten as follows $$\braket{u, v} = \rmat{\overline{u_1} \cdots \overline{u_n}} \rmat{v_1 \\ \vdots \\ v_n}$$ To be precise, this product would yield a $1 \times 1$ matrix, which can be interpreted as a scalar. Through Dirac notation, we will write $$\braket{u, v} = \braket{u | v}$$ where $\bra u$ is called \tbf{bra}, and $\ket v$ is called \tbf{ket} (as in \curlyquotes{bra-ket}). In other words, we have that $\ket v$ is just a regular column vector $v \in V$ $$\ket v = \rmat{v_1 \\ \vdots \\ v_n}$$ defined over some scalar product vector space $V$, while $\bra u$ is a \tit{linear map} that acts as follows: $$\funcmap{\bra \cdot}{V}{\overline V}{\rmat{u_1 \cdots u_n}}{\rmat{\overline{u_1} \cdots \overline{u_n}}}$$

\begin{framedthm}{Cauchy-Schwarz inequality}
	Given a scalar product vector space $V$, it holds that $$\forall u, v \in V \quad \abs{\braket{u|v}} \le \sqrt{\braket{u|v} \braket{u|v}}$$ where the equality holds if and only if $u$ and $v$ are linearly independent.
\end{framedthm}

Moreover, our scalar product induces a \tbf{norm}, which is defined as follows.

\begin{frameddefn}{Norm}
	Given a scalar product vector space $V$, the \tbf{norm} of a vector $v \in V$ is defined as follows $$\norm v = \sqrt{\braket{v|v}}$$
\end{frameddefn}

As usual, two vectors $u, v \in V$ are said to be \tbf{orthogonal} if $\braket{u|v} = 0$. This allows us to define orthonormal bases.

\begin{frameddefn}{Orthonormal basis}
	Given a scalar product vector space $V$, a basis $\{e_1, \ldots, e_n\}$ is said to be \tbf{orthonormal} if $$\forall i, j \in [n] \quad \braket{e_i | e_j} = \delta_{ij}$$ where $\delta_{ij} = \soe{ll}{1 & i = j \\ 0 & i \neq j}$ is called \tbf{Kronecker delta}.
\end{frameddefn}


Let's see the Dirac notation in action. Consider an orthonormal basis $\{e_1, \ldots, e_n\}$ for some scalar product vector space $V$; by definition, we know that we can write any vector $u \in V$ as follows $$u = \sum_{i = 1}^n{\alpha_i e_i}$$ for some coefficients $\alpha_1, \ldots, \alpha_n \in \C$. Now, we observe that for all $i \in [n]$
\begin{equation*}
	\begin{alignedat}{2}
		\braket{e_i|u} & = \braket{e_i|\sum_{j = 1}^n{\alpha_j e_j}}                      & \\
		               & = \abk{e_i \middle|\alpha_1e_1 + \ldots + \alpha_n e_n}               & \\
		               & = \alpha_1 \braket{e_i|e_1} + \ldots + \alpha_n \braket{e_i|e_n} & \\
		               & = \sum_{j = 1}^n{\alpha_j\braket{e_i | e_j}}                     & \\
		               & = \sum_{j = 1}^n{\alpha_j \delta_{ij}}                           & \\
		               & = \alpha_i                                                       & \\
	\end{alignedat}
\end{equation*}
Indeed, with the scalar product we can compute the projection of $u$ onto the $i$-th vector of the basis. Hence, we can rewrite the first equation as follows: $$\ket u = \sum_{i = 1}^n{\alpha_i \ket{e_i}} = \sum_{i = 1}^n{\braket{e_i|u} \ket{e_i}}$$ In particular, we observe that $$\ket u = \sum_{i = 1}^n{\ket{e_i} \braket{e_i|u}} \implies I = \sum_{i = 1}^n{\ket{e_i} \bra{e_i}}$$ which is a famous identity in quantum mechanics called \tbf{resolution of the identity}. As a final note, by the properties of scalar products we also have that $$\braket{v|u} = \abk{v \middle| \sum_{i = 1}^n{\braket{e_i|u} \ket{e_i}}} = \sum_{i = 1}^n {\braket{v|e_i} \braket{e_i|u}}$$

\begin{framedprop}{}
    Given a scalar product vector space $V$, it holds that $$\forall u, v, w \in V \quad \braket{u|v} \ket w = (\ket w \bra u) \ket v$$
\end{framedprop}

\begin{proof}
    TODO \todo{TODO}
\end{proof}

\section{Hilbert spaces}

Now that we covered Dirac notation, we can describe what are \tbf{Hilbert spaces} --- we will see why we care about this particular type of vector spaces later in the chapter. First, consider the following definitions.

\begin{frameddefn}{Weak convergence}
	Given a scalar product vector space $V$, and a vector sequence $\{v_m\}_{m \in \N}$ defined over $V$, we say that the sequence \tbf{converges weakly} to a vector $v \in V$ if $$\forall w \in V \quad \lim_{m \to + \infty}{\braket{v_m|w}} = \braket{v|w}$$
\end{frameddefn}

In other words, this type ocf convergence requires all projections of $v_m$ along any fixed direction $w$ to approach the projection of $v$. Differently, the next type of convergence is more strict.

\begin{frameddefn}{Strong convergence}
	Given a scalar product vector space $V$, and a vector sequence $\{v_m\}_{m \in \N}$ defined over $V$, we say that the sequence \tbf{converges strongly} to a vector $v \in V$ if $$\lim_{m \to + \infty}{\norm{v - v_m}} = 0$$
\end{frameddefn}

In fact, this type of convergence requires the actual vectors of the sequence to get close \tit{in norm} to $v$. We observe the following proposition.

\begin{framedprop}{}
	Given a scalar product vector space $V$, and a vector sequence $\{v_m\}_{m \in \N}$ defined over $V$, if the sequence converges strongly to some vector $v \in V$, it holds that

	\begin{itemize}
		\item the sequence also converges weakly
		\item the scalar products defined over $V$ are continuous, i.e. $$\forall u, v \in V \quad \braket{u|v} = \lim_{m \to + \infty}{\braket{u|v_m}}$$
	\end{itemize}
\end{framedprop}

\begin{frameddefn}{Cauchy sequence}
	Given a scalar product vector space $V$, and a vector sequence $\{v_m\}_{m \in \N}$ defined over $V$, we say that the sequence is a \tbf{Cauchy sequence} if it holds that $$\forall \varepsilon > 0 \quad \exists n_\varepsilon \in \N \quad \forall n, m > n_\varepsilon \quad \norm{v_n - v_m} < \varepsilon$$
\end{frameddefn}

For example, let's consider the space $\R^2$ equipped with the Euclidean norm $$\norm v =  \sqrt{x^2 + y^2}$$ Then, if we consider the following vector sequence $$\cbk{\rmat{\tfrac{1}{m} \\ \vdots \\ \tfrac{1}{m}}}_{m \in \N}$$ we see that for any distinct $m, n$ it holds that $$\norm{v_m - v_n} = \sqrt{\rbk{\dfrac{1}{m} -\dfrac{1}{n}}^2 + \rbk{\dfrac{1}{m} -\dfrac{1}{n}}^2} = \sqrt 2 \abs{\dfrac{1}{m} -\dfrac{1}{n}}$$ Therefore, for any $\varepsilon > 0$ it suffices to take any $N > \dfrac{2 \sqrt 2}{\varepsilon}$ such that $$\forall m, n > N \quad \norm{v_m - v_n} < \varepsilon$$

We are finally ready to define Hilbert spaces.

\begin{frameddefn}{Hilbert space}
	A \tbf{Hilbert space} is a \tit{complete} scalar product vector space, i.e. it is a vector space

	\begin{itemize}
		\item equipped with a scalar product
		\item such that every Cauchy sequence converges strongly to an element in the space.
	\end{itemize}
\end{frameddefn}

For example, the space $\R^n$ is a Hilbert space. Indeed, since every finite vector space of size $n$ is isomporphic to $\R^n$, we can immediately derive the following proposition.

\begin{framedprop}{}
	Finite-dimensional vector spaces are always complete.
\end{framedprop}

\subsection{Linear operators}

Given a Hilbert space $\mathcal H$, we can define \tbf{operators} --- which are nothing but linear maps.

\begin{frameddefn}{Adjoint operator}
	Given a Hilbert space $\mathcal H$, and an operator $A$, the \tbf{adjoint} operator of $A$, denoted with $A^\dag$, is a linear map that satisfies the following property $$\forall u, v \in \mathcal H \quad \braket{u|A^\dag v} = \braket{Au|v}$$ We say that an operator $A$ is \tbf{self-adjoint}, or \tit{Hermitian}, if and only if $A = A^\dag$.
\end{frameddefn}

For instance, the following matrix $S = \rmat{1 & 0 \\ 0 & i}$ is a linear operator whose adjoint is $S^\dag = \rmat{1 & 0 \\ 0 & -i}$. In fact, we have that $$\braket{u|S^\dag v} = \rmat{\overline{u_1} & \overline{u_2}} \rmat{1 & 0 \\ 0 & -i} \rmat{v_1 \\ v_2} = \rmat{\overline{u_1} & \overline{u_2}} \rmat{v_1 \\ -iv_2} = \overline{u_1}v_1 - i\overline{u_2}v_2$$ and since $$Su = \rmat{1 & 0 \\ 0 & i}\rmat{u_1 \\ u_2} = \rmat{u_1 \\ iu_2} \implies \bra{Su} = \rmat{\overline{u_1} & \overline{i u_2}}$$ but because $\overline{iu_2} = \overline i \cdot \overline{u_2} = -i \overline{u_2}$ this implies that $$\braket{Su |v} = \rmat{\overline{u_1} & -i\overline{u_2}} \rmat{v_1 \\ v_2} = \overline{u_1}v_2 - i\overline{u_2}v_2$$

\begin{framedprop}[label={adj prop}]{}
	For any adjoint operators $A, B$ defined over some Hilbert space $\mathcal H$, it holds that

	\begin{enumerate}
		\item $(AB)^\dag = B^\dag A^\dag$
		\item for any scalar $z$ it holds that $(zA)^\dag = \overline z A^\dag$
		\item $(A^\dag)^\dag = A$
                \item $(A + B)^\dag = A^\dag + B^\dag$
	\end{enumerate}
\end{framedprop}

How do we evaluate the adjoint of a given operator?

\begin{framedprop}{}
    Given an operator $A$ defined over a scalar product vector space, it holds that $$A_{ij}^\dag = \overline{A_{ji}}$$
\end{framedprop}

This property is incredibly useful, because it implies that the adjoint operator of $A$ is its transposed conjugate matrix. Most notably, due to the way we defined our scalar product, it holds that for any column vector $\ket x$ we have that $$\bra x = \ket x^\dag$$ which gives an intuition of the reason why we defined our scalar product as such.

\begin{framedprop}{}
	If an operator $A$ is self-adjoint, it holds that $$\braket{u|Av} = \braket{Au|v} = \overline{\braket{v|Au}}$$
\end{framedprop}

\begin{proof}
	TODO \todo{TODO}
\end{proof}

At the beginning of the previous chapter we said that all quantum gates are \tit{unitary transformations}, but we did now provide a definition of unitary. Now we are ready to introduce it, and start to grasp why we are discussing Hilbert spaces.

\begin{frameddefn}{Unitary operators}
	Given a Hilbert space $\mathcal H$, and an operator $U$, we say that $U$ is \tbf{unitary} if it holds that $$UU^\dag = U^\dag U = I$$
\end{frameddefn}

In other words, $U$ is unitary if and only if its adjoint operator is also its inverse. An interesting characterization of unitary transformations is the following property.

\begin{framedprop}{Unitary operators (alt. def.)}
	An operator $U$ defined over a Hilbert space $\mathcal H$ is unitary if and only if

	\begin{itemize}
		\item $U$ is surjective
		\item $\forall x, y \in \mathcal H \quad \braket{Ux|Uy} = \braket{x|y}$ or equivalently, if it holds that $$\forall x \in \mathcal H \quad \norm{Ux} = \norm x$$
	\end{itemize}
\end{framedprop}

\begin{proof}
    TODO \todo{TODO}
\end{proof}

In particular, we observe that the second property of this proposition is very interesting: the \tit{preservation of scalar product}, i.e. the property for which $$\forall x, y \in \mathcal H \quad \braket{Ux|Uy} = \braket{x|y}$$ means that the operator $U$ does not change the geometric relationships between vectors --- i.e. their lengths and angles remain the same.

\begin{framedprop}[label={unitary prod}]{}
    If $A$ and $B$ are two unitary operators, then $AB$ is a unitary operator.
\end{framedprop}

\begin{proof}
    Since $A$ and $B$ are unitary it holds that $$A ^\dag A = A A^ \dag = B ^ \dag B = B B^ \dag = I$$ Now, by \cref{adj prop} we have that $$(AB)^ \dag = B^\dag A^\dag$$ from which we conclude that $$(AB)^\dag(AB) = B^\dag A^\dag AB = B^\dag I B = B^\dag B = I$$
\end{proof}

\begin{frameddefn}{Normal operators}
	Given a Hilbert space $\mathcal H$, and an operator $A$, we say that $A$ is \tbf{normal} if it satisfies the following property $$A^\dag A = AA^\dag$$
\end{frameddefn}

Clearly, from their definition we immediately see that both self-adjoint and unitary operators are both normal.

\section{Spectral theory}

Since unitary operators are linear maps, we are interested in their eigenvectors and eigenvalues --- which are defined as usual. First, let us recall some preliminary definitions.

\begin{frameddefn}{Non-degenerate eigenvalue}
	Given a matrix $A$, and an eigenvalue $\lambda$ of $A$, we say that $\lambda$ is \tbf{non-degenerate} if the associated eigenspace has dimension 1 (or equivalently, if it has only 1 associated eigenvector).
\end{frameddefn}

\begin{frameddefn}{$d$-fold degenerate eigenvalue}
	Given a matrix $A$, and an eigenvalue $\lambda$ of $A$, we say that $\lambda$ is $d$-fold degenerate if there are $d$ linearly independent eigenvectors $u_1, \ldots, u_d$ associated to $\lambda$.
\end{frameddefn}

In Dirac notation, if $\lambda$ is non-degenerate, we refer to the only eigenvector associated to $\lambda$ as $\ket \lambda$, indeed it holds that $$A \ket \lambda = \lambda \ket \lambda$$

\begin{framedprop}{}
	Given a matrix $A$ defined over a Hilbert space $\mathcal H$, and an eigenvalue $\lambda$ associated to $A$, it holds that $$\bra \lambda A^\dag = \overline \lambda \bra \lambda$$
\end{framedprop}

\begin{proof}
	TODO \todo{TODO}
\end{proof}

The following theorem provides a characterization of the eigenvalues and eigenvectors of self-adjoint and unitary operators, which have surprisingly nice properties.

\begin{framedthm}{Spectral theorem}
	The following propositions hold:

	\begin{itemize}
		\item The eigenvalues of a self-adjoint operator are real values.
		\item The eigenvalues of a unitary operator are complex values of modulus 1.
		\item Eigenvectors of self-adjoint and unitary operators associated to different eigenvalues are orthogonal to each other.
	\end{itemize}
\end{framedthm}

Moreover, for finite-dimensional Hilbert spaces the following holds.

\begin{framedthm}{Spectral theorem for fin. Hilbert spaces}
    Given a finite-dimensional Hilbert space $\mathcal H$, and a normal operator $A$ defined over $\mathcal H$, the set of all eigenvectors of $A$ is an orthonormal basis for $\mathcal H$.
\end{framedthm}

We can rewrite this thorem as follows: if we denote with $u_{ij}$ the $j$-th eigenvector associated to the $i$-th eigenvalue of $A$, it holds that $$\forall v \in \mathcal H \quad v = \sum_{i = 1}^m{\sum_{j = 1}^{d_i}{\alpha_{ij} u_{ij}}}$$ where $d_i$ is the geometric multiplicity of the $i$-th eigenvalue of $A$. Indeed, we observe that $\dim \mathcal H = \sum_{i = 1}^m{d_i}$. Lastly, through the Dirac notation we can rewrite the formula as follows $$\forall v \in \mathcal H \quad \ket v = \sum_{i = 1}^m{\sum_{j = 1}^{d_i}{\braket{\lambda_{ij}|v} \ket{\lambda_{ij}}}}$$

\section{Projectors}

Next, we are going to discuss \tbf{projectors}, which are another very crucial pieces of quantum computation. We saw how scalar products are able to perform projection over desired direction, in fact we will use the Dirac notation to define precise operators for our purposes. But as always, first some preliminary definitions.

\begin{frameddefn}{Orthogonal space}
    Given a scalar product vector space $U$, and two linear subspaces $V, W \subset U$, we say that $V$ is orthogonal to $W$ if $$\forall v \in V, w \in W \quad \braket{v|w} = 0$$
\end{frameddefn}

Given a scalar product vector space $U$, and a linear subspace $V \subset U$, the \tbf{orthogonal complement} of $V$ is defined as follows: $$V^\bot := \{u \in U \mid \forall v \in v \quad \braket{u|v} = 0\}$$ In particular, we observe that if $U$ is finite-dimentional it holds that $V = U - V^\bot$ and that $(V^\bot)^\bot = V$.

\begin{frameddefn}{Topologically closed subspace}
    Given a Hilbert space $\mathcal H$, and a linear subspace $V \subset \mathcal H$, we say that $V$ is \tbf{topologically closed} if any sequence of vectors defined over $V$ converges in $V$.
\end{frameddefn}

Interestingly enough, given a topologically closed subspace $V \subset \mathcal H$ of some Hilbert space, we can write any vector $u \in V$ as the sum of two orthogonal vectors of $V$ and $V^\bot$, as follows. Let $\{f_1, \ldots, f_n\}$ be an orthonormal basis of $V$, and define the following vectors $$\forall u \in U \quad u_V := \sum_{i = 1}^n{\braket{f_i|v} f_i}$$ Then, if we call $$u_{V^\bot} := u - u_V$$, we see that
% \begin{equation*}
%     \begin{alignedat}{2}
%         \braket{u_V|u_{V^\bot}} &= \braket{u_V|u - u_V} & \\ 
%                                 &= \braket{u_V|u} - \braket{u_V|u_V} & \\ 
%                                 &= \braket{\sum_{i = 1}^n{\braket{f_i|u} f_i}|u} - \braket{u_V|u_V} & \\ 
%                                 &= \sum_{i = 1}^n{\braket{f_i|u} \braket{f_i|u}} - \braket{u_V|u_V} & \\ 
%                                 &= \sum_{i = 1}^n{\abs{\braket{f_i|u}}^2} - \braket{u_V|u_V} & \\ 
%                                 &= \sum_{i = 1}^n{\overline{\braket{f_i|u}} \braket{f_i|u}} & \quad (\mbox{since $\abs{z}^2 = z \cdot \overline z$}) \\ 
%                                 & = \sum_{i = 1}^n{\sum_{j = 1}^n{\overline{\braket{f_i|u}} \braket{f_j|u} \delta_{i j}}} & \quad (\mbox{since $\delta_{ij} = \braket{f_i|f_j}$})
%     \end{alignedat}
% \end{equation*}
TODO \todo{decommenta e finisci la formula} which indeed proves that $u_V$ and $u_{V^\bot}$ are orthogonal to each other.

With this observation, we can finally define the projector operators.

\begin{frameddefn}{Projector}
    Given a Hilbert space $\mathcal H$, and a closed subspace $V \subset \mathcal H$, the \tbf{projector} operator that projects any given vector $v \in \mathcal H$ onto $V$ is defined as follows: $$\funcmap{P_V}{\mathcal H}{V}{u}{u_V = \sum_{i = 1}^n{\braket{f_i|u} f_i}}$$ where $\{f_1, \ldots, f_n\}$ is an orthonormal basis of $V$.
\end{frameddefn}

In other words, the projector we have that $$P_V := \sum_{i = 1}^n{\ket{f_i} \bra{f_i}}$$ Clearly, by definition of $u_{V^\bot}$ it holds that $$\funcmap{P_{V^\bot}}{\mathcal H}{V^\bot}{u}{u_{V^\bot} := u - u_V}$$ Moreover, since $P_V$ performs a projection, we have that $$\forall u \in \mathcal H \quad u \in V \iff P_Vu = u$$ and that $$\forall u \in \mathcal H \quad u \in V^\bot \iff P_V u = 0$$ Additionally, for any projector $P_V$ it holds that $P_V^2 = P_V$, by idempotency, and $P_V^\dag = P_V$.

\begin{framedprop}{}
    Any projector operator only has 0 and 1 as possible eigenvalues.
\end{framedprop}

\begin{proof}
    Consider a projector operator $P_V$; by definition $v$ is an eigenvalue of $P_V$ associated to the eigenvalue $\lambda$ if it holds that $$P_Vv = \lambda v$$ Hence, we observe that $$P_V^2 v = P_V (P_V v) = P_V(\lambda v) = \lambda P_Vv = \lambda(\lambda v) = \lambda^2 v$$ and by idempotency of $P_V$ it holds that $$\lambda^2 v = P_V^2 = P_Vv = \lambda v$$ which implies that $$\lambda^2v - \lambda v = 0 \iff (\lambda^2 - \lambda)v = 0$$ Finally, since $v$ is an eigenvector it holds that $v \neq 0$, therefore it must be that the last equation is true only if $$\lambda^2 - \lambda = 0 \iff \lambda = 0 \lor \lambda = 1$$
\end{proof}

Given a Hilbert space $\mathcal H$, and two topologically closed subspaces $V, W \subset \mathcal H$, we say that $P_V$ and $P_W$ are \tbf{orthogonal} if it holds that $V \bot W$. Now, fix a vector $u \in \mathcal H$; by definition $P_Vu$ is a vector that lies inside $V$, therefore it holds that $$P_W(P_Vu) = 0$$ since $V \bot W$, and by the same reasoning applied on $W$ first we conclude that $$P_WP_V = P_VP_W = \mathbf 0$$ where $\mathbf 0$ is the \tbf{zero operator} --- indeed, it is a zero matrix.

As a final note, if $A$ is an linear operator, and $\lambda$ is an eigenvalue of $A$, we denote with $P_\lambda$ the projector that projects vectors onto the eigenspace associated to $\lambda$.

\begin{framedprop}[label={projector sum}]{}
    If $P$ and $Q$ are two orthogonal projectors, then $P + Q$ is still a projector.
\end{framedprop}

\begin{proof}
    TODO \todo{TODO}
\end{proof}

\section{Tensor product}

Finally, the last ingredient that we need to discuss is the \tbf{tensor product}

TODO \todo{TODO}

\section{Rules of quantum mechanics}

Now that we defined Hilbert spaces and their operators in great detail, we can finally present we needed this mathematical foundations in order to progress: quantum mechanics is developed over Hilbert spaces with \tit{countable} bases, and quantum computing works with finite-dimensional Hilbert spaces. In particular, there are four fundamental \tbf{postulates of quantum mechanics} which describe the \todo{what?}:

\begin{enumerate}
	\item \tbf{State postulate}: the state of a quantum system is completely described by a vector $\ket \psi$ in a Hilbert space $\mathcal H$ --- $\ket \psi$ is usually normalized, as we saw at the beginning of the previous chapter, and physical systems of different types live in different Hilbert spaces.
	\item \tbf{Measurement postulate}: every measurable (i.e. \tit{observable}) quantity corresponds to a self-adjoint operator on $\mathcal H$. In particular, given an observable $A$, and a state $v \in \mathcal H$, it holds that:
            \begin{itemize}
                \item the only possible results of measuring $A$ are one of its eigenvalues
                \item the probability of measuring eigenvalue $\lambda$ in state $v$ is given by $$\Pr[A = \lambda \mid v] = \braket{v|P_\lambda v}$$ where $P_\lambda$ is the linear map that projects $v$ onto the $\lambda$-eigenspace
            \end{itemize}
        \item \tbf{Time evolution postulate}: a closed system evolves through time according to the SchrÃ¶dinger equation $$i \hbar \dfrac{\diff}{\diff t} v(t) = Hv(t)$$ where the elements of this first-order linear differential equation are the following:

            \begin{itemize}
                \item $v(t)$ is the state vector at time $t$ (a vector in a Hilbert space)
                \item $H$ is the system \tit{Hamiltonian}, a self-adjoint operator that describes the total energy of the system
            \end{itemize}
        \item \tbf{Composite systems postulate}: the Hilbert space of a composite system is the tensor product of the Hilbert spaces of its subsystems. In other words, if system $A$ is defined over $\mathcal H_A$, and system $B$ is defined over $\mathcal H_B$, the total system lives in $$\mathcal H_{AB} = \mathcal H_A \otimes \mathcal H_B$$
\end{enumerate}

If we take a closer look at the second postulate, we can actually explain why we choose that particular scalar product to be the probability. Since by convention any quantum state is normalize, i.e. $\norm v = 1$, it holds that
\begin{equation*}
    \begin{split}
        1 & = \norm{v}^2 \\ 
          & = \braket{v|v} \\ 
          & = \abk{\sum_{i = 1}^m{P_{\lambda_iv}}\middle|\sum_{j = 1}^m{P_{\lambda_jv}}} \\ 
          & = \sum_{i = 1}^m{\sum_{j = 1}^m{\braket{P_{\lambda_i}v|P_{\lambda_j}v}}} \\ 
      \end{split}
\end{equation*}
Now, since each $P_{\lambda_i}$ is a projector, we know that when $i \neq j$ it holds that $P_{\lambda_i}P_{\lambda_j} = \mathbf 0$, therefore by self-adjointness of projectors we have that

\begin{itemize}
    \item if $i \neq j$ then $$\braket{P_{\lambda_i}v|P_{\lambda_j}v} =\braket{v|P_{\lambda_i}P_{\lambda_j}v} = \braket{v| \mathbf 0 v} = 0$$
    \item if $i = j$ then $$\braket{P_{\lambda_i}v|P_{\lambda_j}v} = \braket{P_{\lambda_i}v|P_{\lambda_i}v} = \norm{P_{\lambda_i}v}^2$$
\end{itemize}

Therefore, by adding only the non-zero terms we get that $$\sum_{i = 1}^m{\norm{P_{\lambda_i}v}^2} = 1$$ Hence, we define $$\Pr[A = \lambda_i \mid v] := \norm{P_{\lambda_i}v}^2$$ such that $$\Pr[A \mid v] = \sum_{i = 1}^m{\Pr[A = \lambda_i \mid v]} = \sum_{i = 1}^m{\norm{P_{\lambda_i}v}^2} = \norm{v}^2 = 1$$ which also means that our probabilities will add up to 1 automatically. Finally, we can rewrite this proability as follows (we will drop the index of the eigenvalue):
\begin{equation*}
    \begin{alignedat}{2}
        \Pr[A = \lambda \mid v] & = \braket{P_\lambda v | P_\lambda v} & \quad (\mbox{by def. of adjoint}) \\ 
                             & = \braket{v | P_\lambda^\dag P_\lambda v} & \quad (\mbox{by self-adjointness}) \\ 
                             & = \braket{v | P_\lambda^2 v} & \quad (\mbox{by idempotency}) \\ 
                             & = \braket{v | P_\lambda P_\lambda v} & \\ 
                             & = \braket{v | P_\lambda v} & \\ 
    \end{alignedat}
\end{equation*}

Another postulate that we need to discuss is the third one, regarding the SchrÃ¶dinger equation. In particular, the solution of the latter is $$v(t_1) = U(t_2, t_1)v(t_1)$$ where $U(t_2, t_1)$ is called \tbf{time-evaluation operator}, and it is defined as follows: $$U(t_2, t_1) = e^{-\tfrac{i}{\hbar}H(t_2 - t_1)}$$ (assuming $H$ does not depend on time). We recall that $H$ is a matrix, so we are raising $e$ to the power of a matrix, an operation that is defined by the power series of the exponential as follows: $$e^A = \sum_{n = 0}^\infty{\dfrac{A^n}{n!}}$$ What is interesting about this operator is that $U$ is \tbf{unitary}, and in order to show it is suffices to prove that $U^\dag = U^{-1}$. But how do we compute the adjoint of $U$? We observe that by the properties of the adjoint operation it holds that $$\rbk{e^A}^\dag = \rbk{\sum_{n = 0}^\infty{\dfrac{A^n}{n!}}}^\dag = \sum_{n = 0}^\infty{\dfrac{(A^n)^\dag}{n!}} = \sum_{n = 0}^\infty{\dfrac{(A^\dag)^n}{n!}} = e^{A^\dag}$$ which means that the adjoint of an exponential is the exponential of the adjoint. This suffices to prove that $$U^\dag = \rbk{e^{-\tfrac{i}{\hbar}H(t_2 - t_1)}}^\dag = e^{\rbk{-\tfrac{i}{\hbar}H(t_2 - t_1)}^\dag} = e^{\tfrac{i}{\hbar}H(t_2 - t_1)} = \rbk{e^{-\tfrac{i}{\hbar}H(t_2 - t_1)}}^{-1} = U^{-1}$$ This is a crucial characteristic for quantum mechanics: since $U$ is unitary, we know that it preserves the scalar product, therefore it also preserve \tbf{probabilities and norms}. This is why we say that evolution in quantum systems --- or \tit{quantum evolution}, for short --- is unitary.

\chapter{Quantum algorithms}

TODO \todo{non so di che parlerÃ  il capitolo}

% \section{Peculiarities of quantum computation}

% \subsection{Reversible computation}

When we introduced quantum operators we underlined the fact that each quantum gate has to be a \tit{unitary} operator, and we now have the mathematical foundation to know that if a matrix is unitary, it is clearly also invertible --- indeed, its adjoint is its inverse. This directly implies a very important property of quantum computation: except for the measurement operation, every quantum computation operation is \tbf{reversible}.

Truth be precise, classical computation \tit{admits} reversible computation. In fact, we observe that we do have examples of reversible classical computaion that does not lose efficiency --- in terms of time. In 1963 \textcite{lecerf} proposed a reversible Turing machine, and in 1973 a landmark result by \textcite{bennett} proved that any standard Turing machine can be actually simulated by a reversible one --- his construction involves augmenting the Turing machine with an auxiliary \tit{history tape}, which can potentially lead to a large space overhead.

Consider the following bitwise operator $T$, that given three bits it works as follows: $$\funcmap{T}{\{0, 1\}^3}{\{0, 1\}^3}{(a, b, c)}{(a, b, c \oplus (a \land b))}$$ The corresponding classical gate of this bitwise operator is called \tbf{Toffoli gate}, and it has very special characteristics. First, observe how it computes: while $a$ and $b$ remains unchanged, $c$ is basically flipped if and only if both $a$ and $b$ are set to 1. In other words, both $a$ and $b$ act as \curlyquotes{control bits}over the \curlyquotes{target bit} $c$, indeed the Toffoli gate is sometimes called \tbf{Controlled Controlled NOT (CCNOT)}. As we did for the CNOT, this operator is clearly invertible since we are passing the bits $a$ and $b$ to the output too --- also, the truth table of the Toffoli gate easily shows that it is indeed invertible. Moreover, we observe that by associativity of the XOR it holds that $$(c \oplus (a \land b)) \oplus (a \land b) = c \oplus (a \land b) \oplus (a \land b) = c$$ which directly implies that $$T^2 = I \implies T = T^{-1}$$ However, above all the most important property of the Toffoli gate is that it is \tbf{universal}. In fact, even though gates like NAND and NOR are universal too, they are \tit{irreversible}. This implies that with the $T$ operator we can build any reversible Boolean function, and since any ordinary Boolean function can be embedded into a reversible one --- by adding extra bits to make it invertible --- any classical computation can be simulated using Toffoli gates only.

\section{Deutsch's algorithm}

Even though quantum computation provides reversibility \curlyquotes{for free}, we saw that classical computation can still achieve invertibility of computation. However, the next characteristic that we are going to describe has no classical analogue.

First, let's start with a problem seemingly unrelated to our discussion. Given a Boolean function $\func{f}{\{0, 1\}^n}{\{0, 1\}}$, we would like to embed $f$ inside a quantum computation. However, when $n \ge 3$ we are guaranteed that $f(x)$ is not reversible --- it cannot be injective. This is a problem, since in quantum coputing all gates must be reversible --- given that quantum evolution is unitary. Thus, how do we turn $f$ into a reversible computation?

We define a map $U_f$ defined as follows: $$\funcmap{U_f}{\{0, 1\}^{n + 1}}{\{0, 1\}^{n + 1}}{(x, y)}{(x, y \oplus f(x))}$$ First, we observe that $$(y \oplus f(x)) \oplus f(x) = y \oplus (f(x) \oplus f(x)) = y$$ which trivially proves that $U_f$ is reversible. Moreover, we can actually prove that when applied to qubits the corresponding quantum operator $$\funcmap{U_f}{\mathcal H}{\mathcal H}{\ket x \ket y}{\ket x \ket{y \oplus f(x)}}$$ is indeed unitary --- we observe that we are omitting the tensor product symbol in the function definition, as usual in the literature.

\begin{framedprop}{}
    Given a Boolean function $\func{f}{\{0, 1\}^n}{\{0, 1\}}$, the operator $U_f$ is unitary.
\end{framedprop}

\begin{proof}
    TODO \todo{TODO}
\end{proof}

This proves that the construction of $U_f$ is precisely the gate that allows us to embed $f$ into any quantum computation. Moreover, we observe that

\begin{itemize}
    \item $\ket y = \ket 0 \implies U_f \ket x \ket 0 = \ket x \ket{f(x)}$
    \item $\ket y = \ket 1 \implies U_f \ket x \ket 1 = \ket x \ket{\lnot f(x)}$
\end{itemize}

However, until now we only considered already collapsed qubits, but what if we consider a quantum input that is in a superposition? For instance, let $$\ket x = \dfrac{1}{\sqrt 2}(\ket 0 + \ket 1)$$ and assume that $\ket y = \ket 0$ for simplicity; this implies that
\begin{equation*}
    \begin{alignedat}{2}
        U_f \ket x \ket y & = U_f \dfrac{1}{\sqrt 2}(\ket 0 + \ket 1) \otimes \ket 0 & \\ 
                          & = U_f \dfrac{1}{\sqrt 2}(\ket{00} + \ket{10}) & \\ 
                          & = \dfrac{1}{\sqrt 2}(U_f \ket {00} + U_f \ket {10}) & \quad (\mbox{by linearity of $U_f$}) \\ 
                          & = \dfrac{1}{\sqrt 2}(\ket 0 \ket{f(0)} + \ket{1} \ket{f(1)}) & \\ 
    \end{alignedat}
\end{equation*}
But notice what just happened: both $f(0)$ and $f(1)$ have been computed \tbf{simultaneously}, in one gate application. This has no classical equivalent, we would have to evaluate $f(0)$ and $f(1)$ \tit{separately}. This phenomenon is called \tbf{quantum parallelism}, and it can be achieved only because:

\begin{itemize}
    \item qubits are in superpositions
    \item quantum gates are linear
\end{itemize}

However, we observe that the result of our calculations is \tit{still a superposition}. In fact, if we measure the output of $U_f \ket x \ket y$ we would still get either $\ket 0 \ket{f(0)}$ or $\ket 1 \ket {f(1)}$, both with 50\% probability. This is a problem: the fact that we can compute $f(0)$ and $f(1)$ at the same time seems promising, but can we retrieve their actual values?

Unfortunately, this is not possible. Indeed, quantum parallelism cannot help us with \tit{local} properties --- i.e. when we need all individual outputs --- it can only help when we need \tbf{global} properties. This limit derives from the fact that measurements prevent \curlyquotes{seeing} both outcomes, in fact if we were able to compute $f(0)$ and $f(1)$ simultaneously from this superposition we would be violating the laws of quantum mechanics themselves.

Then, how do we extract useful \tit{global} information from the superposition output? In 1985 \textcite{deutsch} defined a quantum algorithm which is able to compute $f(0) \oplus f(1)$, which clearly tells us if $f(0)$ equals $f(1)$ or not.

\begin{framedalgo}{Deutsch algorithm}
    Given a Boolean function $f$ and 2 qubits, the algorithm returns $\ket 0$ if $f$ if $f(0) = f(1)$, $\ket 1$ otherwise. \\
    \hrule

    \quad
    \begin{algorithmic}[1]
        \Function{Deutsch}{$f$, $q_0$, $q_1$}
            \State $q_1 = X(q_1)$
            \State $q_0, q_1 = (H \otimes H)(q_0, q_1)$
            \State $q_0, q_1 = U_f(q_0, q_1)$
            \State $q_0 = H(q_0)$
            \State \tbf{return} $\mbox{measure}(q_0)$
        \EndFunction
    \end{algorithmic}
\end{framedalgo}

\centeredimage[The quantum circuit for Deutsch's algorithm. The box colored in \tit{magenta} labeled with \texttt{U\_f} represents a \curlyquotes{black-box} for whatever computation $U_f$ represents (which directly depends on the chioce of $f$).]{0.9}{../assets/deutsch.png}

Proving that this quantum circuit is correct will be a little more involved than the quantum teleportation one. First, we need a lemma that will simplify our calculations.

\begin{framedlem}[label={U lemma}]{}
    For any Boolean function $f$ defined on $n$ bits, and $a \in \{0, 1\}^n$, it holds that $$U_f \ket a \otimes \dfrac{1}{\sqrt 2}(\ket 0 - \ket 1) = (-1)^{f(a)} \ket a \otimes \dfrac{1}{\sqrt 2}(\ket 0 - \ket 1)$$
\end{framedlem}

\begin{proof}
    First, by algebraic manipulation we see that
    \begin{equation*}
        \begin{split}
            U_f \ket a \otimes \dfrac{1}{\sqrt 2}(\ket 0 - \ket 1) & = U_f \dfrac{1}{\sqrt 2}(\ket{a0} - \ket{a1}) \\ 
                                                                   & = \dfrac{1}{\sqrt 2}(U_f \ket{a0} - \ket{a1}) \\ 
                                                                   & = \dfrac{1}{\sqrt 2}(\ket{a \ f(a)} - \ket{a \ \lnot f(a)}) \\ 
        \end{split}
    \end{equation*}
    and now, we observe that

    \begin{itemize}
        \item if $f(a) = 0$, then $$\dfrac{1}{\sqrt 2}(\ket{a0} - \ket{a1}) = (-1)^0 \ket{a} \otimes \dfrac{1}{\sqrt 2}(\ket 0 - \ket 1)$$
        \item if $f(a) = 1$, then $$\dfrac{1}{\sqrt 2}(\ket{a1} - \ket{a0}) = (-1)^1 \ket{a} \otimes \dfrac{1}{\sqrt 2}(\ket 0 - \ket 1)$$
    \end{itemize}
\end{proof}

We are now ready to prove the correctness of Deutsch's algorithm. To make things less cluttered, we will use the following standard notation: $$\ket + := \dfrac{1}{\sqrt 2}(\ket 0 + \ket 1) \quad \quad \ket - := \dfrac{1}{\sqrt 2}(\ket 0 - \ket 1)$$ In particular, we observe that $$H \ket 0 = \ket + \quad \quad H \ket 1 = \ket - $$ Moreover, we will omit the subscript of the corresponding qubit when the context is clear enough
\begin{equation*}
    \begin{alignedat}{2}
      & q_0 \otimes q_1 &  \\ 
        = & \ket 0 \otimes \ket 0 & \\ 
        \xrightarrow{X(q_1)} & \ket 0 \otimes \ket 1 & \\ 
        \xrightarrow{(H \otimes H)(q_0, q_1)} & \ket + \otimes \ket - &  \\ 
        = & \dfrac{1}{\sqrt 2}(\ket{00} - \ket{01} + \ket{10} - \ket{11}) & \\ 
        = & \dfrac{1}{\sqrt 2} \ket 0_0 \ket -_1  + \dfrac{1}{\sqrt 2} \ket 1_0 \ket -_1 & \\ 
        \xrightarrow{U_f(q_0, q_1)} & \dfrac{1}{\sqrt 2} (-1)^{f(0)} \ket 0_0 \ket -_1 + \dfrac{1}{\sqrt 2} (-1)^{f(1)} \ket 1_0 \ket -_1 & \quad (\mbox{by the lemma}) \\ 
        = & \dfrac{1}{\sqrt 2}\rbk{(-1)^{f(0)} \ket 0_0 + (-1)^{f(1)} \ket 1_0} \otimes \ket -_1 & \\ 
        \xrightarrow{H(q_0)} & \dfrac{1}{\sqrt 2}\rbk{(-1)^{f(0)} \ket +_0 + (-1)^{f(1)} \ket -_0} \otimes \sqrt 2 \ket -_1 & \\ 
        = & \dfrac{1}{2} \rbk{(-1)^{f(0)}(\ket 0 + \ket 1) + (-1)^{f(1)}(\ket 0 - \ket 1)} \otimes \sqrt 2 \ket -_1 & \\ 
        = & \dfrac{1}{2}\rbk{\rbk{(-1)^{f(0)} + (-1)^{f(1)}} \ket 0 + \rbk{(-1)^{f(0)} - (-1)^{f(1)}} \ket 1} \otimes \sqrt 2 \ket -_1 & \\ 
    \end{alignedat}
\end{equation*}
Now, since the final operation of the circuit involves measuring $q_0 = \alpha \ket 0 + \beta \ket 1$, the only two things that we care about are its probability amplitudes, namely $$\alpha = \dfrac{1}{2}\rbk{(-1)^{f(0)} + (-1)^{f(1)}}$$ $$\beta = \dfrac{1}{2}\rbk{(-1)^{f(0)} + (-1)^{f(1)}}$$ and we see that

\begin{itemize}
    \item if $f(0) = f(1)$, then $$(-1)^{f(0)} = (-1)^{f(1)} \implies \soe{l}{\alpha = \tfrac{1}{2}\rbk{2(-1)^{f(0)}} = (-1)^{f(0)} \\ \beta = 0}$$ which implies that $$q_0 = (-1)^{f(0)} \ket 0 + 0 \cdot \ket 1 = (-1)^{f(0)} \ket 0$$ and we can ignore the $(-1)^{f(0)}$ factor since its a global phase
    \item if $f(0) \neq f(1)$, then $$(-1)^{f(0)} = - (-1)^{f(1)} \implies \soe{l}{\alpha = 0 \\ \beta = \tfrac{1}{2}\rbk{2(-1)^{f(0)}} = (-1)^{f(0)}}$$ which implies that $$q_0 = 0 \cdot \ket 0 + (-1)^{f(0)} \ket 1 = (-1)^{f(0)} \ket 1$$ by the same reasoning as the other case
\end{itemize}

In the end, this proves that if $f(0) = f(1)$, $q_0$ will collapse to $\ket 0$, while if $f(0) \neq f(1)$ $q_1$ will colapse to $\ket 1$, proving that Deutsch's algorithm works correctly.

\section{Deutsch-Josza algorithm}

Even though Deutsch's algorithm is quite interesting and offers advantages that classical computation cannot achieve, still it seems like it wouldn't be very usefult in practice. In fact, usually we are interested in the \tit{values} of $f(0)$ and $f(1)$, and as we already mentioned quantum mechanics will not allow us to compute both the values at the same time --- meaning that even if we use Deutsch's algorithm to now whether $f(0)$ is equal to $f(1)$ or not, we would still need to compute at least one between $f(0)$ and $f(1)$ in order to know both values.

This is because, in reality, the algorithm that we are using is only solving a particular case of a more complex problem. In fact, a couple of years later \textcite{dj} realized that if we use $q_1 = \ket 1$ and $q_0 = \ket{0^n}$ (i.e. we use $n$ qubits set to $\ket 0$) this algorithm is actually able to tell \tbf{constant} and \tbf{balanced} functions apart.

\begin{frameddefn}{Constant function}
    A Boolean function $\func{f}{\{0, 1\}^n}{\{0,1\}}$ is said to be \tbf{constant} if $$\exists b \in \{0, 1\} \quad \forall x \in \{0, 1\}^n \quad f(x) = b$$
\end{frameddefn}

The definition of constant Boolean function has nothing special, and balanced functions are exactly what the name suggests, i.e. half of the inputs output 0 and the other half output 1, which can be succintly expressed as follows.

\begin{frameddefn}{Balanced function}
    A Boolean function $\func{f}{\{0, 1\}^n}{\{0,1\}}$ is said to be \tbf{balanced} if it holds that $$\sum_{x \in \{0, 1\}^n}{f(x)} = 2^{n - 1}$$
\end{frameddefn}

We observe that a Boolean function can be neither constant nor balanced, so this decision problem is actually a \tbf{promise problem}: given a Boolean function $f$ that is either constant or balanced --- note that it cannot be both --- decide if the function is constant or balanced. Indeed, we see that Deutsch's algorithm solved the same exact problem for $n = 2$: in fact, if $f(0) = f(1)$ it means that $f$ is constant, otherwise the latter is balanced.

Moreover, this problem actually shows the power of quantum parallelism more evidently: with a classical computation, to solve this decision problem we would need at most $$2^{n - 1} + 1 = O(2^n)$$ queries to $f$, instead our quantum computation still only requires \und{one} evaluation of $f$ to solve the problem.

\begin{framedalgo}{Deutsch-Josza algorithm}
    Given a Boolean function $f$ and $n + 1$ qubits, the algorithm returns $\ket{0^n}$ if $f$ is constant, $\ket 1$ otherwise. \\
    \hrule

    \quad
    \begin{algorithmic}[1]
        \Function{DeutschJosza}{$f$, $q_0$, $q_1$}
            \State $q_1 = X(q_1)$
            \State $q_0, q_1 = (H^{\otimes n} \otimes H)(q_0, q_1)$
            \State $q_0, q_1 = U_f(q_0, q_1)$
            \State $q_0 = H^{\otimes n}(q_0)$
            \State \tbf{return} $\mbox{measure}(q_0)$
        \EndFunction
    \end{algorithmic}
\end{framedalgo}

Note that in this algorithm $q_0$ are actually $n$ qubits, thus $q_0$ is initially set to $\ket{0^n}$. Before proving the correctness of this general version of the algorithm, let us first take a look at the quantum circuit that defines it.

TODO \todo{drawing}

For our discussion we will call $\B = \{0, 1\}$.

\claim[Claim 1]{
    $\forall a \in \B \quad H \ket a = \tfrac{1}{\sqrt 2} \sum_{b \in \B}{(-1)^{a \cdot b} \ket b}$.
}{
    We observe that $$H \ket 0 = \ket + = \dfrac{1}{\sqrt 2}(\ket 0 + \ket 1) = \dfrac{1}{\sqrt 2} \sum_{b \in \B}{(-1)^{0 \cdot b} \ket b}$$ and analogously $$H \ket 1 = \ket - = \dfrac{1}{\sqrt 2}(\ket 0 - \ket 1) = \dfrac{1}{\sqrt 2} \sum_{b \in \B}{(-1)^{1 \cdot b} \ket b}$$
}

In the following claim, we will denote with the $\cdot$ symbol the \curlyquotes{canonical} scalar product, i.e. $$\forall x, y \in \B^n \quad x \cdot y := \sum_{i = 1}^n{x_i y_i}$$

\claim[Claim 2]{
    $\forall x \in \B^n \quad H^{\otimes n}\ket x = \tfrac{1}{\sqrt 2 ^n} \sum_{a \in \B^n}{(-1)^{x \cdot a} \ket a}$
}{
    By the previous claim, we have that
    \begin{equation*}
        \begin{alignedat}{2}
            H^{\otimes n} \ket x & = \bigotimes_{i = 1}^n{H \ket{x_i}} & \\ 
                                 & = \bigotimes_{i = 1}^n{\rbk{\dfrac{1}{\sqrt 2} \sum_{b \in \B}{(-1)^{x_ib} \ket b}} \ket{x_i} }  & \quad (\mbox{by Claim 1}) & \\ 
                                 & = \dfrac{1}{\sqrt 2^n} \bigotimes_{i = 1}^n{\rbk{\ket 0 + (-1)^{x_i} \ket 1}} & \\ 
                                 & = \dfrac{1}{\sqrt 2^n} \sum_{a \in \B^n}{(-1)^{x \cdot a} \ket a } & \\ 
        \end{alignedat}
    \end{equation*}
}

Finally, we are ready to prove the correctness of the algorithm.

\begin{equation*}
    \begin{alignedat}{2}
        & q_0 \otimes q_1 & \\ 
        = & \ket{0^n} \otimes \ket 0 & \\ 
        \xrightarrow{X(q_1)} & \ket{0^n} \otimes \ket 1 & \\ 
        \xrightarrow{H^{\otimes n}(q_0, q_1)} & H^{\otimes n} \ket{0^n} \otimes H \ket 1 & \\ 
        = & \dfrac{1}{\sqrt 2^n} \sum_{a \in \B^n}{(-1)^{0^n \cdot a} \ket a} \otimes \ket - & \quad (\mbox{by Claim 2}) \\ 
        = & \dfrac{1}{\sqrt 2^n} \sum_{a \in \B^n}{\ket a} \otimes \ket -  & \\ 
        = & \dfrac{1}{\sqrt 2^n } \sum_{a \in \B^n}{(\ket a \otimes \ket - )} & \\ 
        \xrightarrow{U_f(q_0, q_1)} & \dfrac{1}{\sqrt 2^n} \sum_{a \in \B^n}{\rbk{(-1)^{f(a)} \ket a \otimes \ket - }} & \quad (\mbox{by \cref{U lemma}}) \\ 
        = & \dfrac{1}{\sqrt 2 ^n} \sum_{a \in \B^n}{(-1)^{f(a)} \ket a} \otimes \ket - & \\ 
        \xrightarrow{H^{\otimes n}(q_0)} & H^{\otimes n} \rbk{\dfrac{1}{\sqrt 2 ^n} \sum_{a \in \B^n}{(-1)^{f(a)} \ket a }} \otimes \ket - & \\ 
        = & \dfrac{1}{\sqrt 2 ^n} \sum_{a \in \B^n}{\rbk{(-1)^{f(a)} H^{\otimes n} \ket a }} \otimes \ket - & \\ 
        = & \dfrac{1}{\sqrt 2 ^n} \sum_{a \in \B^n}{(-1)^{f(a)} \rbk{\dfrac{1}{\sqrt 2^n} \sum_{b \in \B^n} {(-1)^{a \cdot b} \ket b}}} \otimes \ket - & \quad (\mbox{by Claim 2}) \\ 
        = & \dfrac{1}{2^n} \sum_{a \in \B^n}{\sum_{b \in \B^n}{(-1)^{f(a) + a \cdot b}} \ket b } \otimes \ket - & \\ 
        = & \dfrac{1}{2^n} \sum_{a \in \B^n}{\sum_{b \in \B^n}{(-1)^{f(a) + a \cdot b}} \ket b } \otimes \ket - & \\ 
        = & \sum_{b \in \B^n}{\rbk{\dfrac{1}{2^n}\sum_{a \in \B^n}{(-1)^{f(a) + a \cdot b}}} \ket b_0} \otimes \ket -_1 & \\
    \end{alignedat}
\end{equation*}

Now note that this state describes the superposition of the system, but the next step of the algorithm will only measure $q_0$, therefore we can ignore $\ket -_1$ and just focus on the amplitudes of $q_0$. Then, by calling $$\forall b \in \B^n \quad \alpha_b := \dfrac{1}{2^n} \sum_{a \in \B^n}{(-1)^{f(a) + a \cdot b}}$$ we can rewrite $q_0$ as follows $$q_0 = \sum_{b \in \B^n}{\alpha_b \ket b}$$ Finally, since we want to determine the probability that $q_0$ collapses into the state $\ket{0^n}$ specifically, we can easily evaluate the associated amplitude of the latter, i.e. $$\alpha_{0^n} = \dfrac{1}{2^n}\sum_{a \in \B^n}{(-1)^{f(a)+ a \cdot 0^n}} = \dfrac{1}{2^n} \sum_{a \in \B^n}{(-1)^{f(a)}}$$ From this, we can easily conclude that:

\begin{itemize}
    \item if $f$ is constant, then $$\alpha_{0^n} = \dfrac{1}{2^n} \sum_{a \in \B^n}{(-1)^{f(a)}} = \dfrac{1}{2^n} \cdot 2^n \cdot (-1)^{b} = (-1)^b$$ where $b \in \B$, meaning that it is guaranteed that $q_0$ will collapse to $0^n$
    \item if $f$ is balanced, then $$\alpha_{0^n} = \dfrac{1}{2^n} \sum_{a \in \B}{(-1)^{f(a)}} = \dfrac{1}{2^n} \cdot 0 = 0$$ meaning that it is guaranteed that $q_0$ will \tit{not} collapse to $0^n$
\end{itemize}

\section{Grover's algorithm}

Even though Deutsch-Josza algorithm is able to solve the decision problem we described through \tit{one} single evaluation of $f$, it is fairly apparent that the problem their algorithm solves is quite artificial. However the next algorithm that we are going to discuss solves a problem that is definitely more useful.

In 1997, \textcite{grover} published a landmark paper called \curlyquotes{Quantum Mechanics Helps in Searching for a Needle in a Haystack}, and the name already suggests the problem his work tried to solve: the search problem. The setting is the following: we are given an array of $N$ elements --- we can assume that $N$ is always a power of 2 for some $n$, i.e. $N= 2^n$ --- that contains $M$ \curlyquotes{solution} elements. However, we do not know their positions, and the problem asks to find the index of any solution element.

More formally, given a Boolean function $$\funcmap{f}{\{0, \ldots, N - 1\}}{\B}{x}{\soe{ll}{1 & A[x] \in S \\ 0 & \mbox{otherwise}}}$$ where $A$ is our array, and $S$ is the set of solution elements, the problem asks to return an $x$ such that $f(x) = 1$, i.e. such that $A[x]$ is a solution.

With a classical computation, it is easy to see that we need $O \rbk{\dfrac{N}{M}}$ accesses to $A$ to solve our problem, however we will see that the algorithm Grover developed is able to return a \curlyquotes{solution index} in $O\rbk{\sqrt{\dfrac{M}{N}}}$ with \tit{high probability}, i.e. Grover's algorithm provides a \tbf{quadratic} speedup compared to any classical algorithm --- however, it is probabilistic.

Before explaining the details of the algorithm, we need to define some new operators that will be used in Grover's algorithm, and introduce some general notation:

\begin{itemize}
    \item given an arbitrary qubit $\ket \psi$, we will write its superposition of states as follows $$\ket \psi = \sum_{b \in \B^n}{\alpha_b \ket b}$$ where $\sum_{b \in \B^n}{\abs{\alpha_b}} = 1$
    \item we define an operator $O_f$ (where $f$ is the indicator function of the array defined before) that computes as follows: $$\forall x \in \B^n \quad O_f \ket x := (-1)^{f(x)} \ket x$$
    \item given an arbitrary qubit $\ket \psi$, we define a new operator $W$ as follows: $$W := 2 \ket s \bra s - I$$ where $\ket s$ is the \tbf{uniform superposition}, a superposition of states in which each amplitude is equally likely $$\ket s = \dfrac{1}{\sqrt N} \sum_{x \in \B^n}{\ket x}$$
    \item finally, we will define an operator $G$ that will simply compose the last two operators we described $$G = W \cdot O_f$$
\end{itemize}

\begin{framedalgo}{Grover's algorithm}
    Given a Boolean function $f$ that describes the solution elements of an array having $M$ solution elements, and $n$ qubits, the algorithm returns a solution index with high probability TODO SPIEGA CHE VUOL DIRE. \\
    \hrule

    \quad
    \begin{algorithmic}[1]
        \Function{Grover}{$f$, $q_0$}
            \State $q_0 = H^{\otimes n}(q_0)$
            \For{$i \in \sbk{1, O\rbk{\sqrt{\tfrac{N}{M}}}}$} \Comment{where $N = 2^n$}
                \State $q_0 = G(q_0)$
            \EndFor
            \State \tbf{return} $\mbox{measure}(q_0)$
        \EndFunction
    \end{algorithmic}
\end{framedalgo}

First of all, we see that this algorithm takes only $n$ qubits as input, however the actual implementation of the algorithm is slightly different, as shown in the following quantum circuit:

TODO \todo{drawing}

Indeed, we can see that the real quantum circuit takes $n + 1$ inputs, and the additional register is called \curlyquotes{ancilla} becuase its only purpose is to actually implement the $O_f$ operator, which is designed exactly as if it was the $U_f$ black-box we discussed in previous algorithms. This can be done thanks to \cref{U lemma}, which guarantees that if we give $\ket -$ as the second input to $U_f$ we get $$O_f \ket x \otimes \ket -  = (-1)^{f(x)} \otimes \ket - $$ which implies that the ancilla register will sill be $\ket -$, therefore we can just ignore the second register completely throughout the whole computation, and we are sure that $O_f$ computes correctly.

Furthermore, before proceeding, let us prove that $G$ is actually a unitary operator, i.e. that this is a valid quantum computation.

\claim[Claim]{
    The operator $G$ is unitary.
}{
    By \cref{unitary prod} we know that proving that $W$ and $O_f$ are unitary is sufficient to prove the claim. \todo{todo da finire}
}

Now that we know this operator is unitary, we can delve into the details of the algorithm. To see what happens at each iteration, we will describe the complete state of the system in a rather unusual way. Let $\ket a$ and $\ket b$ be the following superpositions: $$\ket a := \dfrac{1}{\sqrt{N - M}} \sum_{x \in \overline S}{\ket x} \quad \quad \ket b := \dfrac{1}{\sqrt M}\sum_{x \in S}{\ket x}$$ In other words, $\ket a$ is the uniform superposition of non-solution indices, and $\ket b$ is the superposition of solution ones. To explain the $\tfrac{1}{\sqrt M}$ factor in front of $\ket b$, we recall that in quantum mechanics all state vectors must be normalized, and since the squared norm of $\ket {\tilde b}$ (which will denote $\ket b$ without the multiplicative factor in front) is equal to
\begin{equation*}
    \begin{alignedat}{2}
        \braket{\tilde b| \tilde b} & = \rbk{\sum_{x \in S}{\ket x}}^\dag \rbk{\sum_{x \in S}{\ket x}} & \\ 
                                    & = \rbk{\sum_{x \in S}{\bra x}} \rbk{\sum_{y \in S}{\ket y}} & \\ 
                                    & = \sum_{x \in S}{\sum_{y \in S}{\braket{x|y}}} & \\ 
                                    & = \sum_{x \in S}{\sum_{y \in S}{\delta_{x,y}}} & \quad (\mbox{basis states are orthonormal}) \\ 
                                    & = \sum_{x \in S}{1} & \\ 
                                    & = \abs{S} \\ 
                                    & = M
    \end{alignedat}
\end{equation*}
the norm of $\ket{\tilde b}$ is $\sqrt M$, hence to normalize it it suffices to add $\tfrac{1}{\sqrt M}$ in front of $\ket{\tilde b}$--- the reasoning for $\ket a$ is analogous. Moreover, it's easy to see that $\ket a$ and $\ket b$ are orthogonal, so they form an orthonormal basis for a 2D space.

Now, because of how we defined $\ket a$ and $\ket b$, we can rewrite the uniform superposition $$\ket s = \dfrac{1}{\sqrt N} \sum_{x \in \B}{\ket x}$$ as shown below $$\ket s = \sqrt{\dfrac{N - M}{N}} \ket a + \sqrt{\dfrac{M}{N}} \ket b$$ This is quite interesting, because it means that we can describe $\ket s$ in terms of $\ket a$ and $\ket b$. Let us do exactly this, and plot the resulting graph on a 2D space that has $\ket a$ and $\ket b$ as axis.

\centeredsvg{0.5}{../assets/grover1}

We observe that:

\begin{itemize}
    \item both $\ket s$, $\ket a $ and $\ket b$ are normalized, and all the vectors that we are going to consider are quantum states, so they will be normalized too, therefore we can restrict our focus on a 2D circumference of radius 1 --- this plane is usually called \tbf{Grover plane}
    \item since we expect that $M \ll N$, we have that $\sqrt{\tfrac{M}{N}} \ll \sqrt{\tfrac{N - M}{N}}$, which basically means that the vector $\ket s$ is almost parallel to $\ket a$ (the bigger is the numer of solution indices, the bigger the angle between $\ket s$ and $\ket a$)
\end{itemize}

Consider any state $\ket \psi = \sum_{x \in \B^n}{\alpha_x \ket x}$; we observe that
\begin{equation*}
    \begin{split}
        O_f \ket \psi & = O_f \sum_{x \in \B^n}{\alpha_x \ket x}  \\ 
                      & = \sum_{x \in \B^n}{\alpha_x O_f \ket x} \\ 
                      & = \sum_{x \in \B^n}{\alpha_x (-1)^{f(x)} \ket x} 
    \end{split}
\end{equation*}
which basically means that each time we apply the $O_f$ operator we are flipping the sign of the amplitudes of the compoents of $\ket \psi$ that represent solution indices. In other words, the $O_f$ operator flips its input w.r.t. $\ket a$.

Moreover, we observe that any state $\ket \psi$ can be decomposed into $$\ket \psi = \alpha \ket s + \ket{\psi_\bot}$$ where $\alpha \ket s$ is the projection of $\ket \psi$ along $\ket s$'s space --- thus $\alpha = \braket{s|\psi}$ --- and $\ket {\psi_\bot}$ is the projection of $\ket \psi$ along the space that is orthogonal to $\ket s$'s. Therefore, we have that
\begin{equation*}
    \begin{split}
        W \ket \psi & = W(\alpha \ket s + \ket{\psi_\bot}) \\ 
                    & = 2 \ket s \bra s (\alpha \ket s + \ket{\psi_\bot}) - (\alpha \ket s + \ket{\psi_\bot}) \\ 
                    & = 2 \alpha \ket s \braket{s|s} + 2 \ket s \braket{s|\psi_\bot} - (\alpha \ket s + \ket{\psi_\bot}) \\ 
                    & = 2 \alpha \ket s \cdot 1 + 0 - (\alpha \ket s + \ket{\psi_\bot}) \\ 
                    & = \alpha \ket s - \ket{\psi_\bot}
    \end{split}
\end{equation*}
which means that what $W$ actually performs is leaving the component along $\ket s$'s space unchanged, and it flips the sign of the component of the orthogonal space. In other words, what $W$ computes is the reflection of $\ket \psi$ w.r.t. $\ket s$.

We can finally describe Grover's algorithm in detail. First, we see that
\begin{equation*}
    \begin{split}
        & q_0 \\ 
        = & \ket{0^n} \\ 
        \xrightarrow{H^{\otimes(q_0)}} & \dfrac{1}{\sqrt{2^n}}\sum_{x \in \B^n}{(-1)^{0^n \cdot x} \ket x} \\ 
        = & \dfrac{1}{\sqrt N} \sum_{x \in \B^n}{\ket x} \\ 
        = & \ket s
    \end{split}
\end{equation*}

Indeed, the only purpose of the first Hadamard operator is to \curlyquotes{move} the initial state slightly away from $\ket a$ --- and also making each component initially equally likelly. Now, let's see what happens at each application of the $G = W \cdot O_f$ operator:

\begin{itemize}
    \item as previously shown, the operator $O_f$ reflects its input w.r.t. the $\ket a$ axis --- below we show what happens when we first apply $O_f \ q_0 = O_f \ket s$: \centeredsvg{0.5}{../assets/grover2}
    \item additionally, as previously explained the operator $W$ reflects its input w.r.t. the axis described by the $\ket s$ vector, thus when we compute $W \cdot O_f \ q_0$ we will end up with the following vector: \centeredsvg{0.5}{../assets/grover3}
    \item this suggests that each time we apply the operator $G$ we are making $q_0$ closer and closer to $\ket b$, as depicted below: \centeredsvg{0.5}{../assets/grover4}
\end{itemize}

This is the core idea of Grover's algorithm: if we rewrite $q_0$ in terms of $\ket a$ and $\ket b$ $$q_0 = \beta_0 \ket a + \beta_1 \ket b$$ we get that through Grover's algorithm we transformed $q_0$ such that it is now very close to $\ket b$, meaning that $\beta_0 \ll \beta_1$. This direcly implies that when we will measure $q_0$ at the end of Grover's procedure the likelihood that it will collapse into some $\ket x$ that is a component of $\ket b$ --- i.e. a solution index --- is very high. In other words, what happens with Grover's algorithm is that we gradually increase the amplitudes of the solution indices, in order to maximize the probability that our qubit will collapse in one them when it will be measured at the end of the procedure.

Now that we know how Grover's algorithm works, the only thing left to discuss is the $O \rbk{\sqrt{\dfrac{N}{M}}}$. Why is it guaranteed that after this amount of applications of the $G$ operator we are done with the algorithm? Well, we actually have the opposite problem: in reality, we have to \tit{stop early enough}. Consider again how Grover's algorithm operates in the Grover plane; clearly, if we apply $G$ too many times, what happens is that $q_0$ will end up past $\ket b$ itself: \todo{drawing}

Let the angle between $\ket a$ and $\ket s$ be $\theta$; since $O_f$ flips $q_0$ w.r.t. $\ket a$, and $W$ flips $O_f \ q_0$ w.r.t. $\ket s$, $G$ will cumulatively rotate $q_0$ by $2 \theta$: \todo{drawing} Indeed, with each application of $G$ we are rotating $q_0$ by $2 \theta$, which means that at the $k$-th application it holds that $$G^k \ q_0 = \cos (2k + 1) \theta \ket a + \sin (2k + 1) \theta \ket b$$ for any $k \in \N$, where the additional 1 comes from the fact that $q_0 = \ket s$ through the Hadamard transformation at the start of the process. Thus, to evaluate the optimal number of iterations we need to find the optimal $k$, i.e. the one that maximizes the probability of measuring a solution index, which is equal to the squared amplitude of $\ket b$, namely $$\Pr[\mbox{measure}(q_0) = \ket b] = \sin^2(2k + 1) \theta$$ Hence, we have that $\sin^2(2k + 1)\theta = 1$ when $(2k +1) \theta = \tfrac{\pi}{2}$, and solving for $k$ we get that $$k = \dfrac{\pi}{4 \theta} - \dfrac{1}{2}$$ Lastly, since $\theta$ is the angle between $\ket a$ and $\ket s$, we can rewrite $\ket s$ as $$\ket s = \cos \theta \ket a + \sin \theta \ket b$$ which directly implies that $$\sin \theta = \sqrt {\dfrac{M}{N}} \iff \theta = \arcsin \sqrt{\dfrac{M}{N}}$$ and therefore
\begin{equation*}
    \begin{split}
        k & = \dfrac{\pi}{4 \theta} - \dfrac{1}{2}  \\
          & = \dfrac{\pi}{4 \arcsin \sqrt{\dfrac{M}{N}}} - \dfrac{1}{2}  \\  
                                               & \le  \dfrac{\pi}{4 \arcsin \sqrt{\dfrac{M}{N}}} \\ 
                                               & \le \dfrac{\pi}{4 \sqrt{\dfrac{M}{N}}} \\ 
                                               & = \dfrac{\pi}{4} \sqrt{\dfrac{N}{M}} \\ 
                                               & = O \rbk{\sqrt{\dfrac{N}{M}}}
   \end{split}
\end{equation*}
which finally explains the quadratic speedup of Grover's algorithm w.r.t. the classical version of the problem.

As a final note, we observe that Grover's algorithm assumes that $\theta \le \tfrac{\pi}{4}$, otherwise we overshoot $\ket b$ with a single iteration of the $G$ operator --- since $\theta > \tfrac{\pi}{4}$ would mean that $\ket s$ is placed on the \curlyquotes{upper half} of the Grover plane. However, to ensure this constraint on $\theta$ we only need that $M \le \tfrac{N}{2}$, i.e. at most half of the elements in our array are solution elements. Indeed, we see that $$M \le \dfrac{N}{2} \implies \sin \theta = \sqrt{\dfrac{M}{N}} \le \sqrt{\dfrac{1}{2}} \implies \theta \le \arcsin \sqrt{\dfrac{1}{2}} = \dfrac{\pi}{4}$$ What can we do if the number of solutions is more than half the size of the array? We simply invert the problem and find the non-solutions!

\subsection{Another perspective}

Interestingly enough, we can look at what happens to the amplitudes of $q_0$ from another perspective. Usually, the $W$ operator is called \tbf{diffusion operator}, and what it does is computing the so called \tit{inversion about the mean} of its input. Consider any state $\ket \psi = \sum_{x \in \B^n}{\alpha_x \ket x}$, and observe that
\begin{equation*}
    \begin{alignedat}{2}
        \braket{s|\psi} & = \rbk{\dfrac{1}{\sqrt N} \sum_{y \in \B^n}{\ket y}}^\dag \rbk{\sum_{x \in \B^n}{\alpha_x \ket x}} & \quad (\mbox{since $\bra s = \ket s ^\dag$})\\ 
                        & = \rbk{\dfrac{1}{\sqrt N} \sum_{y \in \B^n}{\bra y}} \rbk{\sum_{x \in \B^n}{\alpha_x \ket x}} & \\ 
                        & = \dfrac{1}{\sqrt N} \sum_{y \in \B^n}{\sum_{x \in \B^n}{\alpha_x \braket{y|x}}} & \\ 
                        & = \dfrac{1}{\sqrt N} \sum_{y \in \B^n}{\sum_{x \in \B^n}{\alpha_x \delta_x,y}} & \quad (\mbox{basis states are orthonormal}) \\ 
                        & = \dfrac{1}{\sqrt N} \sum_{x \in \B^n}{\alpha_x \sum_{y \in \B^n}{\delta_x,y}} & \\ 
                        & = \dfrac{1}{\sqrt N} \sum_{x \in \B^n}{\alpha_x} & \\ 
                        & = \sqrt N \overline{\alpha_\psi}
    \end{alignedat}
\end{equation*}
where $\overline{\alpha_\psi}$ is the average amplitude of $\ket \psi$. This implies that
\begin{equation*}
    \begin{alignedat}{2}
        W \ket \psi & = (2 \ket s \bra s - I) \ket \psi & \\ 
                    & = 2 \ket s \braket{s|\psi} - \sum_{x \in \B^n}{\ket x} & \\  
                    & = 2 \ket s \rbk{\sqrt N \overline{\alpha_\psi}} - \sum_{x \in \alpha_x \ket x} & \quad (\mbox{for the previous observation}) \\ 
                    & = 2 \rbk{\dfrac{1}{\sqrt N} \sum_{y \in \B^n}{\ket y}} \sqrt N \overline{\alpha_\psi} - \sum_{x \in \B^n}{\alpha_x \ket x} & \\
                    & = \sum_{x \in \B^n}{\rbk{2 \overline{\alpha_\psi} - \alpha_x} \ket x} & \\ 
    \end{alignedat}
\end{equation*}
In other words, when we apply $W$ to a superposition of states, what happens is that each amplitude of the basis states is trasformed as follows: $$\func{W}{\alpha_x}{2 \overline{\alpha_\psi} - \alpha_x}$$ To understand why this is important, let's look at what happens in Grover's algorithm after the first Hadamard application. TODO \todo{da finire}

\subsection{Fixed-Point Quantum Search}

As we saw in the previous sections, with Grover's algorithm we need to be careful on how many times we apply the $G$ operator, however we observe that the right value for $k$ --- i.e. the numeber of iterations --- strictly depends on both $N$ and $M$. Assuming that $N$, the length of the array, is known, it is not guaranteed that we know $M$ too (the number of solutions in the array). Therefore, if we want to apply Grover's algorithm we also need to be able to to a rough estimate on $M$, otherwise we might end up stopping either too early or too late. This makes Grover's algorithm fragile in some real-world scenarios.

Grover was actually aware of this problem, and in 2005 he designed a new algorithm which is able to solve this issue \cite{grover2}. The idea of the algorithm is to \tit{monotonically} increase the probability of finding a solution, without oscillating back down $\ket a$. This algorithm is called \tbf{Fixed-Point Quantum Search}, becuase the solutions actually become a \curlyquotes{stable fixed point} of the transformation --- i.e. once you reach a good solution, further iterations leave it basically unchanged. Indeed, with Grover's search each iteration is a constant-angle rotation, while in fixed-point search we will see that the phase angles in each rotation changes such that the rotation angle decreases over time.

First, we need to define two new operators: $$R_s := I - (1 - e^{i \theta}) \ket s \bra s \quad \quad R_t := I - (1 - e^{i \theta}) \ket t \bra t$$ where $\ket s$ is the starting state and $\ket t$ is the target state (in Grover's algorithm this was $\ket b$) --- this is the original notation that Grover used in his paper, and actually explains why we used $\ket s$ in the previous version of the algorithm, it's just the \curlyquotes{start}. These two operators are called \tbf{phase shift operators}; as usual, before proceeding we need to show that both operators are actually unitary.

\claim{
    $R_t$ and $R_s$ are unitary operators.
}{
    TODO \todo{todo}
}

What are these two operators in the first place? When we presented the $W$ operator, we also noticed how it actually performes a reflection of its input w.r.t. the space of $\ket s$. Well, through a very similar argument it can be shown that $$W^\bot := I - 2 \ket s \bra s$$ performs a reflection of its input w.r.t. the space \tit{orthogonal} to $\ket s$ --- indeed, we end up with $$W^\bot \ket \psi = \ket{\psi_\bot} - \alpha \ket s$$ If we now look at the $R_s$ operator, we can see that when we actually compute the reflection it performs we end up with $$R_s \ket \psi = \ket{\psi_\bot} + e^{i \theta} \alpha \ket s$$ This suggests that what $R_s$ actually computes is a \curlyquotes{soft reflection} w.r.t. the space perpendicular to $\ket s$. Through an analogous argument, we can see that $$R_t \ket \psi= \ket{\psi_\bot} + e^{i \theta} \alpha \ket t $$ meaning that $R_t$ computes a \curlyquotes{soft reflection} w.r.t. the space perpendicular to $\ket t$ --- however, we observe that the latter is literally the space of $\ket a$, indeed $O$ in Grover's algorithm could have been defined as $$O = I - 2 \ket b \bra b$$ and indeed it is sometimes defined such.

Now, we are going to define an addition operator called $U$ as such: let $U$ be any unitary operator such that, for some small $\varepsilon > 0$, it holds that $$\abs{\braket{t|Us}}^2 = 1 - \varepsilon$$ We observe that

\begin{itemize}
    \item by the laws of quantum mechanics we have that $$\Pr[\mbox{measure}\rbk{U \ket s} = \ket t] = \abs{\braket{t|Us}}^2$$ therefore we require $U$ to be an operator such that it \curlyquotes{drives $\ket s$ close to $\ket t$ with high probability} --- i.e. $1 - \varepsilon$
    \item we know that $U$ exists since it's just a rotation in a 2D space
    \item also, a geometric interpretation of the scalar product is that we are measuring the cosine of the angle between $\ket t$ and $\ket{Us}$, and we want this value to be very high (such that the angle woule be very small) --- we obeserve that we are in Hilbert spaces so the notion of \curlyquotes{angle} is not the same of the one we are used to with Euclidean spaces, but this is just to have an idea of what is happening with $U$
\end{itemize}

Moreover, define the following operator $$G := U R_s U^\dag R_t U$$ Let's see what happens when we evaluate $G \ket s$. By denoting $U_{ts} = \braket{t|Us}$, we can prove the following equality.

\begin{framedlem}{}
 It holds that $$G \ket s = U \ket s \sbk{e^{i\theta} + \abs{U_{ts}}^2  \rbk{e^{i \theta }-1}^2} + \ket t U_{ts}\rbk{e^{i \theta} - 1}$$
\end{framedlem}

\begin{proof}
    First, we prove the following equality.

    \claim{
        It holds that $\bra s U^\dag \ket t U_{ts} = \abs{U_{ts}}^2$.
    }{
        \begin{equation*}
            \begin{alignedat}{2}
                \bra s U^\dag \ket t U_{ts} & = \braket{s|U^\dag t} U_{ts} & \\ 
                         & = \braket{Us|t} U_{ts} & \quad (\mbox{by def. of adjoint}) \\ 
                         & = \overline{\braket{t|Us}} U_{ts} & \quad (\mbox{by prop. of scalar products}) \\ 
                         & = \overline{U_{ts}} U_{ts} & \\ 
                         & = \abs{U_{ts}}^2 & \quad (z \cdot \overline z = \abs{z}^2)\\ 
            \end{alignedat}
        \end{equation*}
    }

    Let $P_s = \ket s \bra s$ and $P_t = \ket t \bra t$; thus, we have that
    \begin{equation*}
        \begin{alignedat}{2}
            G & = U R_s U^\dag R_t U & \\ 
              & = U(I - (1 - e^{i\theta})P_s)U^\dag(I - (1 - e^{i\theta})P_t)U & \\ 
              & = (U - (1 - e^{i\theta})UP_s)U^\dag(U - (1 - e^{i\theta})UP_t) & \\ 
              & = (UU^\dag - (1 - e^{i\theta}) UP_sU^\dag)(U - (1 - e^{i\theta})UP_t) & \\ 
              & = (I - (1 - e^{i \theta})UP_sU^\dag)(U - (1 - e^{i\theta})UP_t) & \quad (UU^\dag = I) \\ 
              & = U - (1 - e^{i \theta}) P_tU - (1 - e^{i \theta})UP_s + (1 - e^{i\theta})^2 UP_s U^\dag P_t U & \\ 
        \end{alignedat}
    \end{equation*}
    Therefore, we find that $G \ket s$ can be evaluated as follows:
    \begin{equation*}
        \hspace{-1cm}
        \begin{alignedat}{2}
            G \ket s & = \rbk{U - (1 - e^{i \theta}) P_tU - (1 - e^{i \theta})UP_s + (1 - e^{i\theta}) UP_s U^\dag P_t U} \ket s & \\ 
                     & = U \ket s - (1 - e^{i \theta}) P_tU \ket s- (1 - e^{i \theta})UP_s \ket s + (1 - e^{i\theta})^2 UP_s U^\dag P_t U \ket s & \\  
                     & = U \ket s - (1 - e^{i \theta}) \ket t U_{ts} - (1 - e^{i\theta}) U \ket s + (1 - e^{i\theta})^2 U \ket s \bra s U^\dag \ket t U_{ts} & \\ 
                     & = U \ket s - (1 - e^{i \theta}) \ket t U_{ts} - (1 - e^{i\theta}) U \ket s + (1 - e^{i\theta})^2 U \ket s \abs{U_{ts}}^2 & \quad (\mbox{by the claim}) \\ 
                     & = \ldots & \quad (\mbox{algebraic manipulation}) \\ 
                     & = U \ket s \sbk{e^{i\theta} + \abs{U_{ts}}^2  \rbk{e^{i \theta }-1}^2} + \ket t U_{ts}\rbk{e^{i \theta} - 1} & \\ 
        \end{alignedat}
    \end{equation*}
\end{proof}

Most importantly, this equality can be used in the following proposition, which shows that we can actually find an angle $\theta$ for which the distance from $\ket t$ decreases significantly.

\begin{framedprop}{}
    There exists an angle $\theta$ such that $$\Pr[\mbox{measure}(G \ \ket s) = \ket t] = 1 - \varepsilon^3$$
\end{framedprop}

\begin{proof}
    Thanks to the previous lemma, we obtain that
    \begin{equation*}
        \hspace{-1cm}
        \begin{alignedat}{2}
            \Pr[\mbox{measure}\rbk{G \ket s} = \ket t] & = \abs{\braket{t|Gs}}^2 & \\ 
                                                       & = \abs{\bra t \sbk{U \ket s \rbk{e^{i\theta} + \abs{U_{ts}}^2  \rbk{e^{i \theta} - 1}^2} + \ket t U_{ts}\rbk{e^{i \theta} - 1}}}^2 & \\ 
                                                       & = \abs{\braket{t|Us} \rbk{e^{i\theta} + \abs{U_{ts}}^2  \rbk{e^{i \theta } - 1}^2 }+ \braket{t|t} U_{ts}\rbk{e^{i \theta} - 1}}^2 & \\ 
                                                       & = \abs{U_{ts} \rbk{e^{i\theta} + \abs{U_{ts}}^2  \rbk{e^{i \theta } - 1}^2 }+ U_{ts}\rbk{e^{i \theta} - 1}}^2 & \\ 
                                                       & = \abs{U_{ts} \sbk{2e^{i\theta} - 1 + \abs{U_{ts}}^2 \rbk{e^{i \theta} - 1}^2}}^2 & \\ 
                                                       & = \abs{U_{ts}}^2 \cdot \abs{2e^{i\theta} - 1 + \abs{U_{ts}}^2 \rbk{e^{i \theta} - 1}^2}^2 & \\ 
                                                       & = (1 - \varepsilon) \cdot \abs{2e^{i\theta} - 1 + (1 - \varepsilon)\rbk{e^{i \theta} - 1}^2}^2 & \quad \rbk{\abs{U_{ts}}^2 = 1 - \varepsilon}\\ 
        \end{alignedat}
    \end{equation*}

    Now, let's see what happens if we set $\theta = \tfrac{\pi}{3}$: we obtain that
    \begin{equation*}
        \begin{alignedat}{2}
            \Pr[\mbox{measure}\rbk{G \ket s} = \ket t] & = (1 - \varepsilon) \cdot \abs{2e^{i \tfrac{\pi}{3}} - 1 + (1 - \varepsilon)\rbk{e^{i\tfrac{\pi}{3}} - 1}^2}^2 & \\ 
                                                       & = (1 - \varepsilon) \cdot \abs{2e^{i \tfrac{\pi}{3}} - 1 - (1 - \varepsilon){i \tfrac{\pi}{3}}}^2 & \quad \rbk{\rbk{e^{i\tfrac{\pi}{3}} - 1}^2 = - e^{i \tfrac{\pi}{3}}} & \\ 
                                                       & = (1 - \varepsilon) \cdot \abs{(1 + \varepsilon) e^{i \tfrac{\pi}{3}} - 1}^2 & \\ 
                                                       & = (1 - \varepsilon) \cdot \abs{(1 + \varepsilon)\rbk{\dfrac{1}{2} + i \dfrac{\sqrt 4}{2}} - 1}^2 & \quad (\mbox{by Euler's formula}) & \\ 
                                                       & = (1 - \varepsilon) \cdot \abs{\dfrac{1}{2} +i  \dfrac{\sqrt 3}{2} + \dfrac{\varepsilon}{2} + i \dfrac{\sqrt 3}{2} \varepsilon - 1}^2 & \\ 
                                                       & = (1 - \varepsilon) \cdot \abs{\dfrac{\varepsilon}{2} - \dfrac{1}{2} + i \dfrac{\sqrt 3}{2}(1 + \varepsilon)}^2 &  \\ 
                                                       & = (1 - \varepsilon) \cdot \sbk{\rbk{\dfrac{\varepsilon}{2} - \dfrac{1}{2}}^2 + \rbk{\dfrac{\sqrt 3}{2}(1 + \varepsilon)}^2 } & \quad \rbk{\abs{z}^2 = \Re^2(z) + \Im^2(z)} \\ 
                                                       & = (1 - \varepsilon) \cdot (1 + \varepsilon + \varepsilon^2)&  \\ 
                                                       & = 1 + \varepsilon + \varepsilon^2 - \varepsilon - \varepsilon^2 - \varepsilon^3 & \\ 
                                                       & = 1 - \varepsilon^3 & \\ 
        \end{alignedat}
    \end{equation*}

This shows that by applying $G$ the probability of measuring $\ket t$ has increased from $1 - \varepsilon$ to $1 - \varepsilon^3$.
\end{proof}

Indeed, it can be shown that by defining the following recursive sequence of operators $$\soe{ll}{U_0 := U & m = 0 \\ U_m = U_{m - 1} R_s U_{m-1}^\dag R_t U_{m - 1} & m \ge 1}$$ we get that $$\Pr[\mbox{measure}(\ket{U_m} \ket s) = \ket t] = \abk{\braket{t|U_ms}}^2 = 1 - \varepsilon^{2{q_m} + 1}$$ where $q_m$ is the number of queries of $f(x)$.

TODO \todo{drawing of the vector that grows monotonically}

Unfortunately, there already exists a classical probabilistic algorithm that the failure probability drops as $\varepsilon^{q + 1}$  after $q$ queries to $f$. Thus, the quantum advantage with this method is lost.

So, what do we do now? In 2014 \textcite{yoder} proposed a fixed-point quantum serach algorithm that monotonically converges to the target state while still retaining the quadratic advantage of the original Grover's algorithm over classical algorithms. The algorithm involves phase-shift operators that are parametrized with angles different from $\theta = \tfrac{\pi}{3}$, and again involes building a sequence of operators using said phase shifts. However, the details of this result are way beyond the scope of our discussion, so we won't describe the details of their findings.

\section{Quantum Fourier Transform}

Before explaining the next algorithm that we are going to describe, we first need to present a very useful tool that is widely employed in various areas of computer science and mathematics, such as integer multiplication, signal processing (e.g. speech recognition, audio compression) and much more.

\subsection{A recap on the DFT}

The \tbf{Discrete Fourier Transform (DFT)} is a very powerful tool used in many applications and probably most widely known for its use in signal processing. Indeed, the Fourier transform is used to transform a signal defined on the \tit{time domain} into its equivalent in the \tit{frequency domain}. We will briefly explore the most important concepts that define the DFT to have a better understanding in order to the progress with its quantum counterpart.

First, what is the \curlyquotes{time domain}? When we sample a signal, we usually sample such signal through some finite amount of time, and we discretize such time interval --- this is why we consider the Discrete Fourier transform. What we are interested in is to transform such signal into the \curlyquotes{frequency domain}, which essentially means to understand what frequencies of sinusoids contribute to our signal and in which percentage. To do this, we will use some concepts of Linear Algebra. Say that the signal we sampled is discretized into $N$ parts, thus our sample lives in the $\R^N$ space --- it is nothing but a vector of $N$ complex values which describe the signal at each timestep. When we say \curlyquotes{time domain}, what we mean is basically this exact vector, i.e. expressed in the canonical orthonormal basis $$e_0 = \rmat{1 \\ 0 \\ \vdots \\ 0 } \quad e_1 = \rmat{0 \\ 1 \\ \vdots \\ 0} \quad \ldots \quad e_{N - 1} = \rmat{0 \\ \vdots \\ 0 \\ 1}$$ (we will start counting at 0 for convenience). In other words, if our signal is defined by some vector $$\mathbf x = \rmat{t_0 \\ t_1 \\ \vdots \\ t_{N - 1}}$$ it holds that $$\mathbf x = \sum_{n = 0}^{N - 1}{t_n e_n}$$

Then, what is the \curlyquotes{frequency domain}? We need some preliminary observations to describe it. Our goal is to take into account the frequencies that define our signal, therefore we need to move into a space in which frequencies are \curlyquotes{first-class citizens}. The idea is to use each possible sinusoid by varying the frequency, add up their contributions, and weight the latter in the precise way that allows us to reconstruct our original signal. Let us fix a component $n \in [0, N - 1]$, and first consider only cosinusoids, for instance $$\cos(2 \pi \cdot 0 \cdot n) \quad \cos(2 \pi \cdot 1 \cdot n) \quad \cos(2 \pi \cdot 2 \cdot n) \quad \ldots \quad \cos(2 \pi \cdot (N - 1) \cdot n)$$ Basically, we are trying to construct a basis built on each possible frequency $f$ by obtaining a basis vector $\cos(2 \pi \cdot f \cdot n)$. Moreover, we will consider $\cos (2 \pi m n / N)$ for $m \in [0, N - 1]$ since it can be shown that the number of possible frequencies that can be represented on a $N$-sized time window is exactly $N$, and to scale the frequency accordingly we just need to divide $m$ over $N$, the size of the sample.

Is this enough to reach our goal? Can we describe any signal in this way? Well, we observe that $m$ is ranging from $0$ to $N -1$, but $\cos(- \theta) = \cos(\theta)$, which implies that $$m > \dfrac{N}{2} \implies \cos(2 \pi m n/ N) = \cos(2 \pi (N - m) n / N)$$ In other words, basically half of our basis vectors add no information at all. Indeed, the span of the vectors we chose has size $$\dim \rbk{\mbox{span}\rbk{\cbk{\cos(2 \pi \cdot m\cdot n/ N)}_{m = 1}^{N - 1}}} = \soe{ll}{\tfrac{N}{2} + 1 & \mbox{$N$ is even} \\ \tfrac{N + 1}{2} & \mbox{$N$ is odd}} $$ where the last added 1 comes from the fact that when $m = 0$ we generate $\cos(0) = 1$ which is really linearly independent from the others cosines. This suggests that cosinusoids alone are not enough to describe our space.

Hence, the most natural thing that we can do is add the contributions of sinusoids as well. A geometric interpretation of the fact that cosinusoids are not enough is that sinusoids are just phase-shifted cosines, but the shift in phase is not captured by changing the frequencies of the cosines alone. We need the contributions of some \curlyquotes{altered} cosinusoids --- in terms of phases --- to get an actual basis and be able to represent any possible vector. Hence, let's consider additional $N$ sinusoids $$\sin(2 \pi \cdot 0 \cdot n) \quad \sin(2 \pi \cdot 1 \cdot n) \quad \sin(2 \pi \cdot 2 \cdot n) \quad \ldots \quad \sin(2 \pi \cdot (N - 1) \cdot n)$$ Do we get a base of size more than $N$ then? We observe that $\sin(\theta) = - \sin (-\theta)$, therefore we have that $$m > \dfrac{N}{2} \implies \sin(2 \pi m n/N) = - \sin (2\pi (N - m) n/N)$$ which again it implies that half of these sinusoids add no useful information. However, in this case we also have the fact that $$\sin(2 \pi \cdot 0 \cdot n /M) =  \sin(0)  = 0$$ and for $N$ even it also happens that $$\sin(2\pi \cdot (N/2) \cdot n/N) = \sin(\pi n) = 0$$ which means that these two sinusoid cannot be considered because they are the 0 vector of this space. Therefore, we get that $$\dim \rbk{\mbox{span}\rbk{\cbk{\cos(2 \pi \cdot m\cdot n/ N)}_{m = 1}^{N - 1}}} = \soe{ll}{\tfrac{N}{2} - 1 & \mbox{$N$ is even} \\ \tfrac{N - 1}{2} & \mbox{$N$ is odd}}$$ Finally, this means that putting all these cosines and sinusoids together we form a base for a space that has size $$\soe{ll}{\tfrac{N}{2} + 1 + \tfrac{N}{2} - 1 = N & \mbox{$N$ is even} \\ \tfrac{N + 1}{2} + \tfrac{N - 1}{2} = N & \mbox{$N$ is odd}} = N$$ Hence, we can fully describe $\R^N$, namely for any vector $\mathbf x \in \R^N$ we have that $$\forall n \in [0, N- 1] \quad \mathbf x(n) = \sum_{m = 0}^{N - 1}{\alpha_m \cos(2\pi m n/N)} + \sum_{m = 0}^{N - 1}{\beta_m \sin(2\pi m n/N)}$$ by describing the vector component-wise.

Furthermore, we observe that this basis is actually orthogonal. \todo{prove it, boring}

Now, the last thing that we need to do is move into the complex space $\C^N$. Thankfully, we can leverage the very powerful Euler's formula $$e^{i \theta} = \cos \theta + i \sin \theta$$ to immediately some the contributions of cosinusoids and sinusoids into a single value $e^{2 \pi i m n/N}$. In other words, for any vector $\mathbf x \in \C^N$ it holds that $$\forall n \in [0, N - 1] \quad \mathbf x(n) = \sum_{m = 0}^{N - 1}{\gamma_m e^{2\pi i m n/N}}$$ The basis of this space is thus $$e^{2 \pi i \cdot 0 \cdot x / N} \quad e ^{2 \pi i \cdot 1 \cdot x / N } \quad \ldots \quad e^{2 \pi i \cdot (N - 1) \cdot n / N}$$ We observe that this basis is orthogonal as well, but it's not orthonormal in fact \todo{prove it, boring too} Hence, because the norm of each vector is
\begin{equation*}
    \begin{split}
        \sqrt{\abk{e^{2 \pi i m n/N}|e^{2 \pi i mn/N}}} & = \sqrt{\sum_{n = 0}^{N - 1}{e^{2 \pi i mn/N} \cdot \overline{e^{2 \pi i mn/N}}}} \\ 
                                                       & = \sqrt{\sum_{n = 0}^{N - 1}{\abs{e^{2 \pi i m n/ N}}^2}} \\ 
                                                       & = \sqrt{\sum_{n = 0}^{N - 1}{1}} \\ 
                                                       & = \sqrt N \\ 
    \end{split}
\end{equation*}
we usually consider the same base but scaled by a factor of $\tfrac{1}{\sqrt N}$, i.e. $$\forall n \in [0, N - 1] \quad \mathbf x(n) = \dfrac{1}{\sqrt N}\sum_{m = 0}^{N - 1}{\delta_m e^{2\pi i m n/N}}$$ We define $\delta_m$ to be the $m$-th component of the \tbf{discrete Fourier transform} of $\mathbf x$, and the operation that reconstructs $\mathbf x(n)$ is called \tbf{inverse discrete Fourier transform}. In other words, the DFT of a vector computes the projection of said vector into the space $\C^N$ through the orthonormal basis $$\cbk{\tfrac{1}{\sqrt N}e^{2 \pi i m n/N}}_{m = 0}^{N - 1}$$ Indeed, we can compute the DFT by simply applying the matrix that performs the \tbf{change of basis}, but we will see the details of this idea in the next section.

\subsection{Roots of unity}

Now that we have a general understanding of what the DFT is, let's see how the matrix operation is actually defined.

\begin{frameddefn}{$n$-th root of unity}
    Given a commutative ring $R$, and a natural number $n \in N$, an element $\omega \in R$ is called \tbf{$n$-th root of unity} if $\omega ^n = 1$.
\end{frameddefn}

For instance, $-i$ is a $4$-th root of unity, in fact $(-i)^4 = 1$. We observe that, since multiplication in the complex plane is a rotation, the $n-$-th roots of 1 evenly divide the unit circle as shown below:

TODO \todo{drawing}

Now, among all possible $n$-th roots of unity we are going to provide a more specific definition. Let the \tbf{order of an $n$-th root of unity} $\omega$ be the smallest power $d$ such that $\omega^d = 1$. For instance, $(-1)^4 = 1$ indeed $-1$ is a $4$-th root of unity, however its order is 2 since $(-1)^2 = 1$ and $2 < 4$.

\begin{frameddefn}{Principal $n$-th root of unity}
    Given an $n$-th root of unity $\omega$, we say that $\omega$ is \tbf{principal} if and only if $\omega \neq 1$ and the order of $\omega$ is $n$.
\end{frameddefn}

In other words, the principal $n$-th root of unity is the first root (after 1) that we encounter on the unit circle. Indeed, thanks to Euler's formula we usually define the principal $n$-th principal root of unity as $$\omega := e^{2 \pi i /n}$$ since $\tfrac{2 \pi}{n}$ is the $n$-th slice of the unit circle. Furthermore, the second condition of the definition is usually not provided in terms of order of the root, and it's written as follows $$\forall p \in [n - 1] \quad \sum_{j = 0 }^{n - 1}{\omega^{jp}} = 0$$ Aside from the geometric interpretation of this sum, this is a finite complex geometric series with common ratio $\omega^p$, which implies that

\begin{itemize}
    \item if $\omega^p = 1$, then every term is $(\omega^p)^j = 1^j = 1$ for any $j$, meaning that the sum is equal to $$\sum_{j = 0}^{n - 1}{\omega^{jp }} = \sum_{j = 0}^{n - 1}{1} = n$$
    \item if $\omega^p \neq 1$, then we know that $$\sum_{j = 0}^{n - 1}{\omega^{jp }} = \dfrac{1 - (\omega^p)^n}{1 - \omega^p}$$ and this ratio is equal to 0 if and only if $$1 - (\omega^p)^n = 0 \iff \omega^{pn} = 1$$
\end{itemize}

Therefore, we have that this sum is equal to 0 if and only if $\omega^p \neq 1$ and $\omega^{pn} = 1$. However, we observe that if $\omega^p = 1$, since $p \in [n - 1]$ this would imply that the order of $\omega$ is less than $n$, meaning that $\omega$ was not a principal root.

Now that we presented roots of unity we can finally present how the DFT matrix is usually defined.

\begin{frameddefn}{Discrete Fourier Transform}
    Given a commutative ring $R$ of dimension $N$, the \tbf{Discrete Fourier Transform (DFT)} matrix in $R$ is defined as follows: $$\forall j,k \in [0, N - 1] \quad \mbox{DFT}_{jk} = \omega^{-jk}$$ where $\omega$ is the principal $N$-th root of unity.
\end{frameddefn}

Therefore, in general the DFT matrix looks like this: $$\mbox{DFT} := \rmat{1 & 1 & 1 & \cdots & 1 \\ 1 & \omega^{-1} & \omega^{-2} & \cdots & \omega^{-(N - 1)} \\ 1 & \omega^{-2} & \omega^{-4} & \cdots & \omega^{-2(N - 1)} \\ \vdots & \vdots & \vdots & \ddots & \vdots \\ 1 & \omega^{-(N - 1)} & \omega^{-2(N - 1)} & \cdots & \omega^{-(N - 1)^2}}$$

Moreover, we usually define the DFT matrix by setting $\omega = e^{\tfrac{2 \pi i}{n}}$, thus getting $$\forall j , k \in [0, N - 1] \quad \mbox{DFT}_{jk} = e^{-2 \pi i j k /N}$$ Indeed, having presented the intuition behing the DFT beforehand, it's now fairly obvious the reason why the DFT matrix looks like this: its just the matrix that performs the change of basis, and since the basis of the DFT space can be defined in terms of $n$-th roots of unity, this is the matrix we get.

Furthermore, not surprisingly the DFT matrix is invertible, in fact we have that $$\forall j , k \in [0, N - 1] \quad \mbox{IDFT}_{jk} = \mbox{DFT}^{-1}_{jk } = \dfrac{1}{N}\omega^{jk} = \dfrac{1}{N}e^{+2 \pi i j k /N}$$ and this matrix takes the name of \tbf{Inverse Discrete Fourier Transform (IDFT)}.

Then, can't we just use the DFT matrix in quantum computations whenever we need it and be done? The problem is that the DFT matrix is clearly not unitary: if we look closely, we see that the columns of the DFT matrix are only orthogonal and not orthonormal. Thus, to solve this problem, we need to normalize the column vectors.

\begin{frameddefn}{Quantum Fourier Transform}
    Given a Hilbert space $\mathcal H$ of size $N$, the \tbf{Quantum Fourier Transform (QFT)} matrix in $\mathcal H$ is defined as follows: $$\forall j, k \in [0, N - 1] \quad \mbox{QFT}_{jk} = \dfrac{1}{\sqrt N}\omega^{jk}$$ where $\omega$ is the principal $N$-th root of unity.
\end{frameddefn}

We observe that the QFT matrix has positive exponents, which is just a convention employed in quantum computing. Therefore, we get the following matrix: $$\mbox{QFT} := \rmat{1 & 1 & 1 & \cdots & 1 \\ 1 & \omega & \omega^2 & \cdots & \omega^{N - 1} \\ 1 & \omega^2 & \omega^4 & \cdots & \omega^{2(N - 1)} \\ \vdots & \vdots & \vdots & \ddots & \vdots \\ 1 & \omega^{N - 1} & \omega^{2(N - 1)} & \cdots & \omega^{(N - 1)^2}}$$ In particular, for any $\ket x$ basis state, the quantum Fourier transform can also be expressed as follows: $$\mbox{QFT} \ket x = \dfrac{1}{\sqrt N} \sum_{k = 0}^{N - 1}e^{+2 \pi i x k / N} \ket k$$ which directly implies that we can rewrite the QFT matrix as follows: $$\mbox{QFT} = \sum_{j = 0}^{N - 1}{\rbk{\rfrac{1}{\sqrt N} \sum_{k = 0}^{N - 1}{e^{2 \pi i j k /N} \ket k}} \bra j }$$

\begin{framedprop}{}
    The QFT is a unitary operator, and in particular $$\mbox{QFT}^\dag  = \sum_{j = 0}^{N - 1}{\ket j \rbk{\dfrac{1}{\sqrt N} \sum_{k = 0}^{N  -1}{e^{-2 \pi i j k /N} \bra k}}}$$
\end{framedprop}

\begin{proof}
    We will leave the proof of correctness of $\mbox{QFT}^\dag$ as an exercise, and we will focus on proving that the QFT operator is indeed unitary. \todo{da finire}
\end{proof}

\printbibliography % UNCOMMENT FOR BIBLIOGRAPHY

\end{document}
